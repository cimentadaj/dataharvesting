[{"path":"index.html","id":"welcome","chapter":"1 Welcome","heading":"1 Welcome","text":"Every day millions gigabytes data shared across internet. standard way internet users download data manually clicking downloading files formats Excel files, CSV files, Word files. However, process downloading files clicking works well downloading one two files. needed download 500 CSV files? happens need download data refreshed every 20 seconds? process manually clicking file simply feasible solution downloading data scale frequent intervals. ’s increasing amount data internet, ’s also increase tools allow access data programmatically. course focus exploring technologies.Let give example. COVID pandemic struck world 2020, paramount understand mortality rates causing general decomposition mortality (affecting males versus females well different age groups). team researchers Max Planck Institute Demographic Research set collect administrative data reported deaths country, regions, gender age. Now might sound like fun ’s actually awfully difficult countries uploaded data refreshed every day others weekly basis. regions given country might update data different schedule. website might different process getting know website can take time. Scale process collect data hundreds countries process collecting mortality data can become incredibly cumbersome: might spend hours web simply clicking links download data (see sample countries dates collect data ). Aside long tedious task boringly clicking links hours, chances ’ll also introduce errors along way. might confuse one region another assign wrong name CSV file, might skip particular country mistake might simply misspell country’s name, messing order files saving.example typical task want automate: wouldn’t great type robot automatically collect data different websites save files correct, explicity names ? ’s exactly team . created COVerAGE-DB dataset. help dozens collaborators, created web scraping scripts automatically collected data automated process run frequently needed. ’s managed created hundreds small robots work brought back data analyze. perfect motivation automated data acquisition extremely important tool every person works data benefit lot knowing tools.Throughout course, ’ll describe create upon basic formats data transfer JSON, HTML XML. learn subset, manipulate transform formats suitable data analysis tasks. data formats can accessed scraping website: programmatically accessing data website asking program download much data need frequently needed. integral part course focus perform ethical web scrapping, scale web scrapping download massive amounts data well program scraper access data frequent intervals.course also focus emerging technology websites share data: Application Programming Interfaces (API). touch upon basic formats data sharing APIs well make thousands data requests just seconds. Special emphasis made security ethical guidelines speaking APIs.big part course emphasize automation, way student create robust scalable data acquisition pipelines. step process, focus practical applications techniques exercise active participation students hands-challenges data acquisition. course make heavy use Github students need share homework well explore immense repository data acquisition technology already available open source software.goal course empower students right toolset ideas able create data acquisition programs, automate data extraction pipeline quickly transform data formats suitable analysis. course place contents light legal ethical consequences data acquisition can entail, always informing students best practices grabbing data internet.course assumes students familiar R programming language, transforming manipulating datasets well saving work Git Github. prior knowledge software development data acquisition techniques needed.","code":""},{"path":"index.html","id":"curriculum","chapter":"1 Welcome","heading":"1.1 Curriculum","text":"","code":""},{"path":"index.html","id":"todo-update-once-the-course-is-ready","chapter":"1 Welcome","heading":"1.2 TODO: Update once the course is ready","text":"introduction Web Scraping\nWeb Scraping?\nTypes Web Scraping\nData formats: XML HTML\nPractical access XML HTML\nAutomation Web Scraping programs\nSelenium JavaScript based scraping\nEthical issues Web Scraping\nPractical exercises\nWeb Scraping?Types Web ScrapingData formats: XML HTMLPractical access XML HTMLAutomation Web Scraping programsSelenium JavaScript based scrapingEthical issues Web ScrapingPractical exercisesData APIs\nAPI\nFundamentals API communication\nintroduction JSON format\nCreate API (share )\nREST architecture\nAPIs way share obtain data (kind)\nAutomation API requests\nTalking Databases\nAuthentication ethical access APIs\nPractical exercises\nAPIFundamentals API communicationAn introduction JSON formatCreate API (share )REST architectureAPIs way share obtain data (kind)Automation API requestsTalking DatabasesAuthentication ethical access APIsPractical exercisesAutomation Data Acquisition\nneed automation?\nAccessing servers\nTechnologies automating programs\nAutomating cron jobs\nLogging tasks\nPractical exercises\nneed automation?Accessing serversTechnologies automating programsAutomating cron jobsLogging tasksPractical exercises","code":""},{"path":"introduction-to-webscraping.html","id":"introduction-to-webscraping","chapter":"2 Introduction to Webscraping","heading":"2 Introduction to Webscraping","text":"Welcome world collecting data internet. Although \nTODOInstall scrapex explain problem decribed scrapex README web scraping tutorial doomed obsolete etc..\nWebscraping creative: clear way get want. Need come creative solutions.Install scrapex explain problem decribed scrapex README web scraping tutorial doomed obsolete etc..\nWebscraping creative: clear way get want. Need come creative solutions.Install packages used book.Install packages used book.TODO: Make website wider plots render nicelyYou need mad data cleaning skills string / regex intermediatte. Come general regex encompasses need know read book can’t figure , study . Talk string heavy lifter book.Highlight ’ll using xml2 package rvest package interchangeably. Explain rvest just wrapper around xml2 slightly high level. ’ll use along course.TODO: need write entire webscraping part can done without internet\nTODO: Add general script save plots folder chapter automatically (look R4DS initial scripts)","code":""},{"path":"a-primer-on-webscraping.html","id":"a-primer-on-webscraping","chapter":"3 A primer on Webscraping","heading":"3 A primer on Webscraping","text":"Webscraping subtle art ninja. need expert many things swiss army knife skills comes data cleaning data manipulation. String manipulations, data subsetting clever tricks rather exact solutions companion day day scraping needs. Throughout chapter ’ll give one--one tour expect webscraping wild. reason, book skip usual ‘start basics’ sections directly webscrape website see results efforts right away. Ready? Let’s get going .aim primer create plot like one:plot shows election results political parties Spain since 1978. data local computer ’ll need find online scrape . scrape specifically mean write little R script go website manually select data points tell . Wikipedia data throughout book work mostly local copies websites outlined # TODO introduction chapter.","code":""},{"path":"a-primer-on-webscraping.html","id":"getting-website-data-into-r","chapter":"3 A primer on Webscraping","heading":"3.1 Getting website data into R","text":"scrapex package function calledhistory_elections_spain_ex() points locally saved copy Wikipedia website persist time(original online link https://en.wikipedia.org/wiki/Elections_in_Spain). Let’s load packages take look website.website saved locally computer. can directly visualize browser like :bottom right can see plot ’d like generate. plots political parties since 1978 2020 elections. first step webscraping ‘read’ website R. can read_html function. pass website link (typical https://en.wikipedia.org string) since already website locally, just pass path local website:TODO: explain set user agent. Rewrite paragraph mix chapter.\n, set something called User-Agent. short, User-Agent . good practice identify person scraping website ’re causing trouble website, website can directly identify causing problems. can figure user agent paste string . addition, add time sleep 5 seconds function want make sure don’t cause troubles website ’re scraping due overload requests.can’t understand much output read_html HTML code behind website long. read_html shows top level details. case, don’t need understand details first. Now website already R need figure actual data elections website. scroll , near end website ’ll see table like one:precisely data need. contains election results parties since 1978. rvest package (already loaded) handy function called html_table() automatically extracts tables website R. However, html_table() needs know website ’re working need pass html_website read previous step:html_table() reads tables ’s lot information all_tables (list 10 tables precise). won’t print entire R object ’s verbose encourage reader write all_tables R console explore tables scraped automatically. understand information ’re looking . carefully inspecting tables, figure table ’re looking slot 5:can see ’s table saw website :","code":"\nlibrary(scrapex)\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\n\nlink <- history_elections_spain_ex()\nlink## [1] \"/home/runner/.local/share/renv/cache/v5/R-4.2/x86_64-pc-linux-gnu/scrapex/0.0.1.9999/9aa78cbb3eccaaa811756b4bf07fdd1a/scrapex/extdata/history_elections_spain//Elections_Spain.html\"\nbrowseURL(prep_browser(link))\nhtml_website <- link %>% read_html()\nhtml_website## {html_document}\n## <html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n## [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n## [2] <body class=\"mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject  ...\nall_tables <-\n  html_website %>%\n  html_table()\nelections_data <- all_tables[[5]]\nelections_data## # A tibble: 16 × 18\n##    Election  `UCD[a]`     PSOE `PP[b]` `IU[c]` `CDC[d]`   PNV `ERC[e]` `BNG[f]`\n##    <chr>     <chr>       <dbl>   <dbl> <chr>      <dbl> <dbl>    <dbl>    <dbl>\n##  1 Election  \"\"           NA      NA   \"\"          NA    NA       NA       NA  \n##  2 1977      \"34.4\"       29.3     8.3 \"9.3\"        2.8   1.7      0.8      0.1\n##  3 1979      \"34.8\"       30.4     6.1 \"10.8\"       1.7   1.6      0.7      0.3\n##  4 1982      \"6.8\"        48.1    26.4 \"4.0\"        3.7   1.9      0.7      0.2\n##  5 1986      \"Dissolved\"  44.1    26   \"4.6\"        5     1.5      0.4      0.1\n##  6 1989      \"Dissolved\"  39.6    25.8 \"9.1\"        5     1.2      0.4      0.2\n##  7 1993      \"Dissolved\"  38.8    34.8 \"9.6\"        4.9   1.2      0.8      0.5\n##  8 1996      \"Dissolved\"  37.6    38.8 \"10.5\"       4.6   1.3      0.7      0.9\n##  9 2000      \"Dissolved\"  34.2    44.5 \"5.4\"        4.2   1.5      0.8      1.3\n## 10 2004      \"Dissolved\"  42.6    37.7 \"5.0\"        3.2   1.6      2.5      0.8\n## 11 2008      \"Dissolved\"  43.9    39.9 \"3.8\"        3     1.2      1.2      0.8\n## 12 2011      \"Dissolved\"  28.8    44.6 \"6.9\"        4.2   1.3      1.1      0.8\n## 13 2015      \"Dissolved\"  22      28.7 \"3.7\"        2.2   1.2      2.4      0.3\n## 14 2016      \"Dissolved\"  22.6    33   \"[k]\"        2     1.2      2.6      0.2\n## 15 Apr. 2019 \"Dissolved\"  28.7    16.7 \"[l]\"        1.9   1.5      3.9      0.4\n## 16 Nov. 2019 \"Dissolved\"  28      20.8 \"[l]\"        2.2   1.6      3.6      0.5\n## # … with 9 more variables: `EHB[g]` <chr>, `CDS[h]` <chr>, `CC[i]` <dbl>,\n## #   UPyD <chr>, Cs <chr>, Com. <chr>, `Pod.[j]` <dbl>, Vox <dbl>, MP <dbl>"},{"path":"a-primer-on-webscraping.html","id":"data-cleaning","chapter":"3 A primer on Webscraping","heading":"3.2 Data cleaning","text":"first column year column name political party. However, table problems need fix. Let’s outline things need fix:first row table emptyThe Election column character column row 15 16 contains months Apr. Nov..political party column values numbers (usually values representing foot notes [k] others Dissolved parties dissolved years). forces columns character columns fact want class numeric able visualize plot.Column names also footnote values names. probably remove .recalled first paragraphs primer, able webscraping, need data ninja. ’ll need become familiar basics regular expressions (fancy name manipulating strings) also cleaning data. ’ll use basics string manipulation, ’s fine feel completely lost. Just work hard little little ’ll learn tricks along way.first thing ’d want keep columns character. problems related columns:areWe can see different string values columns. ’d want replace non-numeric values NA’s. way convert columns numbers won’t loose information. can remove values? went columns wrote character values need remove:Now just need apply regular expression (regex now ) skills remove . Let’s explain want . regex world | stands . means want find words Banned Boycotted replace NA' write Banned|Boycotted. literally means Banned Boycotted. can take previous wrong_labels vector insert | wrong labels:effectively says: Dissolved [k] [l], …string can use function str_replace_all replace wrong labels NA’s. ’s ’d :Alright, don’t get stressed, ’ll explain line line. second line references data (elections_data, one ’ve working now). third line uses mutate_if function works applying function columns subset based criteria. Let’s break explanation even . can actually read code like :elections_dataFor columns character columns (mutate_if(character, ...))Apply transformation (mutate_if(character, ~ str_replace_all(...))example means character columns, function str_replace_all function applied. function replaces wrong_labels NA’s. can see right away:columns still characters, don’t wrong labels identified . problem Election column months Apr. Nov. won’t able convert numeric. can apply regex trick saying replace Apr. Nov. empty string. Let’s :Let’s check everything worked expected:go, don’t strings columns anymore. Let’s transform columns numeric remove first row empty:go, columns class numeric look nice tidy plotting. Last step need take remove footnote values party column names. ’ll need advanced regex patterns ’ll explain briefly (# TODO separate chapter ). pattern ’ll use [.+] means: detect character (.) repeated one times (+) enclosed within brackets ([] part). example, string Election won’t find match bracket values repeated one times. However, column name UCD[] pattern: contains two brackets [] value repeated one time ().’s also last trick need take account brackets ([]) special meaning regex world. signal regex want match brackets literally, need append backslash (\\\\). final regex pattern want match : \\\\[.+\\\\]. Let’s use rename columns:data set ready plot. ’s tidy clean. Let’s plot :. primer gave direct experience webscraping involves. involves read data website R, manually automatically finding chunks data want scrape, extracting cleaning enough able something useful . next chapter ’ll see depth work HTML XML data, ’ll need intuition find stuff within HTML XML document.","code":"\nelections_data %>% select_if(is.character)## # A tibble: 16 × 8\n##    Election  `UCD[a]`    `IU[c]` `EHB[g]`    `CDS[h]`    UPyD        Cs    Com. \n##    <chr>     <chr>       <chr>   <chr>       <chr>       <chr>       <chr> <chr>\n##  1 Election  \"\"          \"\"      \"\"          \"\"          \"\"          \"\"    \"\"   \n##  2 1977      \"34.4\"      \"9.3\"   \"0.2\"       \"\"          \"\"          \"\"    \"\"   \n##  3 1979      \"34.8\"      \"10.8\"  \"1.0\"       \"\"          \"\"          \"\"    \"\"   \n##  4 1982      \"6.8\"       \"4.0\"   \"1.0\"       \"2.9\"       \"\"          \"\"    \"\"   \n##  5 1986      \"Dissolved\" \"4.6\"   \"1.1\"       \"9.2\"       \"\"          \"\"    \"\"   \n##  6 1989      \"Dissolved\" \"9.1\"   \"1.1\"       \"7.9\"       \"\"          \"\"    \"\"   \n##  7 1993      \"Dissolved\" \"9.6\"   \"0.9\"       \"1.8\"       \"\"          \"\"    \"\"   \n##  8 1996      \"Dissolved\" \"10.5\"  \"0.7\"       \"0.2\"       \"\"          \"\"    \"\"   \n##  9 2000      \"Dissolved\" \"5.4\"   \"Boycotted\" \"0.1\"       \"\"          \"\"    \"\"   \n## 10 2004      \"Dissolved\" \"5.0\"   \"Banned\"    \"0.1\"       \"\"          \"\"    \"\"   \n## 11 2008      \"Dissolved\" \"3.8\"   \"Banned\"    \"0.0\"       \"1.2\"       \"0.2\" \"\"   \n## 12 2011      \"Dissolved\" \"6.9\"   \"1.4\"       \"Dissolved\" \"4.7\"       \"Did… \"0.5\"\n## 13 2015      \"Dissolved\" \"3.7\"   \"0.9\"       \"Dissolved\" \"0.6\"       \"13.… \"[k]\"\n## 14 2016      \"Dissolved\" \"[k]\"   \"0.8\"       \"Dissolved\" \"0.2\"       \"13.… \"[k]\"\n## 15 Apr. 2019 \"Dissolved\" \"[l]\"   \"1.0\"       \"Dissolved\" \"Did not r… \"15.… \"0.7\"\n## 16 Nov. 2019 \"Dissolved\" \"[l]\"   \"1.2\"       \"Dissolved\" \"[m]\"       \"6.8\" \"[n]\"\nwrong_labels <- c(\n  \"Dissolved\",\n  \"[k]\",\n  \"[l]\",\n  \"[m]\",\n  \"n\",\n  \"Banned\",\n  \"Boycotted\",\n  \"Did not run\"\n)\nwrong_labels <- paste0(wrong_labels, collapse = \"|\")\nwrong_labels## [1] \"Dissolved|[k]|[l]|[m]|n|Banned|Boycotted|Did not run\"\nsemi_cleaned_data <-\n  elections_data %>%\n  mutate_if(\n    is.character,\n    ~ str_replace_all(string = .x, pattern = wrong_labels, replacement = NA_character_)\n  )\nsemi_cleaned_data %>% select_if(is.character)## # A tibble: 16 × 8\n##    Election  `UCD[a]` `IU[c]` `EHB[g]` `CDS[h]` UPyD  Cs     Com. \n##    <chr>     <chr>    <chr>   <chr>    <chr>    <chr> <chr>  <chr>\n##  1 <NA>      \"\"       \"\"      \"\"       \"\"       \"\"    \"\"     \"\"   \n##  2 1977      \"34.4\"   \"9.3\"   \"0.2\"    \"\"       \"\"    \"\"     \"\"   \n##  3 1979      \"34.8\"   \"10.8\"  \"1.0\"    \"\"       \"\"    \"\"     \"\"   \n##  4 1982      \"6.8\"    \"4.0\"   \"1.0\"    \"2.9\"    \"\"    \"\"     \"\"   \n##  5 1986       <NA>    \"4.6\"   \"1.1\"    \"9.2\"    \"\"    \"\"     \"\"   \n##  6 1989       <NA>    \"9.1\"   \"1.1\"    \"7.9\"    \"\"    \"\"     \"\"   \n##  7 1993       <NA>    \"9.6\"   \"0.9\"    \"1.8\"    \"\"    \"\"     \"\"   \n##  8 1996       <NA>    \"10.5\"  \"0.7\"    \"0.2\"    \"\"    \"\"     \"\"   \n##  9 2000       <NA>    \"5.4\"    <NA>    \"0.1\"    \"\"    \"\"     \"\"   \n## 10 2004       <NA>    \"5.0\"    <NA>    \"0.1\"    \"\"    \"\"     \"\"   \n## 11 2008       <NA>    \"3.8\"    <NA>    \"0.0\"    \"1.2\" \"0.2\"  \"\"   \n## 12 2011       <NA>    \"6.9\"   \"1.4\"     <NA>    \"4.7\"  <NA>  \"0.5\"\n## 13 2015       <NA>    \"3.7\"   \"0.9\"     <NA>    \"0.6\" \"13.9\"  <NA>\n## 14 2016       <NA>     <NA>   \"0.8\"     <NA>    \"0.2\" \"13.1\"  <NA>\n## 15 Apr. 2019  <NA>     <NA>   \"1.0\"     <NA>     <NA> \"15.9\" \"0.7\"\n## 16 Nov. 2019  <NA>     <NA>   \"1.2\"     <NA>     <NA> \"6.8\"   <NA>\nsemi_cleaned_data <-\n  semi_cleaned_data %>%\n  mutate(\n    Election = str_replace_all(string = Election, pattern = \"Apr. |Nov. \", replacement = \"\")\n  )\nsemi_cleaned_data %>% select_if(is.character)## # A tibble: 16 × 8\n##    Election `UCD[a]` `IU[c]` `EHB[g]` `CDS[h]` UPyD  Cs     Com. \n##    <chr>    <chr>    <chr>   <chr>    <chr>    <chr> <chr>  <chr>\n##  1 <NA>     \"\"       \"\"      \"\"       \"\"       \"\"    \"\"     \"\"   \n##  2 1977     \"34.4\"   \"9.3\"   \"0.2\"    \"\"       \"\"    \"\"     \"\"   \n##  3 1979     \"34.8\"   \"10.8\"  \"1.0\"    \"\"       \"\"    \"\"     \"\"   \n##  4 1982     \"6.8\"    \"4.0\"   \"1.0\"    \"2.9\"    \"\"    \"\"     \"\"   \n##  5 1986      <NA>    \"4.6\"   \"1.1\"    \"9.2\"    \"\"    \"\"     \"\"   \n##  6 1989      <NA>    \"9.1\"   \"1.1\"    \"7.9\"    \"\"    \"\"     \"\"   \n##  7 1993      <NA>    \"9.6\"   \"0.9\"    \"1.8\"    \"\"    \"\"     \"\"   \n##  8 1996      <NA>    \"10.5\"  \"0.7\"    \"0.2\"    \"\"    \"\"     \"\"   \n##  9 2000      <NA>    \"5.4\"    <NA>    \"0.1\"    \"\"    \"\"     \"\"   \n## 10 2004      <NA>    \"5.0\"    <NA>    \"0.1\"    \"\"    \"\"     \"\"   \n## 11 2008      <NA>    \"3.8\"    <NA>    \"0.0\"    \"1.2\" \"0.2\"  \"\"   \n## 12 2011      <NA>    \"6.9\"   \"1.4\"     <NA>    \"4.7\"  <NA>  \"0.5\"\n## 13 2015      <NA>    \"3.7\"   \"0.9\"     <NA>    \"0.6\" \"13.9\"  <NA>\n## 14 2016      <NA>     <NA>   \"0.8\"     <NA>    \"0.2\" \"13.1\"  <NA>\n## 15 2019      <NA>     <NA>   \"1.0\"     <NA>     <NA> \"15.9\" \"0.7\"\n## 16 2019      <NA>     <NA>   \"1.2\"     <NA>     <NA> \"6.8\"   <NA>\nsemi_cleaned_data <-\n  semi_cleaned_data %>%\n  mutate_all(as.numeric) %>%\n  filter(!is.na(Election))\n\nsemi_cleaned_data## # A tibble: 15 × 18\n##    Election `UCD[a]`  PSOE `PP[b]` `IU[c]` `CDC[d]`   PNV `ERC[e]` `BNG[f]`\n##       <dbl>    <dbl> <dbl>   <dbl>   <dbl>    <dbl> <dbl>    <dbl>    <dbl>\n##  1     1977     34.4  29.3     8.3     9.3      2.8   1.7      0.8      0.1\n##  2     1979     34.8  30.4     6.1    10.8      1.7   1.6      0.7      0.3\n##  3     1982      6.8  48.1    26.4     4        3.7   1.9      0.7      0.2\n##  4     1986     NA    44.1    26       4.6      5     1.5      0.4      0.1\n##  5     1989     NA    39.6    25.8     9.1      5     1.2      0.4      0.2\n##  6     1993     NA    38.8    34.8     9.6      4.9   1.2      0.8      0.5\n##  7     1996     NA    37.6    38.8    10.5      4.6   1.3      0.7      0.9\n##  8     2000     NA    34.2    44.5     5.4      4.2   1.5      0.8      1.3\n##  9     2004     NA    42.6    37.7     5        3.2   1.6      2.5      0.8\n## 10     2008     NA    43.9    39.9     3.8      3     1.2      1.2      0.8\n## 11     2011     NA    28.8    44.6     6.9      4.2   1.3      1.1      0.8\n## 12     2015     NA    22      28.7     3.7      2.2   1.2      2.4      0.3\n## 13     2016     NA    22.6    33      NA        2     1.2      2.6      0.2\n## 14     2019     NA    28.7    16.7    NA        1.9   1.5      3.9      0.4\n## 15     2019     NA    28      20.8    NA        2.2   1.6      3.6      0.5\n## # … with 9 more variables: `EHB[g]` <dbl>, `CDS[h]` <dbl>, `CC[i]` <dbl>,\n## #   UPyD <dbl>, Cs <dbl>, Com. <dbl>, `Pod.[j]` <dbl>, Vox <dbl>, MP <dbl>\nsemi_cleaned_data <-\n  semi_cleaned_data %>%\n  rename_all(~ str_replace_all(.x, \"\\\\[.+\\\\]\", \"\"))\n\nsemi_cleaned_data## # A tibble: 15 × 18\n##    Election   UCD  PSOE    PP    IU   CDC   PNV   ERC   BNG   EHB   CDS    CC\n##       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1     1977  34.4  29.3   8.3   9.3   2.8   1.7   0.8   0.1   0.2  NA    NA  \n##  2     1979  34.8  30.4   6.1  10.8   1.7   1.6   0.7   0.3   1    NA    NA  \n##  3     1982   6.8  48.1  26.4   4     3.7   1.9   0.7   0.2   1     2.9  NA  \n##  4     1986  NA    44.1  26     4.6   5     1.5   0.4   0.1   1.1   9.2   0.3\n##  5     1989  NA    39.6  25.8   9.1   5     1.2   0.4   0.2   1.1   7.9   0.3\n##  6     1993  NA    38.8  34.8   9.6   4.9   1.2   0.8   0.5   0.9   1.8   0.9\n##  7     1996  NA    37.6  38.8  10.5   4.6   1.3   0.7   0.9   0.7   0.2   0.9\n##  8     2000  NA    34.2  44.5   5.4   4.2   1.5   0.8   1.3  NA     0.1   1.1\n##  9     2004  NA    42.6  37.7   5     3.2   1.6   2.5   0.8  NA     0.1   0.9\n## 10     2008  NA    43.9  39.9   3.8   3     1.2   1.2   0.8  NA     0     0.7\n## 11     2011  NA    28.8  44.6   6.9   4.2   1.3   1.1   0.8   1.4  NA     0.6\n## 12     2015  NA    22    28.7   3.7   2.2   1.2   2.4   0.3   0.9  NA     0.3\n## 13     2016  NA    22.6  33    NA     2     1.2   2.6   0.2   0.8  NA     0.3\n## 14     2019  NA    28.7  16.7  NA     1.9   1.5   3.9   0.4   1    NA     0.5\n## 15     2019  NA    28    20.8  NA     2.2   1.6   3.6   0.5   1.2  NA     0.5\n## # … with 6 more variables: UPyD <dbl>, Cs <dbl>, Com. <dbl>, Pod. <dbl>,\n## #   Vox <dbl>, MP <dbl>\n# Pivot from wide to long to plot it in ggplot\ncleaned_data <-\n  semi_cleaned_data %>%\n  pivot_longer(-Election, names_to = \"parties\")\n\n# Plot it\ncleaned_data %>%\n  ggplot(aes(Election, value, color = parties)) +\n  geom_line() +\n  scale_y_continuous(labels = function(x) paste0(x, \"%\")) +\n  scale_color_viridis_d() +\n  theme_minimal()"},{"path":"a-primer-on-webscraping.html","id":"exercises","chapter":"3 A primer on Webscraping","heading":"3.3 Exercises","text":"Europe ageing problem mandatory retirement age constantly revised. scrapex package ’s copy “Retirement Europe” wikipedia website https://en.wikipedia.org/wiki/Retirement_in_Europe. can find local link function retirement_age_europe_ex(). Can inspect website, parse table replicate plot ? (Hint: might need function str_sub stringr package).parsing elections table, parsed tables Wikipedia table all_tables. Among tables, ’s one table documents years general elections, presidential elections, european elections, local elections, regional elections referendums Spain. Can extract numeric vector years general elections Spain? (Hint: might need str_split stringr functions resulting vector start 1810 end 2019).parsing elections table, parsed tables Wikipedia table all_tables. Among tables, ’s one table documents years general elections, presidential elections, european elections, local elections, regional elections referendums Spain. Can extract numeric vector years general elections Spain? (Hint: might need str_split stringr functions resulting vector start 1810 end 2019).Building previous code, can tell years local elections, european elections general elections overlapped?Building previous code, can tell years local elections, european elections general elections overlapped?","code":""},{"path":"data-formats-for-webscraping.html","id":"data-formats-for-webscraping","chapter":"4 Data Formats for Webscraping","heading":"4 Data Formats for Webscraping","text":"webscraping ’ll involve parsing either XML HTML. two formats much alike fact many examples ’ll notice almost indistinguishable. web ’ll find formal definitions languages ’s definition layman person: series tags formats website structured (HTML) store transfer data (XML). Still rather vague eh? Let’s go concrete examples.XML abbreviation Extensible Markup Language whereas HTML stands Hypertext Markup Language. might’ve guessed, ’re ‘Markup’ languages, share lot common. R can read formats xml2 package. Let’s load package getting started:","code":"\nlibrary(xml2)"},{"path":"data-formats-for-webscraping.html","id":"a-primer-on-xml-and-html","chapter":"4 Data Formats for Webscraping","heading":"4.1 A primer on XML and HTML","text":"Let’s begin simple example. define string look structure:XML HTML basic building blocks called tags. example, first tag structure shown <people>. tag matched <\/people> end string:pay close attention, ’ll see tag XML structure beginning (signaled <>) end (signaled <\/>). example, next tag <people> <jason> right tag <carol> end jason tag <\/jason>.Similarly, ’ll find <carol> tag also matched <\/carol> finishing tag.XML world, tags can whatever meaning attach (<people> <occupation>). However, HTML hundreds tags standard structuring websites. want stop second highlight XML HTML many differences, conceptual visible users. Throughout rest chapter ’ll focus think important ones context webscraping.Let’s compare visibly previous XML example HTML:One key differences tags (<div>, <head>, <title>, etc..) specific properties structure website shown someone browser. Anything inside <head> tag header website. Anything within <body> tag body website, . tags standard across HTML language predetermined behavior format website. Let’s look another HTML also result structured website. ’s code:rendered website:heading contains hyperlink (<> tag href property), bigger bold comparison “paragraph”. specific tags together attributes interpreted give outline text ’ll start notice, trademark value HTML language.contrast, XML tags meaning creator meant . <occupation> tag simply means inside tag occupation related content. ’s say XML transferring data (tags inherent behavior) HTML structuring website.Another difference XML HTML HTML tags don’t need closed, meaning don’t need <\\ > tag (example <br> adds space content website). hand, XML strict ’ll find tags equivalent closing tag.Now, might asking , since standard tags XML, many standard HTML tags ? Well, many remember ’re getting started. ’s short set:comprehensive list see . don’t learn every single tag webscraping (fact know handful) ’s helpful hint able locate specific parts website ’re interested webscraping. theory way, let’s get hands dirty manipulating formats R.R can read XML HTML formats read_xml read_html functions. Let’s read XML string fake example look general structure:can see structure tree-based, meaning tags <jason> <carol> nested within <people> tag. XML jargon, <people> root node, whereas <jason> <carol> child nodes <people>.detail, structure follows:root node <people>child nodes <jason> <carol>child node nodes <first_name>, <married>, <last_name> <occupation> nested within .Put another way, something nested within node, nested node child upper-level node. example, root node <people> can check children:","code":"\nxml_test <- \"<people>\n<jason>\n  <person type='fictional'>\n    <first_name>\n      <married>\n        Jason\n      <\/married>\n    <\/first_name>\n    <last_name>\n        Bourne\n    <\/last_name>\n    <occupation>\n      Spy\n    <\/occupation>\n  <\/person>\n<\/jason>\n<carol>\n  <person type='real'>\n    <first_name>\n      <married>\n        Carol\n      <\/married>\n    <\/first_name>\n    <last_name>\n        Kalp\n    <\/last_name>\n    <occupation>\n      Scientist\n    <\/occupation>\n  <\/person>\n<\/carol>\n<\/people>\n\"\n\ncat(xml_test)## <people>\n## <jason>\n##   <person type='fictional'>\n##     <first_name>\n##       <married>\n##         Jason\n##       <\/married>\n##     <\/first_name>\n##     <last_name>\n##         Bourne\n##     <\/last_name>\n##     <occupation>\n##       Spy\n##     <\/occupation>\n##   <\/person>\n## <\/jason>\n## <carol>\n##   <person type='real'>\n##     <first_name>\n##       <married>\n##         Carol\n##       <\/married>\n##     <\/first_name>\n##     <last_name>\n##         Kalp\n##     <\/last_name>\n##     <occupation>\n##       Scientist\n##     <\/occupation>\n##   <\/person>\n## <\/carol>\n## <\/people>\nhtml_test <- \"<html>\n  <head>\n    <title>Div Align Attribbute<\/title>\n  <\/head>\n  <body>\n    <div align='left'>\n      First text\n    <\/div>\n    <div align='right'>\n      Second text\n    <\/div>\n    <div align='center'>\n      Third text\n    <\/div>\n    <div align='justify'>\n      Fourth text\n    <\/div>\n  <\/body>\n<\/html>\n\"<!DOCTYPE html>\n<html>\n<head>\n<title>Page Title<\/title>\n<\/head>\n<body>\n\n<h1> <a href=\"www.google.com\">This is a Heading <\/a> <\/h1>\n<br>\n<p>This is a paragraph.<\/p>\n\n<\/body>\n<\/html>\nxml_raw <- read_xml(xml_test)\nxml_structure(xml_raw)## <people>\n##   <jason>\n##     <person [type]>\n##       <first_name>\n##         <married>\n##           {text}\n##       <last_name>\n##         {text}\n##       <occupation>\n##         {text}\n##   <carol>\n##     <person [type]>\n##       <first_name>\n##         <married>\n##           {text}\n##       <last_name>\n##         {text}\n##       <occupation>\n##         {text}\n# xml_child returns only one child (specified in search)\n# Here, jason is the first child\nxml_child(xml_raw, search = 1)## {xml_node}\n## <jason>\n## [1] <person type=\"fictional\">\\n  <first_name>\\n    <married>\\n        Jason\\n ...\n# Here, carol is the second child\nxml_child(xml_raw, search = 2)## {xml_node}\n## <carol>\n## [1] <person type=\"real\">\\n  <first_name>\\n    <married>\\n        Carol\\n      ...\n# Use xml_children to extract **all** children\nchild_xml <- xml_children(xml_raw)\n\nchild_xml## {xml_nodeset (2)}\n## [1] <jason>\\n  <person type=\"fictional\">\\n    <first_name>\\n      <married>\\n ...\n## [2] <carol>\\n  <person type=\"real\">\\n    <first_name>\\n      <married>\\n      ..."},{"path":"data-formats-for-webscraping.html","id":"tag-attributes","chapter":"4 Data Formats for Webscraping","heading":"4.2 Tag attributes","text":"Tags can also different attributes usually specified <fake_tag attribute='fake'> ended usual <\/fake_tag>. look XML structure example, ’ll notice <person> tag attribute called type. ’ll see real-world example, extracting attributes often aim scraping adventure. Using xml_attrs function can extract attributes match specific name:Wait, didn’t work? Well, look output child_xml, two nodes <jason> <carol>.tags attribute? , , something like <jason type='fake_tag'>. need look <person> tag within <jason> <carol> extract attribute <person>.sound familiar? <jason> <carol> associated <person> tag , making children. can just go one level running xml_children tags extract .Using xml_path function can even find ‘address’ nodes retrieve specific tags without write xml_children many times. example:‘address’ specific tags tree extract automatically? extract specific ‘addresses’ XML tree, main function ’ll use xml_find_all. function accepts XML tree ‘address’ string. can use simple strings, one given xml_path:expression asking node \"/people/jason/person\". return saying xml_raw %>% xml_child(search = 1). deeply nested trees, xml_find_all many times much cleaner calling xml_child recursively many times.However, cases ‘addresses’ used xml_find_all come separate language called XPath (fact, ‘address’ ’ve looking XPath). XPath complex language (regular expressions strings) ’ll cover chapter # TODO.Attributes flexible XML tag can many attributes see fit. example:attributes can also repeated many times, example, might generic <person> tag used person database:usual HTML, tags also standard specific meaning. Let’s read previous HTML example visualize structure:structure shows <div> tag align attribute. might’ve guessed, attribute aligns text. common attributes HTML tags easy understand. ’s list :<> tag contains text hyperlink (href). <img> tag (abbreviation image) contains src tag points image , together width height image. Finally, <p> tag short paragraph contains style attribute declaring styling properties text. just examples common HTML tags common attributes. case, ’ve outlined , ’s fine know tags; intuition behind enough locate specific parts website.finalize, whenever ’ll scraping something, ’ll want know whether ’s XML HTML based. manage receive .html .xml ’s just simple looking extension file. chance access source code file, can also look tags quickly see many standard HTML tags deduce actual format. Another solution just look root node ’ll see hint right away. ’ll see xml signaled right beginning:html:","code":"\n# Extract the attribute type from all nodes\nxml_attrs(child_xml, \"type\")## [[1]]\n## named character(0)\n## \n## [[2]]\n## named character(0)\nchild_xml## {xml_nodeset (2)}\n## [1] <jason>\\n  <person type=\"fictional\">\\n    <first_name>\\n      <married>\\n ...\n## [2] <carol>\\n  <person type=\"real\">\\n    <first_name>\\n      <married>\\n      ...\n# We go down one level of children\nperson_nodes <- xml_children(child_xml)\n\n# <person> is now the main node, so we can extract attributes\nperson_nodes## {xml_nodeset (2)}\n## [1] <person type=\"fictional\">\\n  <first_name>\\n    <married>\\n        Jason\\n ...\n## [2] <person type=\"real\">\\n  <first_name>\\n    <married>\\n        Carol\\n      ...\n# Both type attributes\nxml_attrs(person_nodes, \"type\")## [[1]]\n##        type \n## \"fictional\" \n## \n## [[2]]\n##   type \n## \"real\"\n# Specific address of each person tag for the whole xml tree\n# only using the `person_nodes`\nxml_path(person_nodes)## [1] \"/people/jason/person\" \"/people/carol/person\"\n# You can use results from xml_path like directories\nxml_find_all(xml_raw, \"/people/jason/person\")## {xml_nodeset (1)}\n## [1] <person type=\"fictional\">\\n  <first_name>\\n    <married>\\n        Jason\\n ...<name>\n<person age=\"23\" status=\"married\" occupation=\"teacher\"> John Doe <\/person>\n<\/name><name>\n<person age=\"23\" status=\"married\" occupation=\"teacher\"> John Doe <\/person>\n<person age=\"25\" status=\"single\" occupation=\"doctor\"> Jane Doe <\/person>\n<\/name>\nhtml_raw <- read_html(html_test)\nhtml_structure(html_raw)## <html>\n##   <head>\n##     <title>\n##       {text}\n##   <body>\n##     {text}\n##     <div [align]>\n##       {text}\n##     {text}\n##     <div [align]>\n##       {text}\n##     {text}\n##     <div [align]>\n##       {text}\n##     {text}\n##     <div [align]>\n##       {text}\n##     {text}<a href=\"https://www.w3schools.com\">Visit W3Schools<\/a>\n<img src=\"img_girl.jpg\" width=\"500\" height=\"600\">\n<p style=\"color:red;\">This is a red paragraph.<\/p><?xml version=\"1.0>\n<company>\n    <name> John Doe <\/name>\n    <email> johndoe@gmail.com <\/email>\n<\/address> <!DOCTYPE html>\n<html>\n<body>\n\n<h1>My First Heading<\/h1>\n<p>My first paragraph.<\/p>\n\n<\/body>\n<\/html>"},{"path":"data-formats-for-webscraping.html","id":"conclusion","chapter":"4 Data Formats for Webscraping","heading":"4.3 Conclusion","text":"frank . Although XML HTML important differences respect technology philosophy, difference pretty much us: webscraping needs, don’t care data formatted website (HTML) whether tags special meanings (XML), care formatted tags can extract information.Let’s brief recap summary. XML HTML code tag based, built opening closing tags like : <tag> <\/tag>. languages hierarchical, meaning nodes within nodes parent-child relationships. tags can attributes signal either behavior (HTML) data (XML) tags.read XML HTML R can use equivalent read_* functions xml2 package. navigate nodes data forms can use xml_child recursively find ’re looking . extract children given node, can use xml_children extract attributes given tag, can resort xml_attr. Finally, want extract text tag, xml_text extracts . functions give handy tool set explore small scale tree nodes XML HTML documents.finish, ’s important highlight HTML much widely used webscraping. webscraping websites HTML specifically designed show website formatted. However, difference indistinguishable webscraping R.","code":""},{"path":"data-formats-for-webscraping.html","id":"exercises-1","chapter":"4 Data Formats for Webscraping","heading":"4.4 Exercises","text":"Extract values align attributes html_raw (Hint, look function xml_children).Extract values align attributes html_raw (Hint, look function xml_children).Extract occupation Carol Kalp xml_rawExtract occupation Carol Kalp xml_rawExtract text <div> tags html_raw. result look specifically like :Extract text <div> tags html_raw. result look specifically like :Manually create XML string contains root node, two children nested within two grandchildren nested within child. first child second grandchild second child attribute called family set ‘1’. Read string find two attributes function xml_find_all xml_attrs.Manually create XML string contains root node, two children nested within two grandchildren nested within child. first child second grandchild second child attribute called family set ‘1’. Read string find two attributes function xml_find_all xml_attrs.output previous exercises either xml_nodeset html_document (can read top print results):output previous exercises either xml_nodeset html_document (can read top print results):Can extract text last name Carol scientist using R subsetting rules object? example some_object$people$person$... (Hint: xml2 function called as_list).","code":"[1] \"\\n      First text\\n    \"  \"\\n      Second text\\n    \"\n[3] \"\\n      Third text\\n    \"  \"\\n      Fourth text\\n    \"{html_document}\n<html>\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n    <div align=\"left\">\\n      First text\\n    <\/div>\\n    <div al ..."},{"path":"what-you-need-to-know-about-regular-expressions.html","id":"what-you-need-to-know-about-regular-expressions","chapter":"5 What you need to know about regular expressions","heading":"5 What you need to know about regular expressions","text":"set write chapter hesitant . don’t consider expert regular expressions think able take beginner expert complicated topic. top , dozens excellent tutorials regular expressions topic justice able . said, web scraping without knowing regular expression techniques. data ’ll extract web need type massaging. times ’ll find data need string, combined stuff. ’ll need know tools clean string extract just need.reason, decided wanted write chapter focusing think basics regexp. Moreover, wanted chapter use webscraping examples soon basics mind. basics bare minimum ’ll need clean usual web scraping strings ’ll extract. means ’ll give incomplete picture regexp can enough get running short amount time. reader interested topic, refer chapter strings book R Data Science. chapter give thorough introduction topic general applications. Without due, let’s begin.","code":""},{"path":"what-you-need-to-know-about-regular-expressions.html","id":"basic-matches","chapter":"5 What you need to know about regular expressions","heading":"5.1 Basic matches","text":"Let’s load packages ’ll use chapter. related t webscraping data cleaning. stringr package ’ll use regular expressions. ’ll see functions package begin str_, denoting related strings:Regular expressions (regexp now ) way find patterns within strings. find pattern, can extract replace original string. Let’s take famous quote Jorge Luis Borges find word “eighteenth” quote:’s one simplest regexp can think . ’s regular expression right : ’re matching word “eighteenth”. stringr two functions backbone using regexp R: str_replace_all str_extract_all. first one replace string another string. example:replaces word eighteenth [DATE].str_extract_all extract portion string matchHowever, ’s handy since know word want match exactly. ’ll see power later .","code":"\nlibrary(stringr)\nlibrary(scrapex)\nlibrary(rvest)\nlibrary(xml2)\nlibrary(lubridate)\nlibrary(vistime)\nlibrary(tibble)\nborges <- \"I like hourglasses, maps, eighteenth century typography.\"\nstr_view_all(borges, \"eighteenth\", match = TRUE)\nstr_replace_all(borges, \"eighteenth\", \"[DATE]\")## [1] \"I like hourglasses, maps, [DATE] century typography.\"\nstr_extract_all(borges, \"eighteenth\")## [[1]]\n## [1] \"eighteenth\""},{"path":"what-you-need-to-know-about-regular-expressions.html","id":"the-.-placeholder","chapter":"5 What you need to know about regular expressions","heading":"5.2 The . placeholder","text":"asked match occurrences eighteenth Eighteenth ? regexp ’s placeholder . matches letter, number type character. example:regexp used .ighteenth meaning want match character (.) followed directly ighteenth. matches capital letter word well lower case word. However, also match character, even empty spaces:","code":"\nborges <- \"I like hourglasses, maps, eighteenth Eighteenth century typography.\"\nstr_view_all(borges, \".ighteenth\", match = TRUE)\nborges <- \"I like hourglasses, maps, ighteenth century typography.\"\nstr_view_all(borges, \".ighteenth\", match = TRUE)"},{"path":"what-you-need-to-know-about-regular-expressions.html","id":"quantifiers","chapter":"5 What you need to know about regular expressions","heading":"5.3 Quantifiers","text":". regexp handy also can imprecise. . often used + means character . needs repeated one times. example, say want extract century phrase maps, [century] century. example ’s easy know exactly century. However, might want make generic extract word maps century. try something like :Nothing matched. ? ’re saying want match something like : maps, [] century.. Replace [] whatever character want ’s matching. course, doesn’t match anywhere string. want instead :saying: match part maps, followed character (.) repeated one times (+) followed century.","code":"\nborges_two_phrase <- c(\n  \"I like hourglasses, maps, eighteenth century typography.\",\n  \"I like hourglasses, maps, seventeenth century typography.\"\n)\n\nstr_view_all(borges_two_phrase, \"maps, . century\")\nstr_view_all(borges_two_phrase, \"maps, .+ century\")"},{"path":"what-you-need-to-know-about-regular-expressions.html","id":"escaping-.","chapter":"5 What you need to know about regular expressions","heading":"5.4 Escaping .","text":". useful can used general placeholder. However, want match literal .? asked match first sentence phrase without specifying “hourglasses”, regexp use?like .+ almost . says: match phrase like followed character repeated one times. However, matches entire phrase doesn’t know want stop first ., right hourglasses. match literal . need append two \\\\. look like : like .+\\\\.. reads like :Match phrase likeFollowed character repeated one times (.+)literal dot (\\\\.)Let’s apply :aware need append \\\\ front special character regexp. example, wanted match quantifier + literally, ’ll need like : \\\\+. ’s bunch special character regexp ’ll just need know like (, ), +, ., $, ^ |.","code":"\nborges <- \"I like hourglasses. I also like maps, eighteenth century typography\"\nstr_view_all(borges, \"I like .+\")\nstr_view_all(borges, \"I like .+\\\\.\")"},{"path":"what-you-need-to-know-about-regular-expressions.html","id":"the-or-operator","chapter":"5 What you need to know about regular expressions","heading":"5.5 The OR (|) operator","text":"make regexp generic, ’ll often want match either one regexp another. | allows succintly. Suppose want extract century vector . can like :However, want extract actual century replace maps, century. can use regexp say: replace either maps, century empty space, give actual century. case regexp like : maps, | century. Using str_replace_all can match expression replace empty space:","code":"\nborges_two_phrase <- c(\n  \"I like hourglasses, maps, eighteenth century typography.\",\n  \"I like hourglasses, maps, seventeenth century typography.\"\n)\n\nres <- str_extract_all(borges_two_phrase, \"maps, .+ century\")\nres## [[1]]\n## [1] \"maps, eighteenth century\"\n## \n## [[2]]\n## [1] \"maps, seventeenth century\"\nres %>% str_replace_all(\"maps, | century\", \"\")## [1] \"eighteenth\"  \"seventeenth\""},{"path":"what-you-need-to-know-about-regular-expressions.html","id":"anchors","chapter":"5 What you need to know about regular expressions","heading":"5.6 Anchors","text":"Anchors also another feature regexp. ^ used match start string $ match end string. example, match first letter entire text use:Conversely, match last letter string:might seem like something doesn’t much use ’s actually handy. Take text example:phrases “like” beginning also “like” end first sentence. Using know now won’t enough:achieve want use trick match literal dot add anchor beginning:","code":"\nborges <- \"I like hourglasses. I also like maps, eighteenth century typography\"\nstr_view_all(borges, \"^.\", match = TRUE)\nstr_view_all(borges, \".$\", match = TRUE)\nborges_long <- c(\n  \"I like cars. I like hourglasses, maps, eighteenth century typography\",\n  \"I like computers. I like hourglasses, maps, eighteenth century typography\"\n)\nstr_view_all(borges_long, \"^I like .+\")\nstr_view_all(borges_long, \"^I like .+\\\\.\")"},{"path":"what-you-need-to-know-about-regular-expressions.html","id":"matching-spaces","chapter":"5 What you need to know about regular expressions","heading":"5.7 Matching spaces","text":"can also match spaces regexp replace :Since spaces come different ways (new lines tabs), can also use special character \\\\s:","code":"\nstr_view_all(borges, \" \")\nstr_replace_all(borges, \"\\\\s\", \"\")## [1] \"Ilikehourglasses.Ialsolikemaps,eighteenthcenturytypography\""},{"path":"what-you-need-to-know-about-regular-expressions.html","id":"special-classes","chapter":"5 What you need to know about regular expressions","heading":"5.8 Special classes","text":"Aside magic ., regexp set special tools matching general patterns. Let’s touch upon three .\\\\d: matches digitsSuppose ’re scraping list countries GDP. scraping data end :keyword \\\\d match single digit. example, wanted match age child string (Angel 8 years old), writing regexp like \\\\d match 8. example:matched instead someone older, \\\\d match digit separately:pay close attention, regexp matched 56 digit separately. However, combine quantifier + match one digits. example:Going back gdp example, can extract GDP every country regexp \\\\d+, meaning “extract number repeated one times”:[abc]: matches , b, c.Regexps also cool shortcut extend regexp | () make flexible. example, say wanted match retirement ages men Europe 67 69:One way explicitly use | match ages like : (67|68|69). Instead, using brackets ([]), regexp match everything inside |. example:regexp match number 6 followed either 7, 8, 9. numbers sequential (now) additional shortcuts make simpler:Note [] works way anything: numbers, letters, punctuation, spaces, etc..[^abc]: matches anything except , b, c.Similarly previous example [abc], [^abc] match anything except letters. recycling previous retirement example, wanted match ages except 65, regexp like :ages 65 matched.","code":"\ngdp <- c(\n  \"Afghanistan 516 US dollars\",\n  \"Albania 6494 US dollars\",\n  \"Algeria 3765 US dollars\",\n  \"American Samoa 12844 US dollars\",\n  \"Andorra 43047 US dollars\"\n)\nstr_view_all(\"Angel is 8 years old\", \"\\\\d\")\nstr_view_all(\n  c(\"Angel is 8 years old\", \"Martha is 56 years old\"),\n  \"\\\\d\"\n)\nstr_view_all(\n  c(\"Angel is 8 years old\", \"Martha is 56 years old\"),\n  \"\\\\d+\"\n)\ngdp_chr <- str_extract_all(gdp, \"\\\\d+\")\nlapply(gdp_chr, as.numeric)## [[1]]\n## [1] 516\n## \n## [[2]]\n## [1] 6494\n## \n## [[3]]\n## [1] 3765\n## \n## [[4]]\n## [1] 12844\n## \n## [[5]]\n## [1] 43047\nretirement <-\n  # Read in our scrapex example with retirement ages\n  retirement_age_europe_ex() %>%\n  read_html() %>%\n  html_table() %>%\n  .[[2]]\n\nretirement## # A tibble: 41 × 6\n##    Country                Men               Women         Year  Notes References\n##    <chr>                  <chr>             <chr>         <chr> <chr> <chr>     \n##  1 Albania                65                61            2020  \"\"    [1]       \n##  2 Austria                65                60            2018  \"In … [1][3]    \n##  3 Belarus                62.5              57.5          2021  \"By … [5]       \n##  4 Belgium                65                65            2018  \"In … [5]       \n##  5 Bosnia and Herzegovina 65                65            2011  \"\"    [1]       \n##  6 Bulgaria               64 (and 4 months) 61 (and 8 mo… 2021  \"In … [2]       \n##  7 Croatia                65                62            2018  \"By … [2]       \n##  8 Cyprus                 65                65            2018  \"\"    [1][3]    \n##  9 Czech Republic         63 (and 4 months) 58 (and 8 mo… 2018  \"In … [7]       \n## 10 Denmark                67                67            2022  \"In … [3][5]    \n## # … with 31 more rows\nstr_view_all(retirement$Men, \"6[789]\")\nstr_view_all(retirement$Men, \"6[7-9]\")\nstr_view_all(retirement$Men, \"6[^5-9]\")"},{"path":"what-you-need-to-know-about-regular-expressions.html","id":"case-study-mapping-the-kings-of-france","chapter":"5 What you need to know about regular expressions","heading":"5.9 Case study: mapping the kings of France","text":"scrapex contains copy Wikipedia page “History France”. Throughout historical details, website several sections outlines kings/queens France years power. case study ’ll plot timeline plot, ’ll able visualize length kingdom king.Let’s load read website:website standard Wikipedia webpage: dozens sections lengthy description historical facts France.want extract names one corresponding years kingdomship. Let’s open developer tools specific chunk website:specifically, text ’re interested inside ul tag:However, ul tags common subsetting ul tags bring many matches. Instead want something like: bring ul tags tag title contains phrase “House Valois”. Let’s break write ://ul: bring ul tags document[.//]: subset tags ul tags (notice .)[contains(@title, \"House Valois\")]: tag needs title attribute contains “House Valois”.Let’s put together see worked:! second slot contains ul tag want (can see Valois right href). can redo previous XPath pick second node done:text pretty dirty text concatenated one single string contains kings/queens period kingdomship. Let’s clean . First thing want split string based character \\n. take close look string names person separated \\n. Let’s apply see looks:Better. 16 kings/queens two slots separated houses. House Valois House Bourbon. probably want remove two strings vector ’re interested distinguishing now. ’ll use regexp ^House matches string begins House. ’ll combine str_detect returns TRUE FALSE ’s match. ’ll use exclude two strings vector:go, 16 names corresponding period. task ’re extract two things vector. years power names. Let’s first extract years. regexp \\\\d+ extract digits strings. However, also extract extra years Charles IX inside parenthesis:Since years just clarification, can remove parenthesis everything inside just keep first period next name. Let’s think regexp like can :Parenthesis special characters regexp match escape like : \\\\( \\\\)don’t care text inside parenthesis can just use . placeholder + match many character neededThe final regexp can something like : \\\\(.+\\\\). Match parenthesis (literal) text inside. Let’s replace empty characterPerfect, removed extra parenthesis can now extract years:Great, regexp worked additional problems. kings/queens reigned single century, end period last two years. whose kingship lasted two centuries, two years written explicitly format YYYY. correct manually. One way like :function accepts character vector length two containing start end year period reigned. function checks number character end year two years, , subsets first two years start year pastes together. Let’s apply function loop years:Perfect, worked. kings/queens corresponding start/end years correct format. Next thing need extract names. Since names follow pattern name king/queen first, comma year, can come regexp extract . ’s one approach:^: beginning string.+: match characters repeated one times,: first commaLet’s try :Great, names matched correctly. just replace comma empty space make cleaner:Alright, way, rest combining years names together data frame. Let’s look sequence_kings convert data frames. Combine king names convert dates date objects R:Great job, ’s final data frame. can feed gg_vistime function package vistime visualize timeline:","code":"\nhistory_france_html <- history_france_ex()\nhistory_france <- read_html(history_france_html)\nbrowseURL(prep_browser(history_france_html))\nhistory_france %>%\n  xml_find_all(\"//ul[.//a[contains(@title, 'House of Valois')]]\")## {xml_nodeset (2)}\n## [1] <ul>\\n<li>\\n<a href=\"https://en.wikipedia.org/wiki/Capetian_Dynasty\" clas ...\n## [2] <ul>\\n<li>\\n<a href=\"https://en.wikipedia.org/wiki/House_of_Valois\" title ...\nall_txt <-\n  history_france %>%\n  xml_find_all(\"//ul[.//a[contains(@title, 'House of Valois')]][2]\") %>%\n  xml_text()\n\nall_txt## [1] \"House of Valois\\nLouis XI the Prudent, 1461–83\\nCharles VIII the Affable, 1483–98\\nLouis XII, 1498–1515\\nFrancis I, 1515–47\\nHenry II, 1547–59\\nFrancis II, 1559–60\\nCharles IX, 1560–74 (1560–63 under regency of Catherine de' Medici)\\nHenry III, 1574–89\\nHouse of Bourbon\\nHenry IV the Great, 1589–1610\\nthe Regency of Marie de Medici, 1610–17\\nLouis XIII the Just and his minister Cardinal Richelieu, 1610–43\\nthe Regency of Anne of Austria and her minister Cardinal Mazarin, 1643–51\\nLouis XIV the Sun King and his minister Jean-Baptiste Colbert, 1643–1715\\nthe Régence, a period of regency under Philip II of Orléans, 1715–23\\nLouis XV the Beloved and his minister Cardinal André-Hercule de Fleury, 1715–74\\nLouis XVI, 1774–92\"\nall_txt <-\n  all_txt %>%\n  str_split(\"\\n\") %>%\n  .[[1]]\n\nall_txt##  [1] \"House of Valois\"                                                                \n##  [2] \"Louis XI the Prudent, 1461–83\"                                                  \n##  [3] \"Charles VIII the Affable, 1483–98\"                                              \n##  [4] \"Louis XII, 1498–1515\"                                                           \n##  [5] \"Francis I, 1515–47\"                                                             \n##  [6] \"Henry II, 1547–59\"                                                              \n##  [7] \"Francis II, 1559–60\"                                                            \n##  [8] \"Charles IX, 1560–74 (1560–63 under regency of Catherine de' Medici)\"            \n##  [9] \"Henry III, 1574–89\"                                                             \n## [10] \"House of Bourbon\"                                                               \n## [11] \"Henry IV the Great, 1589–1610\"                                                  \n## [12] \"the Regency of Marie de Medici, 1610–17\"                                        \n## [13] \"Louis XIII the Just and his minister Cardinal Richelieu, 1610–43\"               \n## [14] \"the Regency of Anne of Austria and her minister Cardinal Mazarin, 1643–51\"      \n## [15] \"Louis XIV the Sun King and his minister Jean-Baptiste Colbert, 1643–1715\"       \n## [16] \"the Régence, a period of regency under Philip II of Orléans, 1715–23\"           \n## [17] \"Louis XV the Beloved and his minister Cardinal André-Hercule de Fleury, 1715–74\"\n## [18] \"Louis XVI, 1774–92\"\nall_txt <- all_txt[!str_detect(all_txt, \"^House\")]\nall_txt##  [1] \"Louis XI the Prudent, 1461–83\"                                                  \n##  [2] \"Charles VIII the Affable, 1483–98\"                                              \n##  [3] \"Louis XII, 1498–1515\"                                                           \n##  [4] \"Francis I, 1515–47\"                                                             \n##  [5] \"Henry II, 1547–59\"                                                              \n##  [6] \"Francis II, 1559–60\"                                                            \n##  [7] \"Charles IX, 1560–74 (1560–63 under regency of Catherine de' Medici)\"            \n##  [8] \"Henry III, 1574–89\"                                                             \n##  [9] \"Henry IV the Great, 1589–1610\"                                                  \n## [10] \"the Regency of Marie de Medici, 1610–17\"                                        \n## [11] \"Louis XIII the Just and his minister Cardinal Richelieu, 1610–43\"               \n## [12] \"the Regency of Anne of Austria and her minister Cardinal Mazarin, 1643–51\"      \n## [13] \"Louis XIV the Sun King and his minister Jean-Baptiste Colbert, 1643–1715\"       \n## [14] \"the Régence, a period of regency under Philip II of Orléans, 1715–23\"           \n## [15] \"Louis XV the Beloved and his minister Cardinal André-Hercule de Fleury, 1715–74\"\n## [16] \"Louis XVI, 1774–92\"\nall_txt[7]## [1] \"Charles IX, 1560–74 (1560–63 under regency of Catherine de' Medici)\"\nall_txt <-\n  all_txt %>%\n  str_replace_all(pattern = \"\\\\(.+\\\\)\", replacement = \"\")\n\nall_txt##  [1] \"Louis XI the Prudent, 1461–83\"                                                  \n##  [2] \"Charles VIII the Affable, 1483–98\"                                              \n##  [3] \"Louis XII, 1498–1515\"                                                           \n##  [4] \"Francis I, 1515–47\"                                                             \n##  [5] \"Henry II, 1547–59\"                                                              \n##  [6] \"Francis II, 1559–60\"                                                            \n##  [7] \"Charles IX, 1560–74 \"                                                           \n##  [8] \"Henry III, 1574–89\"                                                             \n##  [9] \"Henry IV the Great, 1589–1610\"                                                  \n## [10] \"the Regency of Marie de Medici, 1610–17\"                                        \n## [11] \"Louis XIII the Just and his minister Cardinal Richelieu, 1610–43\"               \n## [12] \"the Regency of Anne of Austria and her minister Cardinal Mazarin, 1643–51\"      \n## [13] \"Louis XIV the Sun King and his minister Jean-Baptiste Colbert, 1643–1715\"       \n## [14] \"the Régence, a period of regency under Philip II of Orléans, 1715–23\"           \n## [15] \"Louis XV the Beloved and his minister Cardinal André-Hercule de Fleury, 1715–74\"\n## [16] \"Louis XVI, 1774–92\"\nres <-\n  all_txt %>%\n  str_extract_all(\"\\\\d+\")\n\nres## [[1]]\n## [1] \"1461\" \"83\"  \n## \n## [[2]]\n## [1] \"1483\" \"98\"  \n## \n## [[3]]\n## [1] \"1498\" \"1515\"\n## \n## [[4]]\n## [1] \"1515\" \"47\"  \n## \n## [[5]]\n## [1] \"1547\" \"59\"  \n## \n## [[6]]\n## [1] \"1559\" \"60\"  \n## \n## [[7]]\n## [1] \"1560\" \"74\"  \n## \n## [[8]]\n## [1] \"1574\" \"89\"  \n## \n## [[9]]\n## [1] \"1589\" \"1610\"\n## \n## [[10]]\n## [1] \"1610\" \"17\"  \n## \n## [[11]]\n## [1] \"1610\" \"43\"  \n## \n## [[12]]\n## [1] \"1643\" \"51\"  \n## \n## [[13]]\n## [1] \"1643\" \"1715\"\n## \n## [[14]]\n## [1] \"1715\" \"23\"  \n## \n## [[15]]\n## [1] \"1715\" \"74\"  \n## \n## [[16]]\n## [1] \"1774\" \"92\"\nconvert_time_period <- function(x) {\n  start_year <- x[1]\n  end_year <- x[2]\n  # If end year has only 2 digits\n  if (nchar(end_year) == 2) {\n    # Extract the first two years from the start year\n    end_year_prefix <- str_sub(start_year, 1, 2)\n    # Paste together the correct year for the end year\n    end_year <- paste0(end_year_prefix, end_year)\n  }\n  # Replace correct end year\n  x[2] <- end_year\n  as.numeric(x)\n}\nsequence_kings <- lapply(res, convert_time_period)\nsequence_kings## [[1]]\n## [1] 1461 1483\n## \n## [[2]]\n## [1] 1483 1498\n## \n## [[3]]\n## [1] 1498 1515\n## \n## [[4]]\n## [1] 1515 1547\n## \n## [[5]]\n## [1] 1547 1559\n## \n## [[6]]\n## [1] 1559 1560\n## \n## [[7]]\n## [1] 1560 1574\n## \n## [[8]]\n## [1] 1574 1589\n## \n## [[9]]\n## [1] 1589 1610\n## \n## [[10]]\n## [1] 1610 1617\n## \n## [[11]]\n## [1] 1610 1643\n## \n## [[12]]\n## [1] 1643 1651\n## \n## [[13]]\n## [1] 1643 1715\n## \n## [[14]]\n## [1] 1715 1723\n## \n## [[15]]\n## [1] 1715 1774\n## \n## [[16]]\n## [1] 1774 1792\nall_txt %>%\n  str_extract(\"^.+,\")##  [1] \"Louis XI the Prudent,\"                                                  \n##  [2] \"Charles VIII the Affable,\"                                              \n##  [3] \"Louis XII,\"                                                             \n##  [4] \"Francis I,\"                                                             \n##  [5] \"Henry II,\"                                                              \n##  [6] \"Francis II,\"                                                            \n##  [7] \"Charles IX,\"                                                            \n##  [8] \"Henry III,\"                                                             \n##  [9] \"Henry IV the Great,\"                                                    \n## [10] \"the Regency of Marie de Medici,\"                                        \n## [11] \"Louis XIII the Just and his minister Cardinal Richelieu,\"               \n## [12] \"the Regency of Anne of Austria and her minister Cardinal Mazarin,\"      \n## [13] \"Louis XIV the Sun King and his minister Jean-Baptiste Colbert,\"         \n## [14] \"the Régence, a period of regency under Philip II of Orléans,\"           \n## [15] \"Louis XV the Beloved and his minister Cardinal André-Hercule de Fleury,\"\n## [16] \"Louis XVI,\"\nnames_kings <-\n  all_txt %>%\n  str_extract(\"^.+,\") %>%\n  str_replace_all(\",\", \"\")\n\nnames_kings##  [1] \"Louis XI the Prudent\"                                                  \n##  [2] \"Charles VIII the Affable\"                                              \n##  [3] \"Louis XII\"                                                             \n##  [4] \"Francis I\"                                                             \n##  [5] \"Henry II\"                                                              \n##  [6] \"Francis II\"                                                            \n##  [7] \"Charles IX\"                                                            \n##  [8] \"Henry III\"                                                             \n##  [9] \"Henry IV the Great\"                                                    \n## [10] \"the Regency of Marie de Medici\"                                        \n## [11] \"Louis XIII the Just and his minister Cardinal Richelieu\"               \n## [12] \"the Regency of Anne of Austria and her minister Cardinal Mazarin\"      \n## [13] \"Louis XIV the Sun King and his minister Jean-Baptiste Colbert\"         \n## [14] \"the Régence a period of regency under Philip II of Orléans\"            \n## [15] \"Louis XV the Beloved and his minister Cardinal André-Hercule de Fleury\"\n## [16] \"Louis XVI\"\n# Combine into data frames\nsequence_kings_df <- lapply(sequence_kings, function(x) data.frame(start = x[1], end = x[2]))\nfinal_kings <- do.call(rbind, sequence_kings_df)\n\n# Add king names\nfinal_kings$event <- names_kings\nfinal_kings$start <- make_date(final_kings$start, 1, 1)\nfinal_kings$end <- make_date(final_kings$end, 1, 1)\n\n# Final data frame\nfinal_kings <- as_tibble(final_kings)\nfinal_kings## # A tibble: 16 × 3\n##    start      end        event                                                  \n##    <date>     <date>     <chr>                                                  \n##  1 1461-01-01 1483-01-01 Louis XI the Prudent                                   \n##  2 1483-01-01 1498-01-01 Charles VIII the Affable                               \n##  3 1498-01-01 1515-01-01 Louis XII                                              \n##  4 1515-01-01 1547-01-01 Francis I                                              \n##  5 1547-01-01 1559-01-01 Henry II                                               \n##  6 1559-01-01 1560-01-01 Francis II                                             \n##  7 1560-01-01 1574-01-01 Charles IX                                             \n##  8 1574-01-01 1589-01-01 Henry III                                              \n##  9 1589-01-01 1610-01-01 Henry IV the Great                                     \n## 10 1610-01-01 1617-01-01 the Regency of Marie de Medici                         \n## 11 1610-01-01 1643-01-01 Louis XIII the Just and his minister Cardinal Richelieu\n## 12 1643-01-01 1651-01-01 the Regency of Anne of Austria and her minister Cardin…\n## 13 1643-01-01 1715-01-01 Louis XIV the Sun King and his minister Jean-Baptiste …\n## 14 1715-01-01 1723-01-01 the Régence a period of regency under Philip II of Orl…\n## 15 1715-01-01 1774-01-01 Louis XV the Beloved and his minister Cardinal André-H…\n## 16 1774-01-01 1792-01-01 Louis XVI\ngg_vistime(final_kings, col.group = \"event\", show_labels = FALSE)"},{"path":"what-you-need-to-know-about-regular-expressions.html","id":"exercises-2","chapter":"5 What you need to know about regular expressions","heading":"5.10 Exercises","text":"Extend case study period “State building Kingdom France (987–1453)”:Note require change previous code think slightly different regexp strategies. done, merge results case study produce complete lineage France’s history monarchy.Take look regexp: like .+\\\\.. says: match phrase like followed character (.) repeated one times (+) find dot (\\\\.). applied string extracts entire string:Instead, want :Look ? regexp come way fix regexp obtain want.Can extract unique royal houses Wikipedia document? , produce vector like one:Hint: need use XPath, xml_text() entire document extract text website apply regexp grab houses. ’ll need use ? also figure [:punct:] regexp.","code":"\ntext <- \"I like hourglasses. I also like maps, eighteenth century typography.\"\nstr_extract_all(text, \"I like .+\\\\.\")[[1]]## [1] \"I like hourglasses. I also like maps, eighteenth century typography.\"## [1] \"I like hourglasses.\"##  [1] \"House of Capet\"        \"House of Valois\"       \"House of Plantagenet\" \n##  [4] \"House of Bourbon\"      \"House of Guise\"        \"House of Toulouse\"    \n##  [7] \"House of Hohenstaufen\" \"House of Welf\"         \"House of Brandenburg\" \n## [10] \"House of Lancaster\"    \"House of Bonaparte\"    \"House of Orléans\""},{"path":"what-you-need-to-know-about-xpath.html","id":"what-you-need-to-know-about-xpath","chapter":"6 What you need to know about XPath","heading":"6 What you need to know about XPath","text":"XPath (XML Path Language) language designed identify address one several tags within HTML XML document. address, XPath allows us extract data tags. example, take look XML :extract book ‘Hyperion Cantos’ Dan Simmons, simplest XPath can use /bookshelf/dansimmons/book. Let’s break understand better:first node bookshelf start /bookshelf.child bookshelf <dansimmons> XPath becomes /bookshelf/dansimmons/child <dansimmons> <book> just add XPath: /bookshelf/dansimmons/bookThat doesn’t look hard, right? problem web scraping needs, exact address, node node, generalizable.","code":"<bookshelf>\n  <dansimmons>\n    <book>\n      Hyperion Cantos\n    <\/book>\n  <\/dansimmons>\n<\/bookshelf>"},{"path":"what-you-need-to-know-about-xpath.html","id":"finding-tags-with-xpath","chapter":"6 What you need to know about XPath","heading":"6.1 Finding tags with XPath","text":"read previous XPath let’s load libraries ’ll need chapter.Let’s read XML R test initial XPath:works expected. Now remember told specific address generalizable? someone added authors tag bookshelf?can’t find using previous XPath expression. know , ’s instead use XPath /bookshelf/authors/dansimmons/book. someone (, developer behind website ’re trying scrape) continually changes XML? Can’t build general expression? XPath handy tricks can use . example, ’s one thing know book Hyperion Cantos: written Dan Simmons. Instead, can extract <dansimmons> tag directly //dansimmons. return <dansimmons> tags entire XML document. However, since know ’s one <dansimmons> tag, know ’ll grabbing one ’re :// handy, means: search entire document bring back <dansimmons> tags. doesn’t matter depth <dansimmons> tag, three twenty times deep, // return occurrences tag. Let’s extend example include another Dan Simmons book release date:Can predict XPath return running ? Let’s find :returns <dansimmons> tag, expected. Within <dansimmons> two <book> tags can’t see result two tags directly output. want extract names books. can reuse idea using / book tag. know book direct child dansimmons:go, get two book nodes. book direct child <dansimmons>, / wouldn’t work. example, instead book searched release_year (new tag added well), return empty node:’s expected: dansimmons doesn’t direct child called release_year (grandchild <release_year>. However, know // search tags entire XML tree can instead request release_year tags inside dansimmons:point, might useful know xml_path can return literal, direct address given node. Exactly like first XPath chapter:concrete cases, using literal path one might much quicker coming single XPath makes general. ’re trying scrape something one-thing, manually navigating HTML/XML tree (functions xml_child directly web developer tools browser) copying ’s exact XPath location might better choice.XPath also allows hand pick nodes position. Within dansimmons tag two book tags. XPath expression look like subset 2nd <book> tag dansimmons? can tell XPath position tag want using [number], number replaced position:can supply position node. expected, return empty position doesn’t exist:However, throughout examples specific supplying exact address child node respect ’s parent. //dansimmons return dansimmons tags won’t able see children. need know specific book tags children dansimmons, . XPath introduces * wildcard pattern return children current parent tag. example:result dansimmons tag ’s children, regardless whether <book> tags tag. strategy useful ’re unsure nodes certain parent want extract : fact generalizable can extract children tag pick one ’re string manipulation.Similarly, * can used fill tag don’t know name . know author <book> tags don’t know name authors. extract book tags like :words, XPath saying: extract book tags three tags , doesn’t matter tags . ’ll see later chapter, quite nice trick complex HTML/XML structures.Let’s recap far:/ links two tags direct parent-child relationship// finds tags HTML/XML tree regardless depthUse [number] subset position node. example: //[8] return 8th <> tag.* wildcard allows signal nodes without specifying nodes.rules can take long way building XPath expressions real flexibility comes ’re available filter attributes given node.","code":"\nlibrary(xml2)\nlibrary(magrittr)\nlibrary(scrapex)\nraw_xml <- \"\n<bookshelf>\n  <dansimmons>\n    <book>\n      Hyperion Cantos\n    <\/book>\n  <\/dansimmons>\n<\/bookshelf>\"\n\nbook_xml <- read_xml(raw_xml)\ndirect_address <- \"/bookshelf/dansimmons/book\"\n\nbook_xml %>%\n  xml_find_all(direct_address)## {xml_nodeset (1)}\n## [1] <book>\\n      Hyperion Cantos\\n    <\/book>\n# Note the new `<authors>` tag, a child of `<bookshelf>`.\nraw_xml <- \"\n<bookshelf>\n  <authors>\n    <dansimmons>\n      <book>\n        Hyperion Cantos\n      <\/book>\n    <\/dansimmons>\n  <\/authors>\n<\/bookshelf>\"\n\nbook_xml <- raw_xml %>% read_xml()\n\nbook_xml %>%\n  xml_find_all(direct_address)## {xml_nodeset (0)}\nbook_xml %>%\n  xml_find_all(\"//dansimmons\")## {xml_nodeset (1)}\n## [1] <dansimmons>\\n  <book>\\n        Hyperion Cantos\\n      <\/book>\\n<\/dansimm ...\n# Note the new `<release_year>` tag below the second (also new) `<book>` tag\nraw_xml <- \"\n<bookshelf>\n  <authors>\n    <dansimmons>\n      <book>\n        Hyperion Cantos\n      <\/book>\n      <book>\n        <release_year>\n         1996\n        <\/release_year>\n        Endymion\n      <\/book>\n    <\/dansimmons>\n  <\/authors>\n<\/bookshelf>\"\n\nbook_xml <- raw_xml %>% read_xml()\nbook_xml %>%\n  xml_find_all(\"//dansimmons\")## {xml_nodeset (1)}\n## [1] <dansimmons>\\n  <book>\\n        Hyperion Cantos\\n      <\/book>\\n  <book>< ...\nbook_xml %>%\n  xml_find_all(\"//dansimmons/book\")## {xml_nodeset (2)}\n## [1] <book>\\n        Hyperion Cantos\\n      <\/book>\n## [2] <book><release_year>\\n         1996\\n        <\/release_year>\\n        End ...\nbook_xml %>%\n  xml_find_all(\"//dansimmons/release_year\")## {xml_nodeset (0)}\nbook_xml %>%\n  xml_find_all(\"//dansimmons//release_year\")## {xml_nodeset (1)}\n## [1] <release_year>\\n         1996\\n        <\/release_year>\nbook_xml %>%\n  xml_find_all(\"//dansimmons//release_year\") %>%\n  xml_path()## [1] \"/bookshelf/authors/dansimmons/book[2]/release_year\"\nbook_xml %>%\n  xml_find_all(\"//dansimmons/book[2]\")## {xml_nodeset (1)}\n## [1] <book><release_year>\\n         1996\\n        <\/release_year>\\n        End ...\nbook_xml %>%\n  xml_find_all(\"//dansimmons/book[8]\")## {xml_nodeset (0)}\nbook_xml %>%\n  xml_find_all(\"//dansimmons/*\")## {xml_nodeset (2)}\n## [1] <book>\\n        Hyperion Cantos\\n      <\/book>\n## [2] <book><release_year>\\n         1996\\n        <\/release_year>\\n        End ...\nbook_xml %>%\n  xml_find_all(\"/*/*/*/book\")## {xml_nodeset (2)}\n## [1] <book>\\n        Hyperion Cantos\\n      <\/book>\n## [2] <book><release_year>\\n         1996\\n        <\/release_year>\\n        End ..."},{"path":"what-you-need-to-know-about-xpath.html","id":"filter-by-attributes","chapter":"6 What you need to know about XPath","heading":"6.2 Filter by attributes","text":"parsing complicated websites, ’ll need additional flexibility parse HTML/XML. XPath great property allows pick tags specific attributes. Let’s update XML example include new author tag <stephenking>, one ’s books additional attributes books:power XPath comes can filter tags attributes. Perhaps ’d like extract book tags price, regardless author. catch books certain topic. Whenever want tags match specific attribute can add two brackets end tag match attribute ’re . Say wanted know Dan Simmons book price, XPath look like?new XPath saying: find <book> tags attribute price set yes descendants (necessarily direct child, //) <dansimmons> tag. Quite interesting eh? approach allows us much flexible language parsing HTML/XML documents. Everything inside [] serves add additional filters/criteria matches XPath. help keyword, can alter previous XPath get books price topic horror:grab books price attribute (’s different price set yes ):find books price:correct attribute price set ‘’. can also use keyword match certain properties:XPath goodies perform basic filtering (, , =, !=) also additional functions useful filtering. common ones include:contains()starts-()text()()count()use ? always use functions within context filtering (everything used inside []). can reach level fine-grained filtering can save hours searching source code XML/HTML document. go cases functions useful, let’s load new example scrapex package.rest chapter exercises ’ll working main page newspaper “El País”. “El País” international daily newspaper. among circulated newspapers Spain rich website ’ll scraping. can load function elpais_newspaper_ex():Let’s look website web browser:website news organized along left, right center website. scroll ’ll see dozens news snippets scattered throughout website. news organized sections ‘Culture’, ‘Sports’ ‘Business’.Let’s say ’re interested figuring links sections newspaper able scrape news separately area. avoid complexity, ’ll start first grabbing ‘Science’ section link first step. section want explore :left can see section ‘Science, Tech & Health’ articles belong section. words ‘Science, Tech & Health’ bold contain hyperlink main page science articles. ’s want access. right, ’ll see opened web developer tools browser. clicking manually ‘Science, Tech & Health’ right, source code highlights blue hyperlink .concretely, can see source code want <> tag nested within <section> tag (two tags <> tag). <> tag attribute href contains link:Ok, information can creative build XPath expressions says: find <> tags href attribute containing word ‘Science’ also inherits <section> tag:Hmm, XPath seems right output returns many tags. expecting one link general science section (something like https://english.elpais.com/science-tech/). know <> tag <section> tag two additional <header> <div> tags:two exact tags might sections can try specifying two wild cards tags <section> <>. example:’s one looking . Let’s explain XPath expression://section means search sections throughout HTML tree//section/*/* means search two direct children <section> (regardless tags )[contains(@href, 'science')] finds <> tags @href attribute contains text ‘science’.final expression says: finds <> tags @href attribute contains text ‘science’ descendant <section> tag two tags .might become evident, function contains searches text attribute. matches supplied text contained attribute want. Kinda like regular expressions. However can also use function text() points actual text tag. rewrite previous XPath make even precise:Instead, XPath grabs <> tags contain text ‘Science, Tech & Health’. fact, make even shorter. Since probably <> tag contains text ‘Science, Tech & Health’, can remove wildcards * tags:final expression asks <> tags descendants <section> tags contains specific science text. functions (text, contains) make filtering much precise easy understand. functions start-() perform job contains() matching whether attribute/text starts provided text.function () also useful filtering. negates everything inside filter expression. previous example, using () return sections ones containing text ‘Science, Tech & Health’:see links sections economy--business international. Finally, function count() allows use conditionals based counting something. One interesting question many sections three articles. might interested scraping newspaper sites measure whether bias amount news published certain sections. XPath directly tackles might like :looking result see attribute data-dtm-region contains information name section (see word culture third node). Let’s extract :Five sections, mostly entertainment related except first one front page (‘aperatura’ something like ‘opening’). Although XPath short, contains things might now. Let’s explain ://section find section tags XML document[count(.//article])] counts articles articles current tag. ’s write .//article dot (.) signals search articles current position. instead wrote //article search articles entire HTML tree.[count(.//article])]>3 counts sections three articlesThese XPath filtering rules can take long way building precise expressions. chapter covers somewhat beginner/intermediate introduction XPath one can take far. Trust tell XPath rules can fulfill vast percentage webscraping needs, start easy. start building scraping programs supposed run frequent intervals work bigger team developers dependent scraped data, might need careful build XPath expressions avoid breaking scraper frequently. However, fairly good start achieving scraping needs beginner.","code":"\n# Note the new <stephenking> tag with it's book 'The Stand' and all <book> tags have some attributes\nraw_xml <- \"\n<bookshelf>\n  <authors>\n    <dansimmons>\n      <book price='yes' topic='scifi'>\n        Hyperion Cantos\n      <\/book>\n      <book topic='scifi'>\n        <release_year>\n         1996\n        <\/release_year>\n        Endymion\n      <\/book>\n    <\/dansimmons>\n    <stephenking>\n    <book price='yes' topic='horror'>\n     The Stand\n    <\/book>\n    <\/stephenking>\n  <\/authors>\n<\/bookshelf>\"\n\nbook_xml <- raw_xml %>% read_xml()\nbook_xml %>%\n  xml_find_all(\"//dansimmons//book[@price='yes']\") %>%\n  xml_text()## [1] \"\\n        Hyperion Cantos\\n      \"\nbook_xml %>%\n  xml_find_all(\"//book[@price='yes' and @topic='horror']\") %>%\n  xml_text()## [1] \"\\n     The Stand\\n    \"\nbook_xml %>%\n  xml_find_all(\"//book[@price]\")## {xml_nodeset (2)}\n## [1] <book price=\"yes\" topic=\"scifi\">\\n        Hyperion Cantos\\n      <\/book>\n## [2] <book price=\"yes\" topic=\"horror\">\\n     The Stand\\n    <\/book>\nbook_xml %>%\n  xml_find_all(\"//book[@price!='yes']\")## {xml_nodeset (0)}\nbook_xml %>%\n  xml_find_all(\"//book[@price='yes' or @topic='scifi']\") %>%\n  xml_text()## [1] \"\\n        Hyperion Cantos\\n      \"                  \n## [2] \"\\n         1996\\n        \\n        Endymion\\n      \"\n## [3] \"\\n     The Stand\\n    \"\nnewspaper_link <- elpais_newspaper_ex()\nnewspaper <- read_html(newspaper_link)\nbrowseURL(prep_browser(newspaper_link))\nnewspaper %>%\n  xml_find_all(\"//section//a[contains(@href, 'science')]\")## {xml_nodeset (20)}\n##  [1] <a href=\"https://english.elpais.com/science-tech/2022-10-07/is-climate-c ...\n##  [2] <a href=\"https://english.elpais.com/science-tech/2022-10-07/worlds-top-m ...\n##  [3] <a href=\"https://english.elpais.com/science-tech/\" class=\"b_h_t _pr\">Sci ...\n##  [4] <a href=\"https://english.elpais.com/science-tech/2022-10-07/a-new-drug-t ...\n##  [5] <a href=\"https://english.elpais.com/science-tech/2022-10-07/a-new-drug-t ...\n##  [6] <a href=\"https://english.elpais.com/science-tech/2022-10-06/european-med ...\n##  [7] <a href=\"https://english.elpais.com/science-tech/2022-10-06/european-med ...\n##  [8] <a href=\"https://english.elpais.com/science-tech/2022-10-06/ophiuchus-th ...\n##  [9] <a href=\"https://english.elpais.com/science-tech/2022-10-06/ophiuchus-th ...\n## [10] <a href=\"https://english.elpais.com/science-tech/2022-09-17/one-girls-ge ...\n## [11] <a href=\"https://english.elpais.com/science-tech/2022-09-17/one-girls-ge ...\n## [12] <a href=\"https://english.elpais.com/science-tech/2022-10-06/global-warmi ...\n## [13] <a href=\"https://english.elpais.com/science-tech/2022-10-06/global-warmi ...\n## [14] <a href=\"https://english.elpais.com/science-tech/\" class=\"b_h_t _pr\">Sci ...\n## [15] <a href=\"https://english.elpais.com/science-tech/2022-10-06/how-can-a-sm ...\n## [16] <a href=\"https://english.elpais.com/science-tech/2022-10-06/how-can-a-sm ...\n## [17] <a href=\"https://english.elpais.com/science-tech/2022-10-04/the-orbit-of ...\n## [18] <a href=\"https://english.elpais.com/science-tech/2022-10-06/european-med ...\n## [19] <a href=\"https://english.elpais.com/science-tech/2022-10-01/rare-diamond ...\n## [20] <a href=\"https://english.elpais.com/science-tech/2022-09-30/carole-hoove ...\nnewspaper %>%\n  xml_find_all(\"//section/*/*/a[contains(@href, 'science')]\")## {xml_nodeset (2)}\n## [1] <a href=\"https://english.elpais.com/science-tech/\" class=\"b_h_t _pr\">Scie ...\n## [2] <a href=\"https://english.elpais.com/science-tech/\" class=\"b_h_t _pr\">Scie ...\nnewspaper %>%\n  xml_find_all(\"//section/*/*/a[contains(text(), 'Science, Tech & Health')]\") %>%\n  xml_attr(\"href\")## [1] \"https://english.elpais.com/science-tech/\"\nnewspaper %>%\n  xml_find_all(\"//section//a[contains(text(), 'Science, Tech & Health')]\") %>%\n  xml_attr(\"href\")## [1] \"https://english.elpais.com/science-tech/\"\nnewspaper %>%\n  xml_find_all(\"//section/*/*/a[not(contains(text(), 'Science, Tech & Health'))]\") %>%\n  xml_attr(\"href\")##  [1] \"https://english.elpais.com/economy-and-business/\"                                                                                               \n##  [2] \"https://english.elpais.com/opinion/the-global-observer/\"                                                                                        \n##  [3] \"https://english.elpais.com/international/\"                                                                                                      \n##  [4] \"https://english.elpais.com/culture/\"                                                                                                            \n##  [5] \"https://english.elpais.com/science-tech/\"                                                                                                       \n##  [6] \"https://english.elpais.com/society/\"                                                                                                            \n##  [7] \"https://elpais.com/archivo/#?prm=hemeroteca_pie_ep\"                                                                                             \n##  [8] \"https://elpais.com/archivo/#?prm=hemeroteca_pie_ep\"                                                                                             \n##  [9] \"https://play.google.com/store/apps/details?id=com.elpais.elpais&hl=es&gl=US\"                                                                    \n## [10] \"https://apps.apple.com/es/app/el-pa%C3%ADs/id301049096\"                                                                                         \n## [11] \"https://elpais.com/suscripciones/#/campaign#?prod=SUSDIGCRART&o=susdig_camp&prm=pw_suscrip_cta_pie_eng&backURL=https%3A%2F%2Fenglish.elpais.com\"\nnewspaper %>%\n  xml_find_all(\"//section[count(.//article)>3]\")## {xml_nodeset (5)}\n## [1] <section class=\"_g _g-md _g-o b b-d\" data-dtm-region=\"portada_apertura\">< ...\n## [2] <section class=\"b b-t b-t-ad _g-o \" data-dtm-region=\"portada_tematicos_sc ...\n## [3] <section class=\"b b-m _g-o\" data-dtm-region=\"portada_arrevistada_culture\" ...\n## [4] <section class=\"b b-t b-t-df b-t-1 _g-o \" data-dtm-region=\"portada_temati ...\n## [5] <section class=\"b b-t b-t-df b-t-1 _g-o \" data-dtm-region=\"portada_temati ...\nnewspaper %>%\n  xml_find_all(\"//section[count(.//article)>3]\") %>%\n  xml_attr(\"data-dtm-region\")## [1] \"portada_apertura\"                          \n## [2] \"portada_tematicos_science,-tech-&-health\"  \n## [3] \"portada_arrevistada_culture\"               \n## [4] \"portada_tematicos_celebrities,-movies-&-tv\"\n## [5] \"portada_tematicos_our-selection\""},{"path":"what-you-need-to-know-about-xpath.html","id":"xpath-cookbook","chapter":"6 What you need to know about XPath","heading":"6.3 XPath cookbook","text":"’ve written set cookbook commands might find useful webscraping using XPath:","code":"\n# Find all sections\nnewspaper %>%\n  xml_find_all(\"//section\")\n\n# Return all divs below all sections\nnewspaper %>%\n  xml_find_all(\"//section//div\")\n\n# Return all sections which a div as a child\nnewspaper %>%\n  xml_find_all(\"//section/div\")\n\n# Return the child (any, because of *) of all sections\nnewspaper %>%\n  xml_find_all(\"//section/*\")\n\n# Return all a tags of all section tags which have two nodes in between\nnewspaper %>%\n  xml_find_all(\"//section/*/*/a\")\n\n# Return all a tags below all section tags without a class attribute\nnewspaper %>%\n  xml_find_all(\"//section//a[not(@class)]\")\n\n# Return all a tags below all section tags that contain a class attribute\nnewspaper %>%\n  xml_find_all(\"//section//a[@class]\")\n\n# Return all a tags of all section tags which have two nodes in between\n# and contain some text in the a tag.\nnewspaper %>%\n  xml_find_all(\"//section/*/*/a[contains(text(), 'Science')]\")\n\n# Return all span tags in the document with a specific class\nnewspaper %>%\n  xml_find_all(\"//span[@class='c_a_l']\")\n\n# Return all span tags in the document that don't have a specific class\nnewspaper %>%\n  xml_find_all(\"//span[@class!='c_a_l']\")\n\n# Return all a tags where an attribute starts with something\nnewspaper %>%\n  xml_find_all(\"//a[starts-with(@href, 'https://')]\")\n\n# Return all a tags where an attribute contains some text\nnewspaper %>%\n  xml_find_all(\"//a[contains(@href, 'science-tech')]\")\n\n# Return all section tags which have tag *descendants (because of the .//)* that have a class attribute\nnewspaper %>%\n  xml_find_all(\"//section[.//a[@class]]\")\n\n# Return all section tags which have <td> children\nnewspaper %>%\n  xml_find_all(\"//section[td]\")\n\n# Return the first occurrence of a section tag\nnewspaper %>%\n  xml_find_all(\"(//section)[1]\")\n\n# Return the last occurrence of a section tag\nnewspaper %>%\n  xml_find_all(\"(//section)[last()]\")"},{"path":"what-you-need-to-know-about-xpath.html","id":"conclusion-1","chapter":"6 What you need to know about XPath","heading":"6.4 Conclusion","text":"XPath rich language 20 years development. ’ve covered basics well intermediate parts language ’s much learned. encourage look examples online check additional resources. leave best resources worked :XPath CheetsheetExtensive XPath CheetsheetXPath tutorial","code":""},{"path":"what-you-need-to-know-about-xpath.html","id":"exercises-3","chapter":"6 What you need to know about XPath","heading":"6.5 Exercises","text":"many jpg png images website? (Hint: look source code figure tag attribute contains links images).many jpg png images website? (Hint: look source code figure tag attribute contains links images).many articles entire website?many articles entire website?headlines (headlines mean bold text article begins ), many contain word ‘climate’?headlines (headlines mean bold text article begins ), many contain word ‘climate’?city reporters?city reporters?headline article words description? (Hint: remember .// searcher tags current tag. // search tags document, regardless whether ’s current selected node) text ’ll want measure amount letters bold headline news article:headline article words description? (Hint: remember .// searcher tags current tag. // search tags document, regardless whether ’s current selected node) text ’ll want measure amount letters bold headline news article:","code":""},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"case-study-scraping-spanish-school-locations-from-the-web","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7 Case study: scraping Spanish school locations from the web","text":"chapter ’ll scraping location sample schools Spain. case study involves using XPath find specific chunks code contain coordinates schools Spain, well inspecting source code website depth. final result chapter generate plot like one without previous knowledge schools :usual, website saved locally scrapex package changes made website don’t break code future. Althought links ’ll working hosted locally machine, HTML website similar one hosted online website (exception images/icons deleted purpose make package lightweight). said, local website fairly good representation ’ll find real website internet. website Spanish ’ll make sure point specifically type information ’re looking . Non-spanish speakers able work chapter without problem.begin, let’s load package ’ll use chapter:","code":"\nlibrary(xml2)\nlibrary(httr)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(scrapex)"},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"building-a-scraper-for-one-school","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.1 Building a scraper for one school","text":"Visualizing school locations can useful many things matching population density children across different regions school locations mapping patterns inequality geographical locations. website www.buscocolegio.com contains database schools Spain, containing plethora information school, together coordinates school located. function spanish_schools_ex() contains local links school.Let’s look example one school.Let’s look rendered website code :website shows standard details school “CEIP SANCHIS GUARNER”. can see ’s contact email contact details. Additionally, can see ’s location right. want extract precisely data visualize locations. can access coordinates?Let’s read website R start narrowing obtain coordinates:point, shouldn’t worry exploring actual website R. ’s always better idea look code browser. idea data ’re looking , can come back raw HTML search specific tags.Web scraping strategies specific website ’re . get familiar website ’re interested able match perfectly information ’re looking . many cases, scraping two websites require vastly different strategies.first thing want start looking source code maps section right can start looking hints coordinates somewhere code. popping web developer’s tools. browsers support tool can open press CTRL + SHIFT + c time (Firefox Chrome support hotkey). can also open searching settings menu browser looking ‘Web Developer Tools’. window right popped full code ’re right track:can search source code website. place mouse pointer lines code right-window, ’ll see sections website highlighted blue. indicates parts code refer parts website. never search complete source code find want rather approximate search typing text ’re looking search bar top right window.first intuition search text just Google Maps:Searching text best approximation ’ll just within maps section code can start looking coordinates instead entire website. Let’s search search bar developer tools:click enter, ’ll automatically directed tag information want.point, started browsing tags part code. surprise, actually landed specifically coordinates . can see latitude longitude schools found attributed called href <> tag:Can see latitude longitude fields text highlighted blue? ’s hidden -words. precisely type information ’re . href attribute <> tag contains coordinates.Extracting <> tags website yield hundreds matches <> common tag HTML. Refining search <> tags href attribute also yield hundreds matches href standard attribute attach links within websites. need narrow search within website.One strategy find ‘father’ ‘grandfather’ node particular <> tag match . strategy look ascendant tag particular property unique enough narrow search. example, father <> tag <p> tag, return dozens results. looking structure small HTML snippet right-window, see ‘father’ <> tag <p class=\"d-flex align-items-baseline g-mt-5'> particularly long attribute named class. seems like good candidate use:<p> tag father <> tag <> tag want extract coordinates :’s important intimidated tag names long attributes. also don’t know attributes mean. know ‘father’ <> tag ’m interested . using XPath skills, let’s search <p> tag class set 'd-flex align-items-baseline g-mt-5' see get one match. mean can identify tag precisely:one match, good news. means can uniquely identify particular <p> tag. Let’s refine search say: Find <> tags children specific <p> tag. means ’ll add \"//\" previous expression. Since one <p> tag class, ’re interested checking whether one <> tag <p> tag.go! can see specific href contains latitude longitude data ’re interested . extract href attribute? xml_attr standard function extract attributes tags:","code":"\nschool_links <- spanish_schools_ex()\n\n# Keep only the HTML file of one particular school.\nschool_url <- school_links[13]\n\nschool_url## [1] \"/home/runner/.local/share/renv/cache/v5/R-4.2/x86_64-pc-linux-gnu/scrapex/0.0.1.9999/9aa78cbb3eccaaa811756b4bf07fdd1a/scrapex/extdata/spanish_schools_ex/school_3006839.html\"\nbrowseURL(prep_browser(school_url))\nschool_raw <- read_html(school_url) %>% xml_child()\nschool_raw## {html_node}\n## <head>\n##  [1] <title>Aquí encontrarás toda la información necesaria sobre CEIP SANCHIS ...\n##  [2] <meta charset=\"utf-8\">\\n\n##  [3] <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shri ...\n##  [4] <meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\">\\n\n##  [5] <meta name=\"author\" content=\"BuscoColegio\">\\n\n##  [6] <meta name=\"description\" content=\"Encuentra toda la información necesari ...\n##  [7] <meta name=\"keywords\" content=\"opiniones SANCHIS GUARNER, contacto SANCH ...\n##  [8] <link rel=\"shortcut icon\" href=\"/favicon.ico\">\\n\n##  [9] <link rel=\"stylesheet\" href=\"//fonts.googleapis.com/css?family=Roboto+Sl ...\n## [10] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [11] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-awesome/css/font-awesom ...\n## [12] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-line/css/simple-line-ic ...\n## [13] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-line-pro/style.css\">\\n\n## [14] <link rel=\"stylesheet\" href=\"/assets/vendor/icon-hs/style.css\">\\n\n## [15] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [16] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [17] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [18] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [19] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## [20] <link rel=\"stylesheet\" href=\"https://s3.eu-west-3.amazonaws.com/buscocol ...\n## ...\n# Search for all <p> tags with that class in the document\nschool_raw %>%\n  xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']\")## {xml_nodeset (1)}\n## [1] <p class=\"d-flex align-items-baseline g-mt-5\">\\r\\n\\t                    < ...\nschool_raw %>%\n  xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']//a\")## {xml_nodeset (1)}\n## [1] <a href=\"/Colegio/buscar-colegios-cercanos.action?colegio.latitud=38.8274 ...\nlocation_str <-\n  school_raw %>%\n  xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']//a\") %>%\n  xml_attr(attr = \"href\")\n\nlocation_str## [1] \"/Colegio/buscar-colegios-cercanos.action?colegio.latitud=38.8274492&colegio.longitud=0.0221681\""},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"data-cleaning-1","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.1.1 Data cleaning","text":"Ok, now need regular expression skills get latitude longitude. string coordinates see first coordinate appears first =. Moreover, aside coordinates, ’s words like colegio (just spanish school) longitud middle two coordinates. mind, can write regex:\"=.+$\" captures = followed character (.) repeated 1 times (+) end string ($). layman terms: extract text = end string.Let’s apply :result exactly wanted. Now need replaced everything coordinates. regex look like:\"=|colegio\\\\.longitud\" matches = colegio.longitud (remember | stands regex). . preceded two \\\\? saw previous regex used, . regular expressions means character. signal want . matched literally write like \\\\.. layman terms: match either = colegio.longitud. Let’s apply :go. replaced everything coordinates except & . didn’t replace ? ’ll split two coordinates & separate:Alright, ’s end data cleaning efforts. managed locate specific HTML node contained school coordinates extracted using string manipulations. Now got information one single school, let’s turn function can pass school’s link get coordinates backTODO reference user agent chapter","code":"\nlocation <-\n  location_str %>%\n  str_extract_all(\"=.+$\")\n\nlocation## [[1]]\n## [1] \"=38.8274492&colegio.longitud=0.0221681\"\nlocation <-\n  location %>%\n  str_replace_all(\"=|colegio\\\\.longitud\", \"\")\n\nlocation## [1] \"38.8274492&0.0221681\"\nlocation <-\n  location %>%\n  str_split(\"&\") %>%\n  .[[1]]\n\nlocation## [1] \"38.8274492\" \"0.0221681\""},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"scaling-the-scraper-to-all-schools","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.2 Scaling the scraper to all schools","text":"moving code scrape one single case many cases, ’ll want move code function make compact, organized easy read. Let’s move code function also make sure set user agent well add time sleep 5 seconds function want make sure don’t cause troubles website ’re scraping due overload requests:can see worked single school, can try schools. thing left extract many schools. shown earlier, scrapex contains list 27 school links can automatically scrape. Let’s loop , get information coordinates collapse data frame.now locations schools, let’s plot :’s ! managed extract coordinates schools, clean plot .","code":"\n# This sets your `User-Agent` globally so that all requests are\n# identified with this `User-Agent`\nset_config(\n  user_agent(\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:105.0) Gecko/20100101 Firefox/105.0\")\n)\n\n# Collapse all of the code from above into one function called\n# school grabber\n\nschool_grabber <- function(school_url) {\n  # We add a time sleep of 5 seconds to avoid\n  # sending too many quick requests to the website\n  Sys.sleep(5)\n\n  school_raw <- read_html(school_url) %>% xml_child()\n\n  location_str <-\n    school_raw %>%\n    xml_find_all(\"//p[@class='d-flex align-items-baseline g-mt-5']//a\") %>%\n    xml_attr(attr = \"href\")\n\n  location <-\n    location_str %>%\n    str_extract_all(\"=.+$\") %>%\n    str_replace_all(\"=|colegio\\\\.longitud\", \"\") %>%\n    str_split(\"&\") %>%\n    .[[1]]\n\n  # Turn into a data frame\n  data.frame(\n    latitude = location[1],\n    longitude = location[2],\n    stringsAsFactors = FALSE\n  )\n}\n\nschool_grabber(school_url)##     latitude longitude\n## 1 38.8274492 0.0221681\ncoordinates <- map_dfr(school_links, school_grabber)\ncoordinates##    latitude  longitude\n## 1  42.72779 -8.6567935\n## 2  43.24439 -8.8921645\n## 3  38.95592 -1.2255769\n## 4  39.18657 -1.6225903\n## 5  40.38245 -3.6410388\n## 6  40.22929 -3.1106322\n## 7  40.43860 -3.6970366\n## 8  40.33514 -3.5155669\n## 9  40.50546 -3.3738441\n## 10 40.63826 -3.4537107\n## 11 40.38543 -3.6639500\n## 12 37.76485 -1.5030467\n## 13 38.82745  0.0221681\n## 14 40.99434 -5.6224391\n## 15 40.99434 -5.6224391\n## 16 40.56037 -5.6703725\n## 17 40.99434 -5.6224391\n## 18 40.99434 -5.6224391\n## 19 41.13593  0.9901905\n## 20 41.26155  1.1670507\n## 21 41.22851  0.5461471\n## 22 41.14580  0.8199749\n## 23 41.18341  0.5680564\n## 24 42.07820  1.8203155\n## 25 42.25245  1.8621546\n## 26 41.73767  1.8383666\n## 27 41.62345  2.0013628\ncoordinates <- mutate_all(coordinates, as.numeric)\n\nsp_sf <-\n  ne_countries(scale = \"large\", country = \"Spain\", returnclass = \"sf\") %>%\n  st_transform(crs = 4326)\n\nggplot(sp_sf) +\n  geom_sf() +\n  geom_point(data = coordinates, aes(x = longitude, y = latitude)) +\n  coord_sf(xlim = c(-20, 10), ylim = c(25, 45)) +\n  theme_minimal() +\n  ggtitle(\"Sample of schools in Spain\")"},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"scraping-publicprivate-school-information","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.3 Scraping public/private school information","text":"Suppose working consultant ministry economy, task understanding whether public/private schools correlated income inequality neighborhood level. ’s many public sources nowadays can tell average income given neighborhood can find data already cleaned ready use us internet. However, ’s difficult find list national schools country together information whether ’re public private.saw www.buscocolegio.com, school website also information whether school public private. ’s exactly :“Centro Público” means Public Center. Ig managed build scraper extract information know school either public private. Let’s open web developer tools look specifically tag highlighted clicking “Centro Público”:public/private information ’re interested within <strong> tag. tag HTML means text bold (althought don’t care tag , ’s handy know). Searching simply <strong> tag probably yield dozens matches since <strong> generic. need find unique tag identifies entire box. point, started looking father tags <strong> found 1st ascendant <strong> tag:<div> tag class 'col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px-15 g-py-25' seems unique enough give try. Let’s try:Ok, makes sense. returns 10 nodes <div> present details bigger details box:One strategy extract <strong> tags within tag extract text. find “Centro Público” text , can use regex extract :can see type school right slot 7 vector. Let’s finalize detecting string pattern extracting :go. scaffold code build function extract schools. Let’s move code function accepts one school url loop schools:info hand can merge coordinates school visualize public/private schools location:","code":"\nschool_raw %>%\n  xml_find_all(\"//div[@class='col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px-15 g-py-25']\")## {xml_nodeset (10)}\n##  [1] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [2] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [3] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [4] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [5] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [6] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [7] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [8] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [9] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n## [10] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\ntext_boxes <-\n  school_raw %>%\n  xml_find_all(\"//div[@class='col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px-15 g-py-25']//strong\") %>%\n  xml_text()\n\ntext_boxes##  [1] \"616522706\"                                                         \n##  [2] \"966428835\"                                                         \n##  [3] \"03006839@edu.gva.es\"                                               \n##  [4] \"-\"                                                                 \n##  [5] \"966428836\"                                                         \n##  [6] \"3006839\"                                                           \n##  [7] \"Centro Público\"                                                    \n##  [8] \"Colegio de Educación Infantil y Primaria\"                          \n##  [9] \"GENERALITAT VALENCIANA\"                                            \n## [10] \"Vuelta al cole, inicio y fin de clases, vacaciones y días festivos\"\nsingle_public_private <-\n  text_boxes %>%\n  str_detect(\"Centro\") %>%\n  text_boxes[.]\n\nsingle_public_private## [1] \"Centro Público\"\ngrab_public_private_school <- function(school_link) {\n  Sys.sleep(5)\n  school <- read_html(school_link)\n  text_boxes <-\n    school %>%\n    xml_find_all(\"//div[@class='col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px-15 g-py-25']//strong\") %>%\n    xml_text()\n\n  single_public_private <-\n    text_boxes %>%\n    str_detect(\"Centro\") %>%\n    text_boxes[.]\n\n\n  data.frame(\n    public_private = single_public_private,\n    stringsAsFactors = FALSE\n  )\n}\n\npublic_private_schools <- map_dfr(school_links, grab_public_private_school)\npublic_private_schools##    public_private\n## 1  Centro Privado\n## 2  Centro Público\n## 3  Centro Público\n## 4  Centro Público\n## 5  Centro Privado\n## 6  Centro Público\n## 7  Centro Público\n## 8  Centro Privado\n## 9  Centro Público\n## 10 Centro Público\n## 11 Centro Público\n## 12 Centro Público\n## 13 Centro Público\n## 14 Centro Público\n## 15 Centro Público\n## 16 Centro Público\n## 17 Centro Público\n## 18 Centro Privado\n## 19 Centro Público\n## 20 Centro Público\n## 21 Centro Privado\n## 22 Centro Público\n## 23 Centro Privado\n## 24 Centro Público\n## 25 Centro Público\n## 26 Centro Privado\n## 27 Centro Público\n# Let's translate the public/private names from Spanish to English\nlookup <- c(\"Centro Público\" = \"Public\", \"Centro Privado\" = \"Private\")\npublic_private_schools$public_private <- lookup[public_private_schools$public_private]\n\n# Merge it with the coordinates data\nall_schools <- cbind(coordinates, public_private_schools)\n\n# Plot private/public by coordinates\nggplot(sp_sf) +\n  geom_sf() +\n  geom_point(data = all_schools, aes(x = longitude, y = latitude, color = public_private)) +\n  coord_sf(xlim = c(-20, 10), ylim = c(25, 45)) +\n  theme_minimal() +\n  ggtitle(\"Sample of schools in Spain by private/public\")"},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"extracting-type-of-school","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.4 Extracting type of school","text":"One additional information might also useful know type school: kindergarten, secondary school, primary school, etc. information just next whether school private public:Now, might think can just copy exact code previous extraction one slightly different. problem don’t know advance type school school might . , can’t use regex search (like public/private) something like “Primary” can “Primary school” many types school. search header box:recycle first XPath public/private scraper find exact <div> belongs type school:10 nodes saw . Let’s extract text within one match keyword header “Tipo Centro”:7th node one ’re looking . Let’s subset node can last time: extract strong tag. Note ’ll use XPath .//strong. .//strong means find strong tags . means search tags current selection. write instead //strong, search strong tags entire source code. . front tells XPath search downwards current selection. Let’s write confirmar get correct text:Perfect, ’s type school also website. “Colegio de Educación Infantil y Primaria” means “Pre-school Primary School”. code ready, can wrap function extract schools:Let’s merge previous data schools visualize points private/public well type school:might seen chapter, webscraping creative. Finding somewhat unique tags identify data ’re looking precise science requires tricks using regular expressions, string manipulations handy XPath knowledge. exercises ’ll need creative find clever ways extract additional data schools data.","code":"\ntext_boxes <-\n  school_raw %>%\n  xml_find_all(\"//div[@class='col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px-15 g-py-25']\")\n\ntext_boxes## {xml_nodeset (10)}\n##  [1] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [2] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [3] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [4] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [5] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [6] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [7] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [8] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n##  [9] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\n## [10] <div class=\"col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px ...\nselected_node <- text_boxes %>% xml_text() %>% str_detect(\"Tipo Centro\")\nselected_node##  [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\nsingle_type_school <-\n  text_boxes[selected_node] %>%\n  xml_find_all(\".//strong\") %>%\n  xml_text()\n\nsingle_type_school## [1] \"Colegio de Educación Infantil y Primaria\"\ngrab_type_school <- function(school_link) {\n  Sys.sleep(5)\n  school <- read_html(school_link)\n\n  text_boxes <-\n    school %>%\n    xml_find_all(\"//div[@class='col-6 g-brd-left g-brd-bottom g-theme-brd-gray-light-v3 g-px-15 g-py-25']\")\n\n  selected_node <-\n    text_boxes %>%\n    xml_text() %>%\n    str_detect(\"Tipo Centro\")\n\n  single_type_school <-\n    text_boxes[selected_node] %>%\n    xml_find_all(\".//strong\") %>%\n    xml_text()\n\n\n  data.frame(\n    type_school = single_type_school,\n    stringsAsFactors = FALSE\n  )\n}\n\nall_type_schools <- map_dfr(school_links, grab_type_school)\nall_type_schools##                                     type_school\n## 1                              Escuela Infantil\n## 2                              Escuela Infantil\n## 3                              Escuela Infantil\n## 4                              Escuela Infantil\n## 5                              Escuela Infantil\n## 6  Escuela de Educación Infantil, Casa de Niños\n## 7                              Escuela Infantil\n## 8                              Escuela Infantil\n## 9                              Escuela Infantil\n## 10 Escuela de Educación Infantil, Casa de Niños\n## 11                             Escuela Infantil\n## 12      Escuela Municipal de Educación Infantil\n## 13     Colegio de Educación Infantil y Primaria\n## 14                             Escuela Infantil\n## 15                             Escuela Infantil\n## 16                             Escuela Infantil\n## 17                             Escuela Infantil\n## 18                             Escuela Infantil\n## 19                                       Escola\n## 20                                       Escola\n## 21                                       Escola\n## 22                                       Escola\n## 23                                       Escola\n## 24                                       Escola\n## 25                                       Escola\n## 26                                       Escola\n## 27                                       Escola\nall_schools <- cbind(all_schools, all_type_schools)\n\nggplot(sp_sf) +\n  geom_sf() +\n  geom_point(data = all_schools, aes(x = longitude, y = latitude, color = public_private)) +\n  coord_sf(xlim = c(-20, 10), ylim = c(25, 45)) +\n  facet_wrap(~ type_school) +\n  theme_minimal() +\n  ggtitle(\"Sample of schools in Spain\")"},{"path":"case-study-scraping-spanish-school-locations-from-the-web.html","id":"exercises-4","chapter":"7 Case study: scraping Spanish school locations from the web","heading":"7.5 Exercises","text":"left menu click says “Enseñanzas” (teachings english). want extract unique modality school. whether school remote -presence classes. can find information :school, classes -presence “presencial” Spanish. result data frame one column 27 rows (one school). result like 27 schools:Hint: might handy pick tags XPath using indices //tag[2] get specific index want.left menu click says “Servicios” (services English). want extract services school provides. example, school provides “Comedor escolar” (school cafeteria) “Transporte” (transportation):result look something like 27 schools:main school page, extract address school. information :result like 27 schools:main page website, scroll ’ll find part website like :schools people visited visiting specific school. type network analysis, might want analyze whether people visiting certain type schools interested schools certain properties. , might need actually go school’s websites extract information. Right now won’t , task extract school’s links case wanted automatically scrape schools well. school, ’ll four links similar schools. result like 27 schools:Hint: ’ll want look hyperlink leads school href tag.Combine all_schools previous exercises final data frame school. Final result like 27 schools:","code":"##   mode_school\n## 1  Presencial\n## 2  Presencial\n## 3  Presencial\n## 4  Presencial\n## 5  Presencial\n## 6  Presencial##                      amenities\n## 11                        <NA>\n## 12                        <NA>\n## 13 Comedor Escolar, Transporte\n## 14                     Comedor\n## 15                     Comedor\n## 16                        <NA>##                                                                             addresses\n## 1                  CALZADA S/N-IRIA. 15917. Puente-Cesures, Padrón, A Coruña, GALICIA\n## 2                                 TRABE. 15110. A Trabe, Ponteceso, A Coruña, GALICIA\n## 3                      CL. MEJORADA, S/N. 02690. Alpera, Albacete, CASTILLA-LA MANCHA\n## 4 TR. CANTARRANAS, 3 B. 02610. El Bonillo, Bonillo (El), Albacete, CASTILLA-LA MANCHA\n## 5                                 C/ de la Fuente de Piedra 10. 28018. Madrid, MADRID\n## 6                                    C/ Mayor 29. 28596. Brea de Tajo, Madrid, MADRID## # A tibble: 27 × 4\n##    school_one                                school_two school_three school_four\n##    <chr>                                     <chr>      <chr>        <chr>      \n##  1 /Colegio/detalles-colegio.action?id=1500… /Colegio/… /Colegio/de… /Colegio/d…\n##  2 /Colegio/detalles-colegio.action?id=1502… /Colegio/… /Colegio/de… /Colegio/d…\n##  3 /Colegio/detalles-colegio.action?id=2009… /Colegio/… /Colegio/de… /Colegio/d…\n##  4 /Colegio/detalles-colegio.action?id=2009… /Colegio/… /Colegio/de… /Colegio/d…\n##  5 /Colegio/detalles-colegio.action?id=2804… /Colegio/… /Colegio/de… /Colegio/d…\n##  6 /Colegio/detalles-colegio.action?id=2804… /Colegio/… /Colegio/de… /Colegio/d…\n##  7 /Colegio/detalles-colegio.action?id=2806… /Colegio/… /Colegio/de… /Colegio/d…\n##  8 /Colegio/detalles-colegio.action?id=2805… /Colegio/… /Colegio/de… /Colegio/d…\n##  9 /Colegio/detalles-colegio.action?id=2806… /Colegio/… /Colegio/de… /Colegio/d…\n## 10 /Colegio/detalles-colegio.action?id=2805… /Colegio/… /Colegio/de… /Colegio/d…\n## # … with 17 more rows## # A tibble: 27 × 11\n##    latitude longitude public_private type_school mode_school amenities addresses\n##       <dbl>     <dbl> <chr>          <chr>       <chr>       <chr>     <chr>    \n##  1     42.7     -8.66 Private        Escuela In… Presencial  <NA>      CALZADA …\n##  2     43.2     -8.89 Public         Escuela In… Presencial  <NA>      TRABE. 1…\n##  3     39.0     -1.23 Public         Escuela In… Presencial  <NA>      CL. MEJO…\n##  4     39.2     -1.62 Public         Escuela In… Presencial  <NA>      TR. CANT…\n##  5     40.4     -3.64 Private        Escuela In… Presencial  <NA>      C/ de la…\n##  6     40.2     -3.11 Public         Escuela de… Presencial  <NA>      C/ Mayor…\n##  7     40.4     -3.70 Public         Escuela In… Presencial  Horario … JOSÉ ABA…\n##  8     40.3     -3.52 Private        Escuela In… Presencial  <NA>      C/ Feder…\n##  9     40.5     -3.37 Public         Escuela In… Presencial  <NA>      C/ José …\n## 10     40.6     -3.45 Public         Escuela de… Presencial  <NA>      C/ Subid…\n## # … with 17 more rows, and 4 more variables: school_one <chr>,\n## #   school_two <chr>, school_three <chr>, school_four <chr>"},{"path":"automating-web-scraping-scripts.html","id":"automating-web-scraping-scripts","chapter":"8 Automating Web Scraping Scripts","heading":"8 Automating Web Scraping Scripts","text":"two types webscraping: one-scrapings frequent scrapings. first one chapter TODO primer webscraping. create program scrape something . common approach ’ve done time throughout book. second one involves building scrapers know used frequently. Many examples come mind: scrapers extract news frequent basis, scrapers extract temperature data daily basis, scrapers collect prices groceries supermarket website . scrapers designed run frequent intervals get timely information.one-scrapers, material book chapter enough. However, frequent scrapings need new tools strategies. asked can automate script? automating mean, example, run script every Thursday 08:00 PM. chapter focuses scheduling programs run whenever want . might need collect data website changing constantly request data API frequent intervals (topic API’s second part book) two cases don’t want manually running house 3 morning run program. chapter make sure don’t .Scheduling scripts different operating systems. chapter focus solely scheduling scripts Linux MacOS. Windows users recommended search ‘Windows Task Scheduler’ online find can schedule R programs.","code":""},{"path":"automating-web-scraping-scripts.html","id":"the-scraping-program","chapter":"8 Automating Web Scraping Scripts","heading":"8.1 The Scraping Program","text":"first thing need scraping program. Let’s recycle one chapter TODO xpath loaded example newspaper “El País”. scraper count number articles sections available newspaper “El País”. R script parse “El País” website, extract number articles per section collect everything data frame. ’s look like:11 sections, one respective number articles. personal research project , ’re interested collecting counts every day three different times day. idea try map newspapers shift writing efforts different sections. hypothesis newspapers certain ideologies might give weight certain sections others give weight sections.scraper precisely ’s missing step: saving data. logic want achieve something like :first time scraper run, save csv file count sectionsIf CSV count section exists, open CSV file append newest data current time stampThis approach add rows new counts every time scraper run. plotting time stamp count sections ’ll able visualize counts change time. , add new section code save results CSV file time stamp current date. Let’s add section save data:summary, script read website “El País”, count number sections save results CSV file ~/newspaper/newspaper_section_counter.csv. directory still doesn’t exist, ’ll create first.","code":"\n# Load all our libraries\nlibrary(scrapex)\nlibrary(xml2)\nlibrary(magrittr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(readr)\n\n# If this were being done on the real website of the newspaper, you'd want to\n# replace the line below with the real link of the website.\nnewspaper_link <- elpais_newspaper_ex()\nnewspaper <- read_html(newspaper_link)\n\nall_sections <-\n  newspaper %>%\n  # Find all <section> tags which have an <article> tag\n  # below each <section> tag. Keep only the <article>\n  # tags which an attribute @data-dtm-region.\n  xml_find_all(\"//section[.//article][@data-dtm-region]\")\n\nfinal_df <-\n  all_sections %>%\n  # Count the number of articles for each section\n  map(~ length(xml_find_all(.x, \".//article\"))) %>%\n  # Name all sections\n  set_names(all_sections %>% xml_attr(\"data-dtm-region\")) %>%\n  # Convert to data frame\n  enframe(name = \"sections\", value = \"num_articles\") %>%\n  unnest(num_articles)\n\nfinal_df## # A tibble: 11 × 2\n##    sections                                   num_articles\n##    <chr>                                             <int>\n##  1 portada_apertura                                      5\n##  2 portada_arrevistada                                   1\n##  3 portada_tematicos_science,-tech-&-health              5\n##  4 portada_tematicos_business-&-economy                  2\n##  5 portada_tematicos_undefined                           1\n##  6 portada_branded_                                      2\n##  7 portada_arrevistada_culture                           5\n##  8 portada_tematicos_work-&-lifestyle                    3\n##  9 portada_arrevistada                                   1\n## 10 portada_tematicos_celebrities,-movies-&-tv            4\n## 11 portada_tematicos_our-selection                       4\nlibrary(scrapex)\nlibrary(xml2)\nlibrary(magrittr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(readr)\n\nnewspaper_link <- elpais_newspaper_ex()\n\nall_sections <-\n  newspaper_link %>%\n  read_html() %>%\n  xml_find_all(\"//section[.//article][@data-dtm-region]\")\n\nfinal_df <-\n  all_sections %>%\n  map(~ length(xml_find_all(.x, \".//article\"))) %>%\n  set_names(all_sections %>% xml_attr(\"data-dtm-region\")) %>%\n  enframe(name = \"sections\", value = \"num_articles\") %>%\n  unnest(num_articles)\n\n# Save the current date time as a column\nfinal_df$date_saved <- format(Sys.time(), \"%Y-%m-%d %H:%M\")\n\n# Where the CSV will be saved. Note that this directory\n# doesn't exist yet.\nfile_path <- \"~/newspaper/newspaper_section_counter.csv\"\n\n# *Try* reading the file. If the file doesn't exist, this will silently save an error\nres <- try(read_csv(file_path, show_col_types = FALSE), silent = TRUE)\n\n# If the file doesn't exist\nif (inherits(res, \"try-error\")) {\n  # Save the data frame we scraped above\n  print(\"File doesn't exist; Creating it\")\n  write_csv(final_df, file_path)\n} else {\n  # If the file was read successfully, append the\n  # new rows and save the file again\n  rbind(res, final_df) %>% write_csv(file_path)\n}"},{"path":"automating-web-scraping-scripts.html","id":"the-terminal","chapter":"8 Automating Web Scraping Scripts","heading":"8.2 The Terminal","text":"use cron, general executing R scripts, ’ll need get familiar terminal. Let’s open terminal. Linux, can pressing keys CTRL + ALT + t together. MacOS click Launchpad icon Dock, type Terminal search field click Terminal. operating systems see window like (exactly similar) one pop :terminal. allows things like create directories files just computer code. Let’s create directory ’ll save CSV file R script performs scraping. create directory, use command mkdir stands makedirectory. Let’s create mkdir ~/newspaper/:Great, directory created. Now copy R script wrote save ~/newspaper/. Save newspaper_scraper.R. R file within ~/newspaper/ called newspaper_scraper.R. can confirm typing ls ~/newspaper/ list files/directories inside ~/newspaper/. see output like one:script saved successfully inside directory. Let’s switch ‘directory’ ~/newspaper/ terminal. terminal can change directories cd command, stands changedirectory, followed path want switch . case, cd ~/newspaper/:can see third line image, now appears ~/newspaper blue, denoting directory right now. execute R script terminal can Rscript command followed file name. case Rscript newspaper_scraper.R. Let’s run :first lines show printing package loading finally see print statement added file doesn’t exit: File exist; Creating . opened CSV computer see sheet like one:Great, scraper works! half job done. Now need come way execute Rscript ~/newspaper/newspaper_scraper.R schedule.","code":"ls ~/newspaper/\n# newspaper_scraper.R"},{"path":"automating-web-scraping-scripts.html","id":"cron-your-scheduling-friend","chapter":"8 Automating Web Scraping Scripts","heading":"8.3 cron, your scheduling friend","text":"Luckily , program already exists. ’s called cron allows run script schedule. Ubuntu, can install cron :MacOS can install :cases, install successful, able confirm works crontab -l:output means scheduled scripts computer. schedule script cron need two things: command execute schedule. command execute already know, ’s Rscript ~/newspaper/newspaper_scraper.R. specifying schedules, cron particular syntax. work? Let’s take look:bottom image see 5 *. text tells * stands . order, represent minutes, hours, day month, month day week.* fact placeholder signal whenever * means command repeated instance place holder. Complicated? Look example:writing * * * * *, ’re scheduling program run every minute, every hour, every day month, every month every day week. Say changed schedule run every 30 minutes hour day month day week. sounds awfully complicated say. simpler way say script run every 30 minutes date parameters *, means every unit parameters.\nschedule look like? know first slot minutes can write 30 first slot:’re effectively scheduling something run minute 30 hour, day, month, day week (day week, last slot, clashes schedule third slot day month day matching either day month, day week, shall matched).Now know * represents date parameter can start develop interesting schedules. example, Wednesdays third day week (start counting Monday), can run schedule every 30 minutes Wednesdays:might want run scraper 05:30 Saturday Sunday:expression reads like : run 30th minute 5th hour every month Saturday Sunday (6th 7th day week). simple rules can take long way building scraper.Let’s say wanted run newspaper scraper every 4 hours, every day, look like? sounds bit different ’ve done . syntax ’ve discussed specifically writes day / hour / minute want scraper . way saying, regardless day / hour / minute, run scraper every X hours. cron additional tricks. wanted run scraper every every 4 hours write like :date parameter want make recurrent, add / frequency want. instead wanted run scraper every 4 hours, every 2 days, write something like :. ’s basics cron. simple rules allow go far scheduling scripts scrapers APIs. details enough get example running.Let’s schedule newspaper scraper run every minute, just make sure works. get messy ’ll append results CSV file continuously, filling CSV repeated data. However, give proof script running schedule. want run every minute, cron expression * * * * *, simplest expression.save cron expression type crontab -e terminal. first time using crontab see something like :allow pick editor want use editing cron schedule. Pick whichever options points nano, easiest one. choosing editor (prompted pick editor), continue open new file like one:file write schedule command want cron run. Either scroll mouse hit scroll key bottom right keyboard go last line editor. last line, need write cron schedule expression command want execute:finish writing , terminal look something like :exit cron interface, follow steps:Hit CTRL X (exiting cron interface)prompt save file. PressY save .Press enter save cron schedule file name ., back terminal cron job saved. Nothing special happening moment. Wait two three minutes open CSV file . find records duplicated different time stamp:cron schedule worked expected! see different time stamps date_saved column, reflecting scraper run every minute. close chapter, remember remove schedule Rscript command cron. Enter crontab -e, go last line, delete text exit cron interface instructions detailed .","code":"sudo apt-get update\nsudo apt-get install cronbrew install --cask cron* * * * *30 * * * *30 * * * 330 5 * * 6,71 */4 * * *1 */4 * * */2* * * * * Rscript ~/newspaper/newspaper_scraper.R"},{"path":"automating-web-scraping-scripts.html","id":"conclusion-2","chapter":"8 Automating Web Scraping Scripts","heading":"8.4 Conclusion","text":"framework building scraper, testing scheduling run frequent intervals powerful. commands can automate program (fact, R programming language program). However, approach limitations. computer needs turned time order cron schedule run. ’re school project ’s possible, might get around using computer. However, demanding scrapings (lots data, frequent intervals) ’s almost always better idea run scraper server.Launching server running scraper scope chapter keep mind building scrapers work. many tutorials internet.cron can also become complex schedule patterns difficult. ’s bunch resources can help internet. worked :Crontab GuruCron tutorial","code":""},{"path":"automating-web-scraping-scripts.html","id":"exercises-5","chapter":"8 Automating Web Scraping Scripts","heading":"8.5 Exercises","text":"cron expression run every 15 minutes Monday, Wednesday Friday February?cron expression run every 15 minutes Monday, Wednesday Friday February?R packaged called cronR allows setup cron schedules within R. Can replicate chapter using cronR package? package documentation can found .R packaged called cronR allows setup cron schedules within R. Can replicate chapter using cronR package? package documentation can found .Can write script empties trash folder personal computer every Monday 11AM? Write scraper well cron expression. Remember remove cron deploying avoid unexpected files deleted.Can write script empties trash folder personal computer every Monday 11AM? Write scraper well cron expression. Remember remove cron deploying avoid unexpected files deleted.","code":""},{"path":"scraping-javascript-based-website.html","id":"scraping-javascript-based-website","chapter":"9 Scraping JavaScript based website","heading":"9 Scraping JavaScript based website","text":"Even techniques ’ve covered book, websites can’t scraped traditional way. mean? websites content hidden behind JavaScript code can’t seen simply reading HTML code. website built interactivity, meaning parts revealed interact . read HTML website, won’t able see data want. usual, let’s motivate chapter example.European Social Survey (ESS) academically driven cross-national survey conducted across Europe since establishment 2001. Every two years, face--face interviews conducted newly selected, cross-sectional samples. survey measures attitudes, beliefs behavior patterns diverse populations thirty nations. website ‘Data Portal’ allows pick variables asked questionnaire, pick country/years data want download custom data set . can find data portal . looks like:developer’s tools (CTRL-SHIT-C), can see HTML code behind website. Let’s say want extract variable names associated category questions. ‘Media use trust’ several question names labels belong category:However, unless click drop menu, questions won’t displayed source code website. , right now spent 10 minutes searching source code developer’s tools right, won’t find names variables anywhere:variable names won’t found anywhere. might think, well ’s easy solve: ’ll click drop menu use new generated link (data displayed) read read_html. issue URL website whether click drop menu:URL, despite clicked ‘Media use trust’ variable names displayed. means whenever use read_html link, variable labels won’t displayed won’t able gather data. ’s motivation behind Selenium. ’s objective able scrape websites dynamic interactive impossible using traditional URLs source.","code":""},{"path":"scraping-javascript-based-website.html","id":"introduction-to-rselenium","chapter":"9 Scraping JavaScript based website","heading":"9.1 Introduction to RSelenium","text":"Selenium software program use ‘mimic’ human. Yes, ’s right. Selenium literally open browser click parts page human. ’ve clicked certain parts website, can use knowledge XPath, HTML xml2 extract data need.example ’ll use chapter blog statistician Andrew Gelman. According Rohan Alexander:Gelman’s blog—Statistical Modeling, Causal Inference, Social Science—launched 2004—go-place fun mix somewhat-nerdy statistics-focused content. first post promised ‘…report recent research ongoing half-baked ideas, including … Bayesian statistics, multilevel modeling, causal inference, political science.’ 17 years , site much kept promise.’ll use blog explore topics discussed world statistics last 17 years. website need scraped using Selenium can scraped just reading HTML code. However, websites need use Selenium can’t saved locally doomed change time, entails risk making chapter unusable future. reason going perform scraping using traditional example highlighting JavaScript based website.Let’s load packages ’ll use example:described , Selenium tries mimic human. literally means need open browser. RSelenium rsDriver function. function initializes browser opens us:last line starts withBEGIN ’ll see options browser. example, browserName browserVersion. important thing , aside output, see browser opened computer completely empty:RSelenium opened browser don’t need touch. Everything browser performed using object remDr, browser initiated. remDr property called client can navigate website. Let’s open Andrew Gelman’s blog browser:look browser see blog opened :effective way navigate RSelenium use XPath find parts website want perform action use methods click, send submit something website. example, suppose wanted perform natural language processing posts last 17 years blog posts. first ’d like click blog post name extract text.","code":"\nlibrary(RSelenium)\nlibrary(scrapex)\nlibrary(xml2)\nlibrary(magrittr)\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(tidyr)\n\nblog_link <- gelman_blog_ex()\nremDr <- rsDriver(port = 4445L)## checking Selenium Server versions:\n## BEGIN: PREDOWNLOAD\n## BEGIN: DOWNLOAD\n## BEGIN: POSTDOWNLOAD\n## checking chromedriver versions:\n## BEGIN: PREDOWNLOAD\n## BEGIN: DOWNLOAD\n## BEGIN: POSTDOWNLOAD\n## checking geckodriver versions:\n## BEGIN: PREDOWNLOAD\n## BEGIN: DOWNLOAD\n## BEGIN: POSTDOWNLOAD\n## checking phantomjs versions:\n## BEGIN: PREDOWNLOAD\n## BEGIN: DOWNLOAD\n## BEGIN: POSTDOWNLOAD\n## [1] \"Connecting to remote server\"\n## $acceptInsecureCerts\n## [1] FALSE\n\n## $browserName\n## [1] \"firefox\"\n\n## $browserVersion\n## [1] \"106.0\"\n\n## $`moz:accessibilityChecks`\n## [1] FALSE\n\n## $`moz:buildID`\n## [1] \"20221010110315\"\n\n## $`moz:geckodriverVersion`\n## [1] \"0.32.0\"\n\n## $`moz:headless`\n## [1] FALSE\n\n## $`moz:platformVersion`\n## [1] \"5.4.0-39-generic\"\n\n## $`moz:processID`\n## [1] 271886\n\n## $`moz:profile`\n## [1] \"/tmp/rust_mozprofile5ANm8J\"\n\n## $`moz:shutdownTimeout`\n## [1] 60000\n\n## $`moz:useNonSpecCompliantPointerOrigin`\n## [1] FALSE\n\n## $`moz:webdriverClick`\n## [1] TRUE\n\n## $`moz:windowless`\n## [1] FALSE\n\n## $pageLoadStrategy\n## [1] \"normal\"\n\n## $platformName\n## [1] \"linux\"\n\n## $proxy\n## named list()\n\n## $setWindowRect\n## [1] TRUE\n\n## $strictFileInteractability\n## [1] FALSE\n\n## $timeouts\n## $timeouts$implicit\n## [1] 0\n\n## $timeouts$pageLoad\n## [1] 300000\n\n## $timeouts$script\n## [1] 30000\n\n\n## $unhandledPromptBehavior\n## [1] \"dismiss and notify\"\n\n## $webdriver.remote.sessionid\n## [1] \"a3585bf8-9c4a-462e-a74d-b9d6b35daff7\"\n\n## $id\n## [1] \"a3585bf8-9c4a-462e-a74d-b9d6b35daff7\"\nremDr$client$navigate(prep_browser(blog_link))"},{"path":"scraping-javascript-based-website.html","id":"navigating-a-website-in-rselenium","chapter":"9 Scraping JavaScript based website","heading":"9.2 Navigating a website in RSelenium","text":"Let’s focus first blog post. Let’s find XPath expression return position first blog post name. ’re looking <> tag equivalent ‘clicking’ blog post. know ’s contains text blog name:validate XPath nothing prevents us reading source code trying . point read read_html tried several XPath managed land one ’re interested :Let’s explain . look source image , ’ll see <> tag ’s h1 tag distinctive class entry-title. came following XPath:entire document (//)Find h1 tags class entry-titleThen find <> tags h1 tagWrap everything () collect resultKeep first result [1]now know XPath validated need locate RSelenium click . Everything RSelenium done using property client, many methods associated browser. main methods ’ll using findElement clickElement. findElement used position pointer browser exactly want clickElement click . Let’s use XPath move pointer blog post click :Now look browser:scroll ’ll see comments:clicked first entry browser entered post. Great! simple strategy findElement clickElement can reused needs. strategy simple:Find XPath want clickUse XPath findElementUse clickElement click itOne natural follow question actually extract HTML code extract stuff us analyze. , ’ve opened browser clicked something. actual fun extract data stuff . RSelenium property getPageSource(). property returns list need extract first slot:getPageSource returns list ’s use subsetting characters [[1]] extract first slot. page_source string contains HTML source code website.","code":"\nblog_link %>%\n  read_html() %>%\n  xml_find_all(\"(//h1[@class='entry-title']//a)[1]\")## {xml_nodeset (1)}\n## [1] <a href=\"https://statmodeling.stat.columbia.edu/2022/10/16/mit-built-a-th ...\n# Since we'll use the client a lot let's save it on a separate object\ndriver <- remDr$client\ndriver$findElement(value = \"(//h1[@class='entry-title']//a)[1]\")$clickElement()\npage_source <- driver$getPageSource()[[1]]"},{"path":"scraping-javascript-based-website.html","id":"bringing-the-source-code","chapter":"9 Scraping JavaScript based website","heading":"9.3 Bringing the source code","text":"HTML code string, ’re familiar ground. can use read_html read .Let’s assume want find categories associated post. categories bottom post:’ll see right categories post <footer> tag class entry-meta. actual categories <> tag href attribute word category. can build XPath extract like :// entire documentFind footer tag class entry-meta: footer[@class='entry-meta']//Find tags contain word category: [contains(@href, 'category')]Let’s extract categories using XPath:post Zombies. Let’s also extract date order time stamp categories used time:Note use parse_date_time time format (% symbols) convert string date/time R. information can create clean data frame entry dates respective categories list column:Great, information clean ready recycle code function loop blog posts. , need one final thing: go back main page blog. Don’t forget browser still open whatever new moves make using browser starting left . Luckily, RSelenium makes simple: goBack attribute. Let’s go back:browser now back main page:","code":"\nhtml_code <-\n  page_source %>%\n  read_html()\ncategories_first_post <-\n  html_code %>%\n  xml_find_all(\"//footer[@class='entry-meta']//a[contains(@href, 'category')]\") %>%\n  xml_text()\n\ncategories_first_post## [1] \"Zombies\"\nentry_date <-\n  html_code %>%\n  xml_find_all(\"//time[@class='entry-date']\") %>%\n  xml_text() %>%\n  parse_date_time(\"%B %d, %Y %I:%M %p\")\n\nentry_date## [1] \"2022-10-16 09:31:00\"\nfinal_res <- tibble(entry_date = entry_date, categories = list(categories_first_post))\nfinal_res## # A tibble: 1 × 2\n##   entry_date          categories\n##   <dttm>              <list>\n## 1 2022-10-16 09:31:00 <chr [1]>\ndriver$goBack()"},{"path":"scraping-javascript-based-website.html","id":"scaling-rselenium-to-many-websites","chapter":"9 Scraping JavaScript based website","heading":"9.4 Scaling RSelenium to many websites","text":"ready, can move code function loop blog posts page:code runs, open browser enjoy show. browser start working self ghost scrolling clicking blog post. finishes, ’ll list categories last 20 blog posts. Let’s combine get count used categories:","code":"\ncollect_categories <- function(page_source) {\n  # Read source code of blog post\n  html_code <-\n    page_source %>%\n    read_html()\n\n  # Extract categories\n  categories_first_post <-\n    html_code %>%\n    xml_find_all(\"//footer[@class='entry-meta']//a[contains(@href, 'category')]\") %>%\n    xml_text()\n\n  # Extract entry date\n  entry_date <-\n    html_code %>%\n    xml_find_all(\"//time[@class='entry-date']\") %>%\n    xml_text() %>%\n    parse_date_time(\"%B %d, %Y %I:%M %p\")\n\n  # Collect everything in a data frame\n  final_res <- tibble(entry_date = entry_date, categories = list(categories_first_post))\n  final_res\n}\n\n# Get all number of blog posts on this page\nnum_blogs <-\n  blog_link %>%\n  read_html() %>%\n  xml_find_all(\"//h1[@class='entry-title']//a\") %>%\n  length()\n\nblog_posts <- list()\n\n# Loop over each post, extract the categories and go back to the main page\nfor (i in seq_len(num_blogs)) {\n  print(i)\n  # Go to the next blog post\n  xpath <- paste0(\"(//h1[@class='entry-title']//a)[\", i, \"]\")\n  Sys.sleep(2)\n  driver$findElement(value = xpath)$clickElement()\n\n  # Get the source code\n  page_source <- driver$getPageSource()[[1]]\n\n  # Grab all categories\n  blog_posts[[i]] <- collect_categories(page_source)\n\n  # Go back to the main page before next iteration\n  driver$goBack()\n}\ncombined_df <- bind_rows(blog_posts)\n\ncombined_df %>%\n  unnest(categories) %>%\n  count(categories) %>%\n  arrange(-n)## # A tibble: 16 × 2\n##    categories                   n\n##    <chr>                    <int>\n##  1 Bayesian Statistics          7\n##  2 Zombies                      7\n##  3 Miscellaneous Statistics     5\n##  4 Political Science            5\n##  5 Teaching                     5\n##  6 Sociology                    4\n##  7 Statistical computing        4\n##  8 Causal Inference             3\n##  9 Economics                    3\n## 10 Sports                       3\n## 11 Miscellaneous Science        2\n## 12 Stan                         2\n## 13 Art                          1\n## 14 Decision Theory              1\n## 15 Jobs                         1\n## 16 Public Health                1"},{"path":"scraping-javascript-based-website.html","id":"filling-out-forms","chapter":"9 Scraping JavaScript based website","heading":"9.5 Filling out forms","text":"RSelenium allows much just click links; pretty much anything can browser can RSelenium. look methods driver:can sorts things like taking screenshot website, grabbing URL, delete cookies much . can’t cover everything RSelenium can can focus common ones.One common thing ’ll also want learn fill form. ’s common website ask information showing website fill search bar (like eCommerce website Amazon) obtain results. tasks manually type content form-like box.blog example contains one nice example can recycle filling forms. Whenever want post comment blog’s posts need fill form comment text, author name, email author’s website ’re allowed publish. see , need go blog post scroll . Let’s go first post:scroll see comments section:left ’ll see form. text areas corresponding tag right. one author form highlighted right. ’ll see id attribute author. Similarly, see tiny bit , email input id set email. Using traditional XPath syntax can probably navigate browser specific tag. However, RSelenium nice tricks can make even easier us. findElement assumes ’ll using XPath, previously, also allows specify certain attributes without using XPath. wanted go tag id set author, like :RSelenium also supports attributes ‘name’, ‘tag name’, ‘class name’, ‘link text’ even ‘partial link text’. code leave us author form. use something like $clickElement() want input text. use $sendKeysToElement() sends text provide inside () input:look browser ’ll see author form now filled “Data Harvesting Bot”:can thing email id:find email form filled:point, need fill text comment don’t want actually send comment website. Instead, ’ll show click ‘Post Comment’ button (since haven’t provided comment text, clicking post comment button won’t publish comment). id tag submit button set submit source code can click :post won’t published, instead ’ll see pop saying “Please fill field”, saying need provide comment post behind publishing :just example fill text forms click . done scrape posts locally scrapex (might possible).","code":"\ndriver$Navigate(\"http://somewhere.com\")\ndriver$goBack()\ndriver$goForward()\ndriver$refresh()\ndriver$getTitle()\ndriver$getCurrentUrl()\ndriver$getStatus()\ndriver$screenshot(display = TRUE)\ndriver$getAllCookies()\ndriver$deleteCookieNamed(\"PREF\")\ndriver$switchToFrame(\"string|number|null|WebElement\")\ndriver$getWindowHandles()\ndriver$getCurrentWindowHandle()\ndriver$switchToWindow(\"windowId\")\ndriver$findElement(value = \"(//h1[@class='entry-title']//a)[1]\")$clickElement()\ndriver$findElement(using = \"id\", \"author\")\ndriver$findElement(using = \"id\", \"author\")$sendKeysToElement(list(\"Data Harvesting Bot\"))\ndriver$findElement(using = \"id\", \"email\")$sendKeysToElement(list(\"fake-email@gmail.com\"))\ndriver$findElement(using = \"id\", \"submit\")$clickElement()"},{"path":"scraping-javascript-based-website.html","id":"summary","chapter":"9 Scraping JavaScript based website","heading":"9.6 Summary","text":"find sorts examples extract data using RSelenium nearly scraping needs solved combining functions ’ve used . summary tools ’ve covered ’ll need Selenium scraping.Connect open browserNavigate website, refresh go back previous website:Find part website using XPath click :Extract HTML source current websiteFill text pointClose browser server","code":"\nremDr <- rsDriver(port = 4445L)\ndriver <- remDr$client\ndriver$navigate(\"somewebsite.com\")\ndriver$refresh()\ndriver$goBack()\n# With XPath\ndriver$findElement(value = \"(//h1[@class='entry-title']//a)[1]\")$clickElement()\n\n# Or attribute=value instead of XPath\ndriver$findElement(using = \"id\", \"submit\")$clickElement()\ndriver$getPageSource()[[1]]\n# With XPath\ndriver$findElement(value = \"(//h1[@class='entry-title']//a)[1]\")$sendKeysToElement(list(\"Data Harvesting Bot\"))\n\n# With attribut=value\ndriver$findElement(using = \"id\", \"author\")$sendKeysToElement(list(\"Data Harvesting Bot\"))\ndriver$close()\nremDr$server$stop()"},{"path":"scraping-javascript-based-website.html","id":"exercises-6","chapter":"9 Scraping JavaScript based website","heading":"9.7 Exercises","text":"Building ‘extract categories posts’ example, extend extract blog posts last 20 pages blog posts. ’ll need click ‘Older posts’ see previous page blog posts:use images grown time blog? ’s decide many posts past want go back. Hint: count number <img> tags post.use images grown time blog? ’s decide many posts past want go back. Hint: count number <img> tags post.Gelman big proponent using Bayesian methods Frequentist methods statistics. Can plot frequency usage word “Bayesian” time? peak moment history blog?Gelman big proponent using Bayesian methods Frequentist methods statistics. Can plot frequency usage word “Bayesian” time? peak moment history blog?","code":""},{"path":"ethical-issues-in-web-scraping.html","id":"ethical-issues-in-web-scraping","chapter":"10 Ethical issues in Web Scraping","heading":"10 Ethical issues in Web Scraping","text":"Although quick get running, web scraping delicate issue. ’s delicate involves grabbing information using purposes. respects might even contrary terms services website. also involves cluttering servers website potentially many requests, making functioning website less optimal. chapter ’ll describe affect website scrape , can avoid problems website owners figure indeed website allows scrape information.","code":""},{"path":"ethical-issues-in-web-scraping.html","id":"make-your-scraper-sleep","chapter":"10 Ethical issues in Web Scraping","heading":"10.1 Make your scraper sleep","text":"Whenever run scraping script, program makes request website. request means ask servers behind website send information. learned scraping worked like , completely shocked. thought scraping like copying content website local computer. Nothing harmless , right?Well, surprise, making request many times every day without knowing . Scraping entering website browser. moment hit enter go website information actually displayed time takes server behind website return content. operation important website. involves requesting information, waiting server return rendering browser. human slow enough server handle requests needs imagine human tried enter website 5 times per second continuously 48 hours. amounts 864,000 requests. ’s lot.Big websites Google Amazon enough servers throttle handle millions requests per second content internet . reason, ’s important whenever make request website (calling read_html read_xml), add system sleep scraper. R can Sys.sleep(amount_of_seconds) amount_of_seconds amount seconds want sleep making request.scraping website , making single request, adding system sleep matter. want avoid making many requests shorts amount time. example, example chapter TODO Spanish school scraped information several different schools. school scraped, meant making request website. ’s perfect example want sleep making request. R code, skeleton code look something like system sleep:create single function works well scraping single school. launch scrape different schools, add Sys.sleep(5) scraping school. way, making new request, ’ll let servers rest avoid many requests short amount time.many seconds wait? Sometimes robotstxt file (see section ) tell many seconds per scrape wait. , ’s real estimate. Depending many requests make ’ll want lower number seconds 2 3. Multiplying total number websites ’ll scrape number seconds ’ll sleep give rough estimate much time program last.rule thumb, ’s always good idea limit scraping non-working hours evening. can help reduce chances collapsing website since fewer people visiting websites evening.","code":"\nlibrary(scrapex)\n\n# List of links to make a request\nschool_links <- spanish_schools_ex()\n\n# List where we will save the information for each link\nall_schools <- list()\n\nsingle_school_scraper <- function(single_link) {\n  # Before making a request, sleep 5 seconds\n  Sys.sleep(5)\n\n  # Perform some scraping\n}\n\n# Loop over each link to make a request\nfor (single_link in school_links) {\n  # Save results in a list\n  all_schools[[single_link]] <- single_school_scraper(single_link)\n}"},{"path":"ethical-issues-in-web-scraping.html","id":"terms-of-services","chapter":"10 Ethical issues in Web Scraping","heading":"10.2 Terms of services","text":"General Data Protection Regulation (GDPR) came effect Europe, websites needed make sure every user agreed terms services. terms services lengthy contain lot information website can data. However, also contains information can data. internet users, simply doesn’t matter. ’re building scraping program, however, important.Whenever intend scrape website, make sure read terms services. website clearly states web scraping allowed, must respect . example, Facebook clause specifically automated data collection:collect users’ content information, otherwise access Facebook, using automated means (harvesting bots, robots, spiders scrapers) without prior permission.website explicitly prohibits scraping, . Let make clear : terms services forbids scraping website, . can legal consequences.’s clear terms services, contact website receive written confirmation website.","code":""},{"path":"ethical-issues-in-web-scraping.html","id":"copying-information","chapter":"10 Ethical issues in Web Scraping","heading":"10.3 Copying information","text":"Even website allows user scrape contents, might preferences sections website can scrape forbidden. ’s standard file called robots.txt nearly website internet tells want parts website can scraped. robots.txt just convenient form website tell URLs can scrape; enforce anything block way. cases, follow guidelines robots.txt.robots.txt file websites located main URL. example, robots.txt Facebook www.facebook.com/robots.txt. Similarly, robots.txt Google can found google.com/robots.txt looks like :documents URL Google website explicitly tells ones allowed disallowed. section ’ll use robotstxt R package makes easy tell whether website can scrapable. example, can figure can scrape landing page Wikipedia :TRUE statement tell us can . Let’s see can scrape Facebook homepage:Using paths_allowed can provide link website ’ll automatically extract robots.txt figure can scrape . scraping website, check whether URL want scrape allowed.Another thing aware robotstxt files often contain field Crawl-delay, suggesting time wait requests. keep mind Sys.sleep requests.","code":"\nlibrary(robotstxt)\npaths_allowed(\"https://wikipedia.org\")\npaths_allowed(\"https://facebook.com\")"},{"path":"ethical-issues-in-web-scraping.html","id":"identifying-yourself","chapter":"10 Ethical issues in Web Scraping","heading":"10.4 Identifying yourself","text":"Even website allows scrape URL ’re , need extra careful identify . means need give website clear identification making requests. way website can contact find ’s something wrong requests, even directly block . Remember times: ’re scraping data polite grabbing data. owner data considers don’t want give data, ’re right .identify need something called “User-Agent”. User-Agent contains information computer browser. can find user agent googling “user agent?”. Google directly tell :hand, just need tell scraper incorporate request. ? httr package. code :Notice added name / email user agent? often websites can contact case believe ’m breaking terms services want know purpose scraper. case, ’s just way polite. R user agent can set top script. need include anywhere else inside scraper inside loop scrapes many websites; user agent set globally reused requests.","code":"\nlibrary(httr)\n\nset_config(\n  user_agent(\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:105.0) Gecko/20100101 Firefox/105.0; Jorge Cimentada / cimentadaj@gmail.com\")\n)"},{"path":"introduction-to-rest-apis.html","id":"introduction-to-rest-apis","chapter":"11 Introduction to REST APIs","heading":"11 Introduction to REST APIs","text":"far book ’ve covered subtle art web scraping. art weird sense ’s structured. scrape stuff come unusual tricks gather data websites. also careful often might impact latency website due excessive requests. Moreover, gather data, need apply ninja-style tricks clean results. Overall, complicated process data (website) meant scrape .websites, love prevent scrapers scraping website: nearly cases, bring negative impact website either excessive requests possible slow internal servers reusing data economic benefit.steps start hint towards unilateral effort web scraper. scrape master one interested ’s effort scraping seems like traveling jungle obstacles rather walk park.REST APIs come . web scraping somewhat like ambush website, REST APIs two party contract. REST Application Programming Interfaces (API) just fancy name structured way users gather data internal servers website (web scraping) controlled environment user agrees rules don’t affect negatively data performance servers company.Users careful distinguishing APIs REST APIs. APIs interface program: example, package dplyr, way decisions taken function accepts arguments, functions exported, functions interact (general design package works) called API package, programming interface. contrast, REST API data transfer design users share data internet. chapter ’ll discussing exclusively REST APIs.REST APIs natural answer companies figured web scraping affecting business negatively. Companies decided create custom ‘portal’ user, authenticating accepting agreements using ‘portal’, can access much data can see companies website (necessarily ’re looking , prerogative). However, ‘portal’ website can go download data drop menus clicking buttons: ’s series URLs need consult programmatically.might ask , well, access REST API? Can show one? Yes! scrapex package contains several REST APIs designed specifically educational purposes book. First, let’s launch REST API locally:see print local URL __docs__ REST API? local URL. won’t work computer, don’t try go website: ’ll get URL doesn’t exit. need install scrapex package run api_amazon() function, copy local API path onto web browser ’ll see thing .Since philosophy scrapex package create websites REST APIs can reproduced long term, REST APIs ’ll see book fake. means contain made data deployed locally computer behind scenes.real world, ’ll go website company search API documentation (example, google “Spotify Web API” ’ll directed documentation Spotify’s REST API). Browsing documentation looking online tutorials ’ll able figure API works, data share authenticate. Just beware previous step calling api_amazon something won’t need accessing APIs companies; just way deploying locally developed APIs computer make examples book real possible.REST API example fake database Amazon’s books / users / products database. means can access sorts information books Amazon catalogue together author’s details, users read books products users bought platform. Let’s go explore .’ll assume ran part loading scrapex ran api_amazon function. return local version URL http://localhost:28723/__docs__/ different number part 28723. Let’s paste URL browser see :documentation API. Normally, company APIs something like website (different gist content similar). can see different endpoints description. first one, example, named /api/v1/amazon/authors. base path API http://localhost:28723 endpoint different URL website return different types data.Let’s check data authors endpoint. can usually try API endpoint directly documentation get grasp works. Let’s click endpoint see ’s inside:can see several things endpoint:need parameters. Endpoints often need parameters filter data. example, say requesting books author. Probably endpoint need specify author return data . endpoint returns available authors Amazon Book Store need parameters. endpoints REST API ’ll see need specify parameters.need parameters. Endpoints often need parameters filter data. example, say requesting books author. Probably endpoint need specify author return data . endpoint returns available authors Amazon Book Store need parameters. endpoints REST API ’ll see need specify parameters.allows try endpoint button says \"Try \".allows try endpoint button says \"Try \".REST APIs codes signal status request. Responses, says endpoint two codes: 200 means everything OK 500 means something wrong happened requested data. Note whenever request data, one codes returned. everything alright, 200 return together data wanted. something went wrong, code 500 returned. Note possible values can return, currently returned: remember haven’t made yet request.REST APIs codes signal status request. Responses, says endpoint two codes: 200 means everything OK 500 means something wrong happened requested data. Note whenever request data, one codes returned. everything alright, 200 return together data wanted. something went wrong, code 500 returned. Note possible values can return, currently returned: remember haven’t made yet request.Right Media type, can figure formats endpoint returns. says endpoint returns JSON string, typical format used sharing data web (’ll talk JSON format chapter #TODO).Right Media type, can figure formats endpoint returns. says endpoint returns JSON string, typical format used sharing data web (’ll talk JSON format chapter #TODO).Finally, irrespective particular endpoint, many REST APIs require authenticate requesting data. Normally, ’ll need follow instructions authenticating REST API ’re return ’ll give sort token.fake REST API Amazon, can use code fmcQoAZnnU authenticate. , click top right button Authorize, input token, click authorize close window:way, click “Try ” button “Execute” perform example request data endpoint:Alright, right now performed request /api/v1/amazon/authors endpoint. Let’s discuss happened:Behind scenes, joined base path REST API (http://localhost:28723) authors endpoint (/api/v1/amazon/authors) get path http://localhost:28723/api/v1/amazon/authors. can see path Request URL image . ’s complete URL returns data authors. Try copying URL endpoint (remember different port number website) pasting website:says didn’t provide token ’s correct. long explanation provide token browser tedious short explanation REST APIs meant used constantly browser. ’re aimed used programmatically ’s ’ll use APIs next chapters. now, ’ll continue make requests user interface __docs__ website.Going back results request, let’s explain actual data returned:Server response ’ll see request returned 200 code response. means request data successful good. Response body contains actual results. can see Amazon database five authors author written one genre. way data organized JSON. Later ’ll see read data frame R.Finally, response also contains stuff, aside data. important REST APIs. making request endpoint, REST APIs accept something called Headers. Let’s reason analogy using R list:make request, something like list . make request endpoint also add somewhat like attachment extra information. headers. APIs might require specify certain parameters headers might optional default values. many APIs header must contain authentication token got website using REST API.analogy, see specify authorization token headers well must accept data structured JSON format. Similarly, perform request endpoint, resulting response also contain headers attachment information. Following analogy using R list, response might look something like :list three slots: status_code, response_headers response_body. first two somewhat like attachments containing information third contains important part request: data. response headers (contrary headers submitted hypothetical request) extra information encoding response, date transfer encoding. case, won’t paying much attention response headers. Instead ’ll submitting headers request look status_code make sure request successfully. , ’ll want look status code returned look internet source error.Coming back authors example, can see see information saw R list analogy user interface example request:’s pretty much basics. performed first API request learned basics expect input return values API response.","code":"\nlibrary(scrapex)\nres <- api_amazon()[1] \"Visit your REST API at http://localhost:28723\"\n[1] \"Documentation is at http://localhost:28723/__docs__/\""},{"path":"introduction-to-rest-apis.html","id":"summary-1","chapter":"11 Introduction to REST APIs","heading":"11.1 Summary","text":"REST APIs base path. Normally ’s something like https://api.spotify.com/v1/.REST APIs base path. Normally ’s something like https://api.spotify.com/v1/.REST APIs structured around endpoints. specific URLs return different types data. example https://api.spotify.com/v1/artists endpoint related artist data https://api.spotify.com/v1/tracks returns data related tracks.REST APIs structured around endpoints. specific URLs return different types data. example https://api.spotify.com/v1/artists endpoint related artist data https://api.spotify.com/v1/tracks returns data related tracks.Endpoints might might parameters filter data.Endpoints might might parameters filter data.docs REST API documents endpoint parameters needed use query data.docs REST API documents endpoint parameters needed use query data.Many REST APIs need authentication tokens making requests.Many REST APIs need authentication tokens making requests.REST APIs response always return status codes headers describing whether request successful something went bad.REST APIs response always return status codes headers describing whether request successful something went bad.","code":""},{"path":"introduction-to-rest-apis.html","id":"exercises-7","chapter":"11 Introduction to REST APIs","heading":"11.2 Exercises","text":"Using interface used chapter making requests, can make request countries endpoint tell country Venezuela Amazon country list?Using interface used chapter making requests, can make request countries endpoint tell country Venezuela Amazon country list?Using users/ endpoint, can tell many Amazon users China?Using users/ endpoint, can tell many Amazon users China?Can breakdown Request URL performed previous exercises? , see differently request needs parameters contrast requests authors endpoint?Can breakdown Request URL performed previous exercises? , see differently request needs parameters contrast requests authors endpoint?Using users/ endpoint , make request using customer id (use 2131) country (China fine). status code get? response body says? Can anything fix ? Look web status code.Using users/ endpoint , make request using customer id (use 2131) country (China fine). status code get? response body says? Can anything fix ? Look web status code.","code":""},{"path":"a-primer-on-apis.html","id":"a-primer-on-apis","chapter":"12 A primer on APIs","heading":"12 A primer on APIs","text":"make requests API, probably want make programmatically. programmatically mean request data using programming language. book ’ll R can programming languages.want programmatically can just use documentation page make sample requests chapter # TODO intro API? several reasons. First, can create scheduled program download data specific intervals time. web browser tedious need constantly go computer manually click buttons save data somewhere. Second, APIs built persist thousands requests per minutes. might want make 60 requests every minute. sounds like handful human clicking browser day, right? Third, can much within programming language. example, might want request data endpoint passed input another endpoint, fallback value case request doesn’t return data. Fourth finally, might want things ‘real-time’ program. might build web application user clicks button, grab data API ‘real-time’ demand, train machine learning recently requested data.might think APIs convoluted concept point need think APIs just another way gather data. Using programming language allows automate gathering data add value data gathering process making automatic, persistent available time. reasons programmatically numerous ’s remaining book ’ll focus requesting data APIs R.# TODO primer web scraping, throughout chapter ’ll give one--one tour expect requesting data API wild. ’ll skip usual ‘start basics’ sections directly request data API see results efforts right away. begin, let’s load packages ’ll use chapter:usual, ’ll using scrapex package. package contains internal API wraps COVerAGE-DB database. COVerAGE-DB open-access database including cumulative counts confirmed COVID-19 cases, deaths, tests, vaccines age sex. information, visit website https://www.coverage-db.org/.aim primer create plot like one:plot shows total number COVID cases females males California. data local computer ’ll need request data API.","code":"\nlibrary(scrapex)\nlibrary(httr2)\nlibrary(dplyr)\nlibrary(ggplot2)"},{"path":"a-primer-on-apis.html","id":"getting-familiar-with-an-api","chapter":"12 A primer on APIs","heading":"12.1 Getting familiar with an API","text":"Inside scrapex function api_coveragedb() local API designed used book. Let’s load :API now launched background can access . Note learning new API online won’t launch API. APIs hosted servers elsewhere ’ll just need read access API. Since stand-alone book (don’t want examples depend fast-changing websites APIs, making content book useless just months), use local API won’t change time thus need launch .access API, instead ’ll given directly things like documentation page. can see print previous R call http://localhost:2234/__docs__/. Note link won’t work computer need launch API computer (remember local API?). Go URL see something like :can see API two endpoints. first one allows access data number COVID cases American states California, Utah New York State. second one returns vaccination rates three cities. opposed example # TODO intro API, API need authentication can directly go first endpoint read works:First thing see endpoint two parameters: region sex. required. Also, documentation parameter tells us valid values can accepted. Aside , see endpoint returns JSON structured response well usual return codes standard (200 successful request 500 standard error code). Alright, enough information construct first endpoint. Let’s go things. endpoint composed three things:base URL. case, ’s http://localhost:2234. different launched local computer.endpoint URL specific endpoint. COVID cases, /api/v1/covid_cases.parameters endpoint specified ? parameter concatenated &.complete endpoint URL endpoint http://localhost:2234/api/v1/covid_cases. add parameters? start ? add parameter=value followed & concatenate parameters. Sounds confusing? Let’s construct R:parameters specified ?region=California&sex=m. ’s difficult construct paths rather tedious. Imagine construct endpoint 7 parameters. many & can confuse anyone can start get difficult read. Instead, httr2 package can help us . httr2 just pass endpoint URL use functions add many parameters see fit.","code":"\napi_covid <- api_coveragedb()## [1] \"Visit your REST API at http://localhost:2234\"\n## [1] \"Documentation is at http://localhost:2234/__docs__/\"\n# COVID cases endpoint\napi_web <- paste0(api_covid$api_web, \"/api/v1/covid_cases\")\n\n# Filtering only for males in California\ncases_endpoint <- paste0(api_web, \"?region=California&sex=m\")\n\ncases_endpoint## [1] \"http://localhost:2234/api/v1/covid_cases?region=California&sex=m\""},{"path":"a-primer-on-apis.html","id":"making-your-first-request","chapter":"12 A primer on APIs","heading":"12.2 Making your first request","text":"","code":""},{"path":"a-primer-on-apis.html","id":"todo-you-cant-see-the-results-of-the-request-objetct-in-this-chapter.","chapter":"12 A primer on APIs","heading":"12.2.0.1 TODO: You can’t see the results of the request objetct in this chapter.","text":"Let’s start building request R:go. request function httr2 builds something like placeholder request. says already endpoint URL (can see GET line) say anything parameters headers. Let’s add endpoint parameters:function req_url_query constructs endpoint adding many parameters want. see idea ? request builds placeholder endpoint add many “add-’s” want request. “add-” far just parameters hypothetically add authentication headers retry step, example:request object also added headers well retry policy: request fails reason, try 3 times, waiting 5 seconds . type req_ hit tab IDE’s ’ll get complete list functions allow add stuff request well extract stuff request. Feel free explore get familiar capabilities httr2.Let’s get back initial request. Since don’t need add-’s request, look like :Whenever ’re happy request, can actually make request req_perform:Alright can interpret things request. First, successful. 200 status code means OK, nothing failed. Second, response content-type JSON, meaning data sent JSON format. Third, actual body request data, now loaded RAM memory (instead hard drive).Now, might noticed functions working request httr2 start req_*. means can easily search request related functions quickly. Similarly, response API, can resp_*. response functions allow perform extract stuff response object. can extract status code example:content type:extract body containing data request? depends content type returned. case ’s JSON can use resp_body_json. Whenever receive response, depending type data receive, best bet look resp_body_* content type request. case, can safely extract :go. see data California 5848 rows 11 columns. ’ll notice used argument simplifyVector inside resp_body_json. JSON format know nested format. means can columns within columns within columns . argument tries (always succeeds) simplifying structure data frame. way, let’s visualize overall number COVID cases:Great, can see rather big jump starting 2021 California. Let’s perform request replace sex females compare trend. , let’s count number cases per date males combine everything one data frame. Finally, let’s produce plot sexes:see, start see difference cases right start 2021 females greater number cases 2021. close chapter, remember kill API launched. wan’t request data online APIs, local version:. primer gave direct experience requesting data API involves. ’s usually studying documentation see endpoint returns, constructing URLs directly grab data want extracting data response. next chapter ’ll see depth APIs work behind scenes.","code":"\n# COVID cases endpoint\napi_web <- paste0(api_covid$api_web, \"/api/v1/covid_cases\")\nreq <- request(api_web)\nreq\nreq_california_m <-\n  req %>%\n  req_url_query(region = \"California\", sex = \"m\")\n\nreq_california_m\nreq_california_m %>%\n  req_auth_basic(username = \"fake name\", password = \"fake password\") %>%\n  req_retry(max_tries = 3, max_seconds = 5)\nreq_california_m\nresp_california_m <- \n  req_california_m %>%\n  req_perform()\n\nresp_california_m\nresp_status(resp_california_m)## [1] 200\nresp_content_type(resp_california_m)## [1] \"application/json\"\nresp_body_california_m <-\n  resp_california_m %>%\n  resp_body_json(simplifyVector = TRUE) %>%\n  as_tibble()\n\nresp_body_california_m## # A tibble: 5,848 × 11\n##    Country Region Code  Date  Sex   Age   AgeInt Metric Measure Value templateID\n##    <chr>   <chr>  <chr> <chr> <chr> <chr>  <int> <chr>  <chr>   <int> <chr>     \n##  1 USA     Calif… US-CA 2020… m     0         10 Count  Cases       0 USA_CDC_c…\n##  2 USA     Calif… US-CA 2020… m     10        10 Count  Cases       0 USA_CDC_c…\n##  3 USA     Calif… US-CA 2020… m     20        10 Count  Cases       2 USA_CDC_c…\n##  4 USA     Calif… US-CA 2020… m     30        10 Count  Cases       2 USA_CDC_c…\n##  5 USA     Calif… US-CA 2020… m     40        10 Count  Cases       1 USA_CDC_c…\n##  6 USA     Calif… US-CA 2020… m     50        10 Count  Cases       1 USA_CDC_c…\n##  7 USA     Calif… US-CA 2020… m     60        10 Count  Cases       1 USA_CDC_c…\n##  8 USA     Calif… US-CA 2020… m     70        10 Count  Cases       1 USA_CDC_c…\n##  9 USA     Calif… US-CA 2020… m     0         10 Count  Cases       0 USA_CDC_c…\n## 10 USA     Calif… US-CA 2020… m     10        10 Count  Cases       0 USA_CDC_c…\n## # … with 5,838 more rows\n# To avoid scientific numbering\noptions(scipen = 231241231)\n\nresp_body_california_m <-\n  resp_body_california_m %>%\n  mutate(Date = lubridate::ymd(Date)) %>%\n  group_by(Date, Sex) %>%\n  summarize(cases = sum(Value))\n\nresp_body_california_m %>%\n  ggplot(aes(Date, cases)) +\n  geom_line() +\n  theme_bw()\n# Perform the same request but for females and grab the JSON result\nresp_body_california_f <-\n  req %>%\n  req_url_query(region = \"California\", sex = 'f') %>%\n  req_perform() %>%\n  resp_body_json(simplifyVector = TRUE) %>%\n  as_tibble()\n\n# Add the total number of cases per date for females\nresp_body_california_f <-\n  resp_body_california_f %>%\n  mutate(Date = lubridate::ymd(Date)) %>%\n  group_by(Date, Sex) %>%\n  summarize(cases = sum(Value))\n\n# Combine the female cases with the male cases\nresp_body_california <- bind_rows(resp_body_california_f, resp_body_california_m)\n\n# Visualize both female and male cases\nresp_body_california %>%\n  ggplot(aes(Date, cases, color = Sex, group = Sex)) +\n  geom_line() +\n  theme_bw()\napi_covid$process$kill()## [1] TRUE"},{"path":"a-primer-on-apis.html","id":"exercises-8","chapter":"12 A primer on APIs","heading":"12.3 Exercises","text":"Generate plot created males/females state New York. However, construct path manually (use function req_url_query). might need look spaces written URLs internet.Generate plot created males/females state New York. However, construct path manually (use function req_url_query). might need look spaces written URLs internet.third wave vaccination start time males females New York? might need look documentation second endpoint API get acquainted parameters values.third wave vaccination start time males females New York? might need look documentation second endpoint API get acquainted parameters values.TODO request printout happening?Can recreate request ? need look req_* make sure understand req_* functions reconstructing request.Can recreate request ? need look req_* make sure understand req_* functions reconstructing request.Can make request specifying sex unavailable region (Florida, example)? status code return? Look internet status codes mean. copying endpoint browser (entire endpoint parameters browser), message see?Can make request specifying sex unavailable region (Florida, example)? status code return? Look internet status codes mean. copying endpoint browser (entire endpoint parameters browser), message see?","code":""},{"path":"what-is-a-restful-api.html","id":"what-is-a-restful-api","chapter":"13 What is a RESTful API?","heading":"13 What is a RESTful API?","text":"https://www.gastonsanchez.com/intro2cwd/http.htmlhttps://www.gastonsanchez.com/intro2cwd/apis.htmlTAlk methods (GET / POST / PATCH / DELETE)","code":""},{"path":"what-you-need-to-know-about-json.html","id":"what-you-need-to-know-about-json","chapter":"14 What you need to know about JSON","heading":"14 What you need to know about JSON","text":"content ’ll grab API probably JavaScript Object Notation JSON short. format designed easily shared internet pretty much standard sharing data APIs. JSON’s text based allow store usual stuff ’re used strings numbers. However, JSON’s also allow use arrays, importantly, build complex nested structure.always, might ask , new format important? ’ll skip technical boring details leave three important points:’s lightweight easy shareIt support nearly programming languagesIt allows make nested relationships ’re easy readIn chapter ’ll jump directly content mostly related parsing JSON’s context API requests. means won’t looking toy examples rather typical response might get API, nested structures. idea users perfect knowledge JSON’s rather learn tricks convert format ’re interested .want information JSON’s R, sure visit Gaston Sanchez’s JSON intro, chapter much inspired ( https://www.gastonsanchez.com/intro2cwd/json.html)theory way, let’s get hands dirty. Let’s load package ’ll use chapter:","code":"\nlibrary(jsonlite)\nlibrary(tibble)\nlibrary(tidyr)"},{"path":"what-you-need-to-know-about-json.html","id":"your-first-json-example","chapter":"14 What you need to know about JSON","heading":"14.1 Your first JSON example","text":"JSON syntax straightforward. wrap everything {} specify data key:value pairs. example:key:value needs wrapped quotes (\"). R can parse JSON’s several packages. book ’ll use jsonlite feel free explore options. Let’s lparse string specified function fromJSON:parses key/value pairs named list. ’s exactly yo can think JSON’s now. way store named arrays. point show examples JSON’s work ’s pretty much : key:value pairs. can write number instead strings well ’s nothing gained example JSON moment.’s inevitable say true deal breaker JSON’s nested nature format. ’s can achieve simple syntax:Things taken quite change . Let’s stop discuss changes. still key:value pairs (president, etc..) value string number array. first key:value pair :key followed array [...] contains three key:value pairs. interesting can think arrays JSON rows data frame. Going back full example, can think three keys (names slot) contains 1 row data frame inside. Let’s try parsing R:get precisely named list slot contains data frame. ? far good. JSON’s key:value pairs value self can array key/value pairs. , idea mind, might get something like :Without looking , think result R? Study write . Let’s look result:Well, something ’ve never looked . :data frame first slot. ’s correct JSON contained array two sets key:value pairs. ’s translatable data frame two rows even though one two sets field age.data frame first slot. ’s correct JSON contained array two sets key:value pairs. ’s translatable data frame two rows even though one two sets field age.NA value since null way missing values represented JSON notice null JSON array effectively data frame NA value.NA value since null way missing values represented JSON notice null JSON array effectively data frame NA value.named list. ’s right array structure ([...]) key:value pairs interpreted named list.named list. ’s right array structure ([...]) key:value pairs interpreted named list.reason thinking, important? Let tell much JSON ’ll find wild, ’ll see becomes convoluted read becomes nested. ’ll need learn use subsetting skills actually convert data format want. example, might want convert existing opposition slot data frame array structure missing ([...]):","code":"{\n  \"president\": \"Parlosi\",\n  \"vicepresident\": \"Kantos\",\n  \"opposition\": \"Pitaso\"\n}\njson_str <- '{\n  \"president\": \"Parlosi\",\n  \"vicepresident\": \"Kantos\",\n  \"opposition\": \"Pitaso\"\n}'\n\nfromJSON(json_str)## $president\n## [1] \"Parlosi\"\n## \n## $vicepresident\n## [1] \"Kantos\"\n## \n## $opposition\n## [1] \"Pitaso\"{\n    \"president\": [\n        {\n            \"last_name\": \"Parlosi\",\n            \"party\": \"Free thinkers\",\n            \"age\": 35\n        }\n    ],\n    \"vicepresident\": [\n        {\n            \"last_name\": \"Kantos\",\n            \"party\": \"Free thinkers\",\n            \"age\": 52\n        }\n    ],\n    \"opposition\": [\n        {\n            \"last_name\": \"Pitaso\",\n            \"party\": \"Everyone United\",\n            \"age\": 45\n        }\n    ]\n}\"president\": [\n    {\n        \"last_name\": \"Parlosi\",\n        \"party\": \"Free thinkers\",\n        \"age\": 35\n    }\n]\njson_str <- '\n{\n    \"president\": [\n        {\n            \"last_name\": \"Parlosi\",\n            \"party\": \"Free thinkers\",\n            \"age\": 35\n        }\n    ],\n    \"vicepresident\": [\n        {\n            \"last_name\": \"Kantos\",\n            \"party\": \"Free thinkers\",\n            \"age\": 52\n        }\n    ],\n    \"opposition\": [\n        {\n            \"last_name\": \"Pitaso\",\n            \"party\": \"Everyone United\",\n            \"age\": 45\n        }\n    ]\n}\n'\n\nfromJSON(json_str, simplifyDataFrame = TRUE)## $president\n##   last_name         party age\n## 1   Parlosi Free thinkers  35\n## \n## $vicepresident\n##   last_name         party age\n## 1    Kantos Free thinkers  52\n## \n## $opposition\n##   last_name           party age\n## 1    Pitaso Everyone United  45{\n    \"president\": [\n        {\n            \"last_name\": \"Parlosi\",\n            \"party\": \"Free thinkers\",\n            \"age\": 35\n        },\n        {\n            \"last_name\": \"Stevensson\",\n            \"party\": \"Free thinkers\"\n        }\n    ],\n    \"vicepresident\": [\n        null\n    ],\n    \"opposition\": {\n        \"last_name\": \"Pitaso\",\n        \"party\": \"Everyone United\",\n        \"age\": 45\n    }\n}\njson_str <- '\n{\n    \"president\": [\n        {\n            \"last_name\": \"Parlosi\",\n            \"party\": \"Free thinkers\",\n            \"age\": 35\n        },\n        {\n            \"last_name\": \"Stevensson\",\n            \"party\": \"Free thinkers\"\n        }\n    ],\n    \"vicepresident\": [\n        null\n    ],\n    \"opposition\": {\n        \"last_name\": \"Pitaso\",\n        \"party\": \"Everyone United\",\n        \"age\": 45\n    }\n}\n'\n\nres <- fromJSON(json_str)\nres## $president\n##    last_name         party age\n## 1    Parlosi Free thinkers  35\n## 2 Stevensson Free thinkers  NA\n## \n## $vicepresident\n## [1] NA\n## \n## $opposition\n## $opposition$last_name\n## [1] \"Pitaso\"\n## \n## $opposition$party\n## [1] \"Everyone United\"\n## \n## $opposition$age\n## [1] 45\nres$opposition <- data.frame(res$opposition)\nres## $president\n##    last_name         party age\n## 1    Parlosi Free thinkers  35\n## 2 Stevensson Free thinkers  NA\n## \n## $vicepresident\n## [1] NA\n## \n## $opposition\n##   last_name           party age\n## 1    Pitaso Everyone United  45"},{"path":"what-you-need-to-know-about-json.html","id":"the-enframe-unnest-strategy","chapter":"14 What you need to know about JSON","heading":"14.2 The enframe + unnest strategy","text":"’ve reached important objective chapter. information, can combine final data frame like one:‘categories’ (mean, president, vicepresident, opposition), concatenate data frames single data frame much easier read., ’s general strategy combining two functions: enframe unnest. enframe takes named list two things. First, extracts names slot list stores column data frame. Secondly, takes everything inside slot stores list-column.list-column? ’s column class list can contain different things row (remember, columns R must kind, either numeric, character something else can’t two types column. list-columns, exception). example, first row list-column data frame two rows (data frame president), second row empty data frame NA value third now data frame since altered JSON manually (remember?). ’s result enframe look like:first column contains names named list second column contains list-column. list column telling right away two data frames empty slot. useful reading nested stuff gives high level overview everything ’re working can actually something ? work using unnest. Contrary enframe, unnest takes list-columns ‘unpacks’ common class list. Let repeat . unnest needs passed list-column (list-column can contain anything) try unpack whatever common class objects. objects different classes (data.frame, vectors, etc..), unnest fail. However, objects within list class, combine proper column ‘unpack’ ’s values.’s somewhat convoluted explanation, ’s totally fine ’s clear since many edge cases unnest works. chapter ’ll focus understanding function works.Let’s unpack value column using unnest:Since objects list-column compatible (data frames number columns), unnest unpacks column entirely simply expand data frame’s one single data frame. perfect example unnest fails different objects list easily mergeable. Say use example don’t convert third slot data frame. ’ll JSON parsed like :first second slot can combined third named list. ’s clear can combined. unnest tell try unpack :says exactly : first slot data frame third slot list. can’t find way combine . main problem ’ll encounter JSON’s ’re trying parse JSON many nested arrays arrays compatible unnesting ’ll submerge nested arrays fix whatever data want extract.","code":"## # A tibble: 4 × 4\n##   name          last_name  party             age\n##   <chr>         <chr>      <chr>           <int>\n## 1 president     Parlosi    Free thinkers      35\n## 2 president     Stevensson Free thinkers      NA\n## 3 vicepresident <NA>       <NA>               NA\n## 4 opposition    Pitaso     Everyone United    45\nres %>%\n  enframe()## # A tibble: 3 × 2\n##   name          value       \n##   <chr>         <list>      \n## 1 president     <df [2 × 3]>\n## 2 vicepresident <lgl [1]>   \n## 3 opposition    <df [1 × 3]>\nres %>%\n  enframe() %>%\n  unnest(cols = value)## # A tibble: 4 × 4\n##   name          last_name  party             age\n##   <chr>         <chr>      <chr>           <int>\n## 1 president     Parlosi    Free thinkers      35\n## 2 president     Stevensson Free thinkers      NA\n## 3 vicepresident <NA>       <NA>               NA\n## 4 opposition    Pitaso     Everyone United    45\njson_str <- '\n{\n    \"president\": [\n        {\n            \"last_name\": \"Parlosi\",\n            \"party\": \"Free thinkers\",\n            \"age\": 35\n        },\n        {\n            \"last_name\": \"Stevensson\",\n            \"party\": \"Free thinkers\"\n        }\n    ],\n    \"vicepresident\": [\n        null\n    ],\n    \"opposition\": {\n        \"last_name\": \"Pitaso\",\n        \"party\": \"Everyone United\",\n        \"age\": 45\n    }\n}\n'\n\nres <- fromJSON(json_str)\nres## $president\n##    last_name         party age\n## 1    Parlosi Free thinkers  35\n## 2 Stevensson Free thinkers  NA\n## \n## $vicepresident\n## [1] NA\n## \n## $opposition\n## $opposition$last_name\n## [1] \"Pitaso\"\n## \n## $opposition$party\n## [1] \"Everyone United\"\n## \n## $opposition$age\n## [1] 45\nres %>%\n  enframe() %>%\n  unnest(cols = value)## Error:\n## ! Can't combine `..1` <data.frame> and `..3` <list>."},{"path":"what-you-need-to-know-about-json.html","id":"accessing-deeply-nested-jsons","chapter":"14 What you need to know about JSON","heading":"14.3 Accessing deeply nested JSONs","text":"Suppose part research project, ’ve recently granted access API company. ’re interested studying relationship geo-location clients, shopping patterns social class. grant access private API, endpoint returns geo location client returns JSON like one . use fromJSON parse get:Alright seems two data frames parsed, one client. Let’s pick one two access columns:Great, seems parsed correctly. Let’s paste “Device:” front device just make sure don’t forget:Ups, ? Oh, device column character vector, ’s something like list. Let’s explore :device list data frame inside. Let’s see class columns avoid mistake:location also list. Let’s look ’s inside:Alright, general pattern. Whenever fromJSON encounters array within array, converts one data frame. problem recurrent data frames inside data frames . ’s difficult assess stop good example unnest handy. ’s fix :Let’s apply general enframe + unnest strategy applying unnest many times need:Let’s unpack value:device:location:go. ’s handle nested JSON’s R. Beware example, everything worked well nested object array merged types real challenge parsing JSON’s nested object conform well unpacking everything structure.JSON’s hard main challenge understanding combine everything R data frames lot nested arrays different classes shapes. Remember whenever try unpack something, sure pay attention whether error related unmergeable classes objects different lengths.","code":"\njson_str <- '\n{\n    \"client1\": [\n        {\n            \"name\": \"Kurt Rosenwinkel\",\n            \"device\": [\n                {\n                    \"type\": \"iphone\",\n                    \"location\": [\n                        {\n                            \"lat\": 213.12,\n                            \"lon\": 43.213\n                        }\n                    ]\n                }\n            ]\n        }\n    ],\n    \"client2\": [\n        {\n            \"name\": \"YEITUEI\",\n            \"device\": [\n                {\n                    \"type\": \"android\",\n                    \"location\": [\n                        {\n                            \"lat\": 211.12,\n                            \"lon\": 53.213\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n'\n\nres <- fromJSON(json_str)\nres## $client1\n##               name                 device\n## 1 Kurt Rosenwinkel iphone, 213.12, 43.213\n## \n## $client2\n##      name                  device\n## 1 YEITUEI android, 211.12, 53.213\nres$client1##               name                 device\n## 1 Kurt Rosenwinkel iphone, 213.12, 43.213\npaste0(\"Device: \", res$client1$device)## [1] \"Device: list(type = \\\"iphone\\\", location = list(list(lat = 213.12, lon = 43.213)))\"\nres$client1$device[[1]]##     type        location\n## 1 iphone 213.120, 43.213\nlapply(res$client1$device[[1]], class)## $type\n## [1] \"character\"\n## \n## $location\n## [1] \"list\"\nres$client1$device[[1]]$location[[1]]##      lat    lon\n## 1 213.12 43.213\nres$client1 %>%\n  as_tibble() %>%\n  unnest(cols = device) %>%\n  unnest(cols = location)## # A tibble: 1 × 4\n##   name             type     lat   lon\n##   <chr>            <chr>  <dbl> <dbl>\n## 1 Kurt Rosenwinkel iphone  213.  43.2\nall_res <-\n  res %>%\n  enframe(name = \"client\")\n\nall_res## # A tibble: 2 × 2\n##   client  value       \n##   <chr>   <list>      \n## 1 client1 <df [1 × 2]>\n## 2 client2 <df [1 × 2]>\nall_res %>%\n  unnest(cols = \"value\")## # A tibble: 2 × 3\n##   client  name             device      \n##   <chr>   <chr>            <list>      \n## 1 client1 Kurt Rosenwinkel <df [1 × 2]>\n## 2 client2 YEITUEI          <df [1 × 2]>\nall_res %>%\n  unnest(cols = \"value\") %>%\n  unnest(cols = \"device\")## # A tibble: 2 × 4\n##   client  name             type    location    \n##   <chr>   <chr>            <chr>   <list>      \n## 1 client1 Kurt Rosenwinkel iphone  <df [1 × 2]>\n## 2 client2 YEITUEI          android <df [1 × 2]>\nall_res %>%\n  unnest(cols = \"value\") %>%\n  unnest(cols = \"device\") %>%\n  unnest(cols = \"location\")## # A tibble: 2 × 5\n##   client  name             type      lat   lon\n##   <chr>   <chr>            <chr>   <dbl> <dbl>\n## 1 client1 Kurt Rosenwinkel iphone   213.  43.2\n## 2 client2 YEITUEI          android  211.  53.2"},{"path":"what-you-need-to-know-about-json.html","id":"exercises-9","chapter":"14 What you need to know about JSON","heading":"14.4 Exercises","text":"Can fix JSON?’s excerpt deeply nested JSON. Can parse R unnest columns needed. can’t ? Can manually change code fix ?Say parsing JSON get list like one . can transform data frame like one ? Hint: might need explore function purrr::transpose.","code":"\nlibrary(jsonlite)\n\njson_str <- '\n{\n  \"president\": [\n    {\n      \"last_name\": Parlosi,\n      \"party\": \"Free thinkers\",\n      \"age\": 35,\n    },\n    {\n      \"last_name\": \"Stevensson\",\n      party: \"Free thinkers\"\n    }\n  ,\n  \"vicepresident\": [null],\n  \"opposition\" =\n    {\n      \"last_name\": \"Pitaso\",\n      \"party\": \"Everyone United\",\n      \"age\": 45\n    }\n}\n'\n\nfromJSON(json_str)\njson_str <- '\n{\n    \"client1\": [\n        {\n            \"name\": \"Kurt Rosenwinkel\",\n            \"device\": [\n                {\n                    \"type\": \"iphone\",\n                    \"location\": [\n                        \"213.12\",\n                        \"43.213\"\n                    ]\n                }\n            ]\n        }\n    ],\n    \"client2\": [\n        {\n            \"name\": \"YEITUEI\",\n            \"device\": [\n                {\n                    \"type\": \"android\",\n                    \"location\": [\n                        213.12,\n                        43.213\n                    ]\n                }\n            ]\n        }\n    ]\n}\n'\nparsed_json <-\n  list(\n  list(author = \"Cameron Gutkowski\", genre = \"Ad\"),\n  list(author = \"Stuart Armstrong\", genre = \"Sunt\"),\n  list(author = \"Kameron Grimes\", genre = \"Eius\"),\n  list(author = \"Genoveva Hand\", genre = \"A\"),\n  list(author = \"Kobe Effertz\", genre = \"Possimus\")\n)## # A tibble: 5 × 2\n##   author            genre   \n##   <chr>             <chr>   \n## 1 Cameron Gutkowski Ad      \n## 2 Stuart Armstrong  Sunt    \n## 3 Kameron Grimes    Eius    \n## 4 Genoveva Hand     A       \n## 5 Kobe Effertz      Possimus"},{"path":"case-study-grabbing-data-from-a-public-api.html","id":"case-study-grabbing-data-from-a-public-api","chapter":"15 Case Study: grabbing data from a public API","heading":"15 Case Study: grabbing data from a public API","text":"","code":"\nlibrary(scrapex)\nlibrary(httr2)\nlibrary(dplyr)\n\napi <- api_amazon()\n\napi_authors <- paste0(api$api_web, \"/api/v1/amazon/\")\n\n# Show docs\ntoken <- paste0(\"Bearer \", amazon_bearer_tokens())\n\nreq <-\n  request(api_authors) %>%\n  req_headers(Authorization = token)\n\nreq_res <-\n  req %>%\n  req_url_path_append(\"authors\") %>%\n  req_perform()\n\nparse_data(req_res)\n\nreq_res <-\n  req %>%\n  req_url_path_append(\"texts\") %>%\n  req_url_query(author = \"Stuart Armstrong\") %>%\n  req_perform()\n\nparse_data(req_res)\n\nreq_res <-\n  req %>%\n  req_url_path_append(\"books\") %>%\n  req_url_query(author = \"Stuart Armstrong\", genre = \"Sunt\") %>%\n  req_perform()\n\nreq_res %>%\n  parse_data() %>%\n  select(-user_id) %>%\n  distinct()\n\nreq_res %>%\n  parse_data()\n\nreq_res <-\n  req %>%\n  req_url_path_append(\"countries\") %>%\n  req_perform()\n\nparse_data(req_res)\n\nreq_res <-\n  req %>%\n  req_url_path_append(\"users\") %>%\n  req_url_query(user_id = 319) %>%\n  req_perform()\n\nparse_data(req_res)\n\nreq_res <-\n  req %>%\n  req_url_path_append(\"users\") %>%\n  req_url_query(country = \"Germany\") %>%\n  req_perform()\n\nparse_data(req_res)\n\n# What does the status code mean?\n## req_res <-\n##   req %>%\n##   req_url_path_append(\"users\") %>%\n##   req_url_query(user_id = 319, country = \"Germany\") %>%\n##   req_perform()\n\nreq_res <-\n  req %>%\n  req_url_path_append(\"products_users\") %>%\n  req_url_query(user_id = 319) %>%\n  req_perform()\n\n# Exercise for images\nreq_res %>%\n  resp_body_json() %>%\n  transpose() %>%\n  as_tibble() %>%\n  select(-tags) %>%\n  unnest() %>%\n  unnest()\n\nreq_res <-\n  req %>%\n  req_url_path_append(\"products\") %>%\n  req_url_query(product_id = 74) %>%\n  req_perform()\n\nreq_res %>%\n  resp_body_json() %>%\n  transpose() %>%\n  as_tibble() %>%\n  unnest()"},{"path":"automating-api-programs-real-time-bicycle-data.html","id":"automating-api-programs-real-time-bicycle-data","chapter":"16 Automating API programs: real-time bicycle data","heading":"16 Automating API programs: real-time bicycle data","text":"Grabbing data APIs can achieved techniques ’ve covered book far. However, case web scraping, data ephemeral. ’re interested web scraping temperature data city website, temperature 6 months ago given neighborhood might available . ’s ’s common web scraping API scripts automated.Let tell brief story motivates chapter. Throughout PhD studies living Barcelona used take bike ride house university. common Barcelona, used public bicycle system called “Bicing”. Every day, took bike upper part city, riding city campus next beach. Whenever day coming end, took bike upper part city.Although sounds pleasent, every day tried find bike, bikes available stations. happened upper part city well stations close university. used waste 15-20 minutes per trajectory just waiting bikes arrive one stations. 3-4 stations near university, also juggle station wait bike. ‘estimation’ based experience: day ’ve seen bikes arrive much often station one. However, something leap faith: logged ‘Bicing’ mobile app, saw bikes arrived earlier stations ’d missed opportunity.spending 2-3 years started think must way make ‘estimations’ accurate. example, data availability bikes station, probably estimate type poisson process measure predict rate bikes arrived different periods day. fine didn’t data . ’s found .Bicing real time API allow check status station (number bikes available) time stamp time requesting data. ’s , ’s information needed needed pretty much every minute, able get entire history station day. near second--second snapshot allow record interval day bike retrieved interval another bike deposited station. ? automation.","code":""},{"path":"automating-api-programs-real-time-bicycle-data.html","id":"the-bicing-api","chapter":"16 Automating API programs: real-time bicycle data","heading":"16.1 The Bicing API","text":"scrapex package contains replica bicing API used PhD studies. Let’s load packages ’ll use chapter launch bicing API.usual, first thing want access documentation. ’s get:bicing API one endpoint. Let’s open endpoint see arguments need pass endpoint:documentation endpoint states clear description endpoint returns:Access real time bicycle usage sample 20 stations Barcelona’s public bicycle systemAs example API, ’ve included 20 stations original results. original results contain information 100 stations Barcelona. can also see parameters heading endpoint need parameters.However, important thing endpoint can interpreted description see . Access real time bicycle usage transparent highlighting data real time. means data see request right now, might data see 10 minutes. Since created sample API, made sure case. means every time request data endpoint, ’ll see completely new data previous request. nature API forces us think along two lines: want understand bicycle patterns, need automate process need save data somewhere.Since already know enough single endpoint API, let’s try build request check output structured. image can see endpoint /api/v1/real_time_bicycles let’s append base API:go, let’s make request check body:status code request 200 everything went successful. see content-type JSON format, ’s everything ’re used . Let’s check content body:’ve shown first two slots resulting list (parsed JSON directly using resp_body_json). output looks like slot row data frame contains column, names like slots, in_use, latitude/longitude streetName. different ways parse output first comes mind loop slot simply try convert data frame. Since slot named list inside, data.frame can convert directly data frame can bind rows single data frame. Let’s try :Yep, works well. Let’s go back request extract stations combine together:Great, go. managed access API, request real time information bicycles parse output data frame. important columns slots, current_time in_use. three can calculate rate station used throughout day. addition, cool stuff like latitude/longitude locate station case want make, example, maps bicycle usage. Let’s take step back move code function makes request:function receives bicing API object API website, makes request API, combines results data frame returns data frame. Using example function can confirm making two requests return different data terms bicycle usage. example:performed two requests selected column in_use check two columns different. effect, bicycle usage two requests just 1 second apart shows vastly different usage patterns station. highlights ’re really interested adding two ingredients making robust program: save data fly automate script.","code":"\nlibrary(scrapex)\nlibrary(httr2)\nlibrary(dplyr)\nlibrary(readr)\n\nbicing <- api_bicing()## [1] \"Visit your REST API at http://localhost:9614\"\n## [1] \"Documentation is at http://localhost:9614/__docs__/\"\nrt_bicing <- paste0(bicing$api_web, \"/api/v1/real_time_bicycles\")\nrt_bicing## [1] \"http://localhost:9614/api/v1/real_time_bicycles\"\nresp_output <- \n  rt_bicing %>%\n  request() %>% \n  req_perform()\n\nresp_output\nsample_output <-\n  resp_output %>%\n  resp_body_json() %>%\n  head(n = 2)\n\nsample_output## [[1]]\n## [[1]]$type\n## [1] \"BIKE\"\n## \n## [[1]]$latitude\n## [1] 41.398\n## \n## [[1]]$longitude\n## [1] 2.18\n## \n## [[1]]$streetName\n## [1] \"Gran Via Corts Catalanes\"\n## \n## [[1]]$streetNumber\n## [1] \"760\"\n## \n## [[1]]$slots\n## [1] 29\n## \n## [[1]]$current_time\n## [1] \"2022-12-23 15:30:52\"\n## \n## [[1]]$in_use\n## [1] 4\n## \n## \n## [[2]]\n## [[2]]$type\n## [1] \"BIKE\"\n## \n## [[2]]$latitude\n## [1] 41.3955\n## \n## [[2]]$longitude\n## [1] 2.1771\n## \n## [[2]]$streetName\n## [1] \"Roger de Flor/ Gran Vía\"\n## \n## [[2]]$streetNumber\n## [1] \"126\"\n## \n## [[2]]$slots\n## [1] 27\n## \n## [[2]]$current_time\n## [1] \"2022-12-23 15:30:52\"\n## \n## [[2]]$in_use\n## [1] 14\nsample_output %>%\n  lapply(data.frame) %>%\n  bind_rows()##   type latitude longitude               streetName streetNumber slots\n## 1 BIKE  41.3980    2.1800 Gran Via Corts Catalanes          760    29\n## 2 BIKE  41.3955    2.1771  Roger de Flor/ Gran Vía          126    27\n##          current_time in_use\n## 1 2022-12-23 15:30:52      4\n## 2 2022-12-23 15:30:52     14\nall_stations <-\n  resp_output %>%\n  resp_body_json()\n\nall_stations_df <- \n  all_stations %>%\n  lapply(data.frame) %>%\n  bind_rows()\n\nall_stations_df##    type latitude longitude                     streetName streetNumber slots\n## 1  BIKE  41.3980    2.1800       Gran Via Corts Catalanes          760    29\n## 2  BIKE  41.3955    2.1771        Roger de Flor/ Gran Vía          126    27\n## 3  BIKE  41.3941    2.1813                         Nàpols           82    27\n## 4  BIKE  41.3935    2.1816                          Ribes           13    20\n## 5  BIKE  41.3911    2.1802              Pg Lluís Companys           11    39\n## 6  BIKE  41.3913    2.1806              Pg Lluís Companys           18    39\n## 7  BIKE  41.3889    2.1833              Pg Lluís Companys            1    23\n## 8  BIKE  41.3891    2.1836              Pg Lluís Companys            2    26\n## 9  BIKE  41.3845    2.1849         Marquès de l'Argentera           13    26\n## 10 BIKE  41.3817    2.1939                Passeig Marítim           19    21\n## 11 BIKE  41.3845    2.1957         Pg Marítim Barceloneta           23    29\n## 12 BIKE  41.3869    2.1958               Avinguda Litoral           16     3\n## 13 BIKE  41.3847    2.1850 Avinguda del Marques Argentera           15    26\n## 14 BIKE  41.3948    2.1712                         Girona           68    18\n## 15 BIKE  41.3983    2.1867                  Av. Meridiana           47    21\n## 16 BIKE  41.3982    2.1867                  Av. Meridiana           47    21\n## 17 BIKE  41.4061    2.1742                       Rosselló          453    26\n## 18 BIKE  41.4033    2.1707                       Rosselló          354    28\n## 19 BIKE  41.4102    2.1758                      Cartagena          308    20\n## 20 BIKE  41.4109    2.1740       Sant Antoni Maria Claret          214     2\n##           current_time in_use\n## 1  2022-12-23 15:30:52      4\n## 2  2022-12-23 15:30:52     14\n## 3  2022-12-23 15:30:52     14\n## 4  2022-12-23 15:30:52     11\n## 5  2022-12-23 15:30:52      7\n## 6  2022-12-23 15:30:52      1\n## 7  2022-12-23 15:30:52     22\n## 8  2022-12-23 15:30:52      9\n## 9  2022-12-23 15:30:52     21\n## 10 2022-12-23 15:30:52     18\n## 11 2022-12-23 15:30:52      2\n## 12 2022-12-23 15:30:52      0\n## 13 2022-12-23 15:30:52     15\n## 14 2022-12-23 15:30:52      3\n## 15 2022-12-23 15:30:52     13\n## 16 2022-12-23 15:30:52      1\n## 17 2022-12-23 15:30:52      4\n## 18 2022-12-23 15:30:52     27\n## 19 2022-12-23 15:30:52      6\n## 20 2022-12-23 15:30:52      1\nreal_time_bicing <- function(bicing_api) {\n  rt_bicing <- paste0(bicing_api$api_web, \"/api/v1/real_time_bicycles\")\n  \n  resp_output <- \n    rt_bicing %>%\n    request() %>% \n    req_perform()\n  \n  all_stations <-\n  resp_output %>%\n  resp_body_json()\n\n  all_stations_df <- \n    all_stations %>%\n    lapply(data.frame) %>%\n    bind_rows()\n  \n  all_stations_df\n}\nfirst <- real_time_bicing(bicing) %>% select(in_use) %>% rename(first_in_use = in_use)\nsecond <- real_time_bicing(bicing) %>% select(in_use) %>% rename(second_in_use = in_use)\nbind_cols(first, second)##    first_in_use second_in_use\n## 1             2            11\n## 2            17             0\n## 3            23            18\n## 4            10             2\n## 5            10             9\n## 6             6             7\n## 7             6             2\n## 8            12             6\n## 9            18            20\n## 10           20            18\n## 11           23             7\n## 12            2             1\n## 13            1            16\n## 14            4             5\n## 15            9             2\n## 16            3            19\n## 17            6            20\n## 18           19            16\n## 19           13            12\n## 20            1             0"},{"path":"automating-api-programs-real-time-bicycle-data.html","id":"saving-data-in-api-programs","chapter":"16 Automating API programs: real-time bicycle data","heading":"16.2 Saving data in API programs","text":"first step ’ll take saving data. strategy can different depending needs. example, might want access online database MySQL save data . lot benefits also drawbacks. example ’ll go basics save result local CSV file. valid strategy depending needs. main drawback strategy ’ll save data locally meaning backup case loose computer breaks. alternative host CSV, example, Google Drive access real time save results. ’ll focus simpler example loading/saving CSV local computer.saving data need focus several things:must perform request bicing APIWe must perform request bicing APIWe must specify local path CSV file saved. directory CSV file created, create .must specify local path CSV file saved. directory CSV file created, create .CSV file exist, create scratchIf CSV file exist, create scratchIf CSV file exists, want append resultIf CSV file exists, want append resultYou’ll see remaining part chapter ’ll use local path. replace local path. ’s rough implementation bullet points comments:One way test whether works run twice read results back data frame check data frame correctly structured. Let’s try :called API , saved results allowed program sleep 5 seconds. called API saved results. , read bicing_history.csv file R data frame showed distinct time stamps file . requested data API twice saved results, find two time stamps, rather one. ’s precisely find results. addition, also two different numbers column in_use number bicycles usage. Let’s pick one station check works:first time stamp station 13 bikes second time stamp 19. aim automate run frequently thus ’ll get hundreds rows station.","code":"\nsave_bicing_data <- function(bicing_api) {\n  # Perform a request to the bicing API and get the data frame back\n  bicing_results <- real_time_bicing(bicing_api)\n  \n  # Specify the local path where the file will be saved\n  local_csv <- \"/home/jorge.cimentada/bicing/bicing_history.csv\"\n  \n  # Extract the directory where the local file is saved\n  main_dir <- dirname(local_csv)\n\n  # If the directory does *not* exist, create it, recursively creating each folder  \n  if (!dir.exists(main_dir)) {\n    dir.create(main_dir, recursive = TRUE)\n  }\n\n  # If the file does not exist, save the current bicing response  \n  if (!file.exists(local_csv)) {\n    write_csv(bicing_results, local_csv)\n  } else {\n    # If the file does exist, the *append* the result to the already existing CSV file\n    write_csv(bicing_results, local_csv, append = TRUE)\n  }\n}\nsave_bicing_data(bicing)\nSys.sleep(5)\nsave_bicing_data(bicing)\n\nbicing_history <- read_csv(\"/home/jorge.cimentada/bicing/bicing_history.csv\")\nbicing_history %>%\n  distinct(current_time)## # A tibble: 2 × 1\n##   current_time       \n##   <dttm>             \n## 1 2022-12-20 23:51:56\n## 2 2022-12-20 23:52:04\nbicing_history %>%\n  filter(streetName == \"Ribes\") %>% \n  select(current_time, streetName, in_use)## # A tibble: 2 × 3\n##   current_time        streetName in_use\n##   <dttm>              <chr>       <dbl>\n## 1 2022-12-20 23:51:56 Ribes          13\n## 2 2022-12-20 23:52:04 Ribes          19"},{"path":"automating-api-programs-real-time-bicycle-data.html","id":"automating-the-program","chapter":"16 Automating API programs: real-time bicycle data","heading":"16.3 Automating the program","text":"Just chapter TODO chapter automating web scrapers, APIs need automated. strategy pretty much chapter. create script defines functions ’ll use program run functions end. case, ’s running function save_bicing_data. automate process using cron running script schedule.Let’s organize code script. ’s :Remember need change local path save CSV script. change variable local_csv wherever want save CSV file. ’m going save script bicing_api.R /home/jorge.cimentada/bicing/ need save locally computer, directory defined local_csv. script saved set cron job schedule script. Refer chapter TODO chapter web scraping cron works syntax setting schedule works. point file bicing_api.R saved locally wherever want store bicing data. case, can see :Let’s test works first running Rscript api_bicing.R:works see CSV file saved. Let’s use ls:Moreover, able see CSV file content inside:Let’s type crontab -e open crontab console:Since want run every minute, cron expression want * * * * * Rscript /home/jorge.cimentada/bicing/api_bicing.R. effectively reads : every minute every day every year, run script api_bicing.R. look like crontab:way, remember exit cron interface, follow steps:Hit CTRL X (exiting cron interface)prompt save file. Press Y save .Press enter save cron schedule file name .Immediately cron start schedule. Wait two three minutes open file bicing_history.csv ’ll able see something like :CSV file continually updated every minute. Just web scraping, strategy automation can take long way collecting data. Automation one main building blocks data harvesting often ’ll find online data quick disappear ’ll need automate process collect .","code":"\nlibrary(scrapex)\nlibrary(httr2)\nlibrary(dplyr)\nlibrary(readr)\n\nbicing <- api_bicing()\n\nreal_time_bicing <- function(bicing_api) {\n  rt_bicing <- paste0(bicing_api$api_web, \"/api/v1/real_time_bicycles\")\n  \n  resp_output <- \n    rt_bicing %>%\n    request() %>% \n    req_perform()\n  \n  all_stations <-\n  resp_output %>%\n  resp_body_json()\n\n  all_stations_df <- \n    all_stations %>%\n    lapply(data.frame) %>%\n    bind_rows()\n  \n  all_stations_df\n}\n\nsave_bicing_data <- function(bicing_api) {\n  # Perform a request to the bicing API and get the data frame back\n  bicing_results <- real_time_bicing(bicing_api)\n  \n  # Specify the local path where the file will be saved\n  local_csv <- \"/home/jorge.cimentada/bicing/bicing_history.csv\"\n  \n  # Extract the directory where the local file is saved\n  main_dir <- dirname(local_csv)\n\n  # If the directory does *not* exist, create it, recursively creating each folder  \n  if (!dir.exists(main_dir)) {\n    dir.create(main_dir, recursive = TRUE)\n  }\n\n  # If the file does not exist, save the current bicing response  \n  if (!file.exists(local_csv)) {\n    write_csv(bicing_results, local_csv)\n  } else {\n    # If the file does exist, the *append* the result to the already existing CSV file\n    write_csv(bicing_results, local_csv, append = TRUE)\n  }\n}\n\nsave_bicing_data(bicing)"},{"path":"automating-api-programs-real-time-bicycle-data.html","id":"exercises-10","chapter":"16 Automating API programs: real-time bicycle data","heading":"16.4 Exercises","text":"Can disable cron expression just enable ?Can disable cron expression just enable ?Can create logs.txt file inside directory cron running save time stamp, number rows columns request response? Logs extremely important programs can check see scheduled program might failed. print logs look something like request program makes:Can create logs.txt file inside directory cron running save time stamp, number rows columns request response? Logs extremely important programs can check see scheduled program might failed. print logs look something like request program makes:hints:’ll need create empty logs file doesn’t exist.’ll need extract time stamp dimensions data frameYou’ll need use one write_* functions save txt file remember append results accumulate everything.adding changes, relaunch schedule look logs file: populate every minute status program.","code":"#############################################################################\nCurrent timestamp of request 2022-12-21 01:41:25\nNumber of columns: 8 \nNumber rows: 20\n#############################################################################\nCurrent timestamp of request 2022-12-22 01:40:25\nNumber of columns: 8\nNumber rows: 20"},{"path":"insightful-and-robust-data-collection-progams.html","id":"insightful-and-robust-data-collection-progams","chapter":"17 Insightful and robust data collection progams","heading":"17 Insightful and robust data collection progams","text":"Adding logger\nChecking inputs\nSaving empty rows avoid breaking programs","code":""}]

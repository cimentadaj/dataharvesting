<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>7 Automating Web Scraping Scripts | Data Harvesting with R</title>
<meta name="author" content="Jorge Cimentada">
<meta name="description" content="There are two types of webscraping: one-off scrapings or frequent scrapings. The first one is what we did in chapter 2. You create a program to scrape something only once. This is a very common...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="7 Automating Web Scraping Scripts | Data Harvesting with R">
<meta property="og:type" content="book">
<meta property="og:url" content="https://cimentadaj.github.io/dataharvesting/automating-scripts.html">
<meta property="og:description" content="There are two types of webscraping: one-off scrapings or frequent scrapings. The first one is what we did in chapter 2. You create a program to scrape something only once. This is a very common...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="7 Automating Web Scraping Scripts | Data Harvesting with R">
<meta name="twitter:description" content="There are two types of webscraping: one-off scrapings or frequent scrapings. The first one is what we did in chapter 2. You create a program to scrape something only once. This is a very common...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/_Lato-0.4.1/font.css" rel="stylesheet">
<link href="libs/_Roboto%20Mono-0.4.1/font.css" rel="stylesheet">
<link href="libs/_Montserrat-0.4.1/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><link href="libs/str_view-0.1.0/str_view.css" rel="stylesheet">
<script src="libs/str_view-binding-1.4.0/str_view.js"></script><link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<script>
     (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
         (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
     })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

     ga('create', 'UA-99618359-1', 'auto');
     ga('send', 'pageview');

    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style/style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h2>
        <a href="index.html" title="">Data Harvesting with R</a>
      </h2>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Welcome</a></li>
<li class="book-part">Webscraping</li>
<li><a class="" href="primer-webscraping.html"><span class="header-section-number">2</span> A primer on Webscraping</a></li>
<li><a class="" href="data-formats-for-webscraping.html"><span class="header-section-number">3</span> Data Formats for Webscraping</a></li>
<li><a class="" href="regex.html"><span class="header-section-number">4</span> What you need to know about regular expressions</a></li>
<li><a class="" href="xpath-chapter.html"><span class="header-section-number">5</span> What you need to know about XPath</a></li>
<li><a class="" href="spanish-school.html"><span class="header-section-number">6</span> Case study: Scraping Spanish school locations from the web</a></li>
<li><a class="active" href="automating-scripts.html"><span class="header-section-number">7</span> Automating Web Scraping Scripts</a></li>
<li><a class="" href="scraping-javascript-based-website.html"><span class="header-section-number">8</span> Scraping JavaScript based website</a></li>
<li><a class="" href="ethical-issues.html"><span class="header-section-number">9</span> Ethical issues in Web Scraping</a></li>
<li class="book-part">APIs</li>
<li><a class="" href="intro-apis.html"><span class="header-section-number">10</span> Introduction to REST APIs</a></li>
<li><a class="" href="primer-apis.html"><span class="header-section-number">11</span> A primer on APIs</a></li>
<li><a class="" href="a-dialogue-between-computers.html"><span class="header-section-number">12</span> A dialogue between computers</a></li>
<li><a class="" href="json-chapter.html"><span class="header-section-number">13</span> What you need to know about JSON</a></li>
<li><a class="" href="case-study-exploring-the-amazon-api.html"><span class="header-section-number">14</span> Case study: Exploring the Amazon API</a></li>
<li><a class="" href="automating-api-programs-real-time-bicycle-data.html"><span class="header-section-number">15</span> Automating API programs: real-time bicycle data</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/cimentadaj/dataharvesting">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="automating-scripts" class="section level1" number="7">
<h1>
<span class="header-section-number">7</span> Automating Web Scraping Scripts<a class="anchor" aria-label="anchor" href="#automating-scripts"><i class="fas fa-link"></i></a>
</h1>
<p>There are two types of webscraping: one-off scrapings or frequent scrapings. The first one is what we did in chapter <a href="primer-webscraping.html#primer-webscraping">2</a>. You create a program to scrape something only once. This is a very common approach and we’ve done it a few time throughout this book. The second one involves building scrapers that you know will be used more frequently. Many examples come to mind: scrapers to extract news on a frequent basis, scrapers to extract temperature data on a daily basis, scrapers to collect prices of groceries in a supermarket website and so on. These scrapers are designed to be run on frequent intervals to get timely information.</p>
<p>For one-off scrapers, all of the material of the book until this chapter should be enough. However, for frequent scrapings we need new tools and strategies. Have you asked yourself how can you automate a script? By automating I mean, for example, run that script every Thursday at 08:00 PM. This chapter focuses on scheduling programs to run whenever you want them to. You might need this to collect data on a website that is changing constantly or to request data from an API on frequent intervals (the topic of API’s is the second part of this book) but in any of those two cases you don’t want to be manually running to your house at 3 in the morning to run the program. This chapter will make sure you don’t have to do that.</p>
<div class="rmdnote">
<p>Scheduling scripts is very different between operating systems. This chapter will focus solely on scheduling scripts for Linux and MacOS. Windows users are recommended to search for ‘Windows Task Scheduler’ online to find out how they can schedule their R programs.</p>
</div>
<div id="the-scraping-program" class="section level2" number="7.1">
<h2>
<span class="header-section-number">7.1</span> The Scraping Program<a class="anchor" aria-label="anchor" href="#the-scraping-program"><i class="fas fa-link"></i></a>
</h2>
<p>The first thing we need is a scraping program. Let’s recycle one from chapter <a href="xpath-chapter.html#xpath-chapter">5</a> which loaded an example from the newspaper “El País”. Our scraper will count the number of articles for each of the sections available in the newspaper “El País”. The R script will parse the “El País” website, extract the number of articles per section and collect everything in a data frame. Here’s how it would look like:</p>
<div class="sourceCode" id="cb228"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Load all our libraries</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">scrapex</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://xml2.r-lib.org/">xml2</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://magrittr.tidyverse.org">magrittr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://purrr.tidyverse.org/">purrr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tibble.tidyverse.org/">tibble</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyr.tidyverse.org">tidyr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://readr.tidyverse.org">readr</a></span><span class="op">)</span>

<span class="co"># If this were being done on the real website of the newspaper, you'd want to</span>
<span class="co"># replace the line below with the real link of the website.</span>
<span class="va">newspaper_link</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/scrapex/man/elpais_newspaper_ex.html">elpais_newspaper_ex</a></span><span class="op">(</span><span class="op">)</span>
<span class="va">newspaper</span> <span class="op">&lt;-</span> <span class="fu"><a href="http://xml2.r-lib.org/reference/read_xml.html">read_html</a></span><span class="op">(</span><span class="va">newspaper_link</span><span class="op">)</span>

<span class="va">all_sections</span> <span class="op">&lt;-</span>
  <span class="va">newspaper</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="co"># Find all &lt;section&gt; tags which have an &lt;article&gt; tag</span>
  <span class="co"># below each &lt;section&gt; tag. Keep only the &lt;article&gt;</span>
  <span class="co"># tags which an attribute @data-dtm-region.</span>
  <span class="fu"><a href="http://xml2.r-lib.org/reference/xml_find_all.html">xml_find_all</a></span><span class="op">(</span><span class="st">"//section[.//article][@data-dtm-region]"</span><span class="op">)</span>

<span class="va">final_df</span> <span class="op">&lt;-</span>
  <span class="va">all_sections</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="co"># Count the number of articles for each section</span>
  <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map</a></span><span class="op">(</span><span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu"><a href="http://xml2.r-lib.org/reference/xml_find_all.html">xml_find_all</a></span><span class="op">(</span><span class="va">.x</span>, <span class="st">".//article"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="co"># Name all sections</span>
  <span class="fu"><a href="https://magrittr.tidyverse.org/reference/aliases.html">set_names</a></span><span class="op">(</span><span class="va">all_sections</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="http://xml2.r-lib.org/reference/xml_attr.html">xml_attr</a></span><span class="op">(</span><span class="st">"data-dtm-region"</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="co"># Convert to data frame</span>
  <span class="fu"><a href="https://tibble.tidyverse.org/reference/enframe.html">enframe</a></span><span class="op">(</span>name <span class="op">=</span> <span class="st">"sections"</span>, value <span class="op">=</span> <span class="st">"num_articles"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/nest.html">unnest</a></span><span class="op">(</span><span class="va">num_articles</span><span class="op">)</span>

<span class="va">final_df</span></code></pre></div>
<pre><code>## # A tibble: 11 × 2
##    sections                                   num_articles
##    &lt;chr&gt;                                             &lt;int&gt;
##  1 portada_apertura                                      5
##  2 portada_arrevistada                                   1
##  3 portada_tematicos_science,-tech-&amp;-health              5
##  4 portada_tematicos_business-&amp;-economy                  2
##  5 portada_tematicos_undefined                           1
##  6 portada_branded_                                      2
##  7 portada_arrevistada_culture                           5
##  8 portada_tematicos_work-&amp;-lifestyle                    3
##  9 portada_arrevistada                                   1
## 10 portada_tematicos_celebrities,-movies-&amp;-tv            4
## 11 portada_tematicos_our-selection                       4</code></pre>
<p>There are 11 sections, each one with their respective number of articles. For a personal research project of yours, you’re interested in collecting these counts every day at three different times of the day. Your idea is to try to map how newspapers shift their writing efforts for different sections. Your hypothesis is that newspapers with certain ideologies might give more weight to certain sections while others give their weight to other sections.</p>
<p>The scraper above does precisely that but it’s missing a step: saving the data. The logic that we want to achieve is something like this:</p>
<ol style="list-style-type: decimal">
<li>If this is the first time the scraper is run, save a csv file with the count of sections</li>
<li>If the CSV with the count of section exists, open the CSV file and append the newest data with the current time stamp</li>
</ol>
<p>This approach will add rows with new counts every time the scraper is run. By plotting the time stamp and the count of sections you’ll be able to visualize how these counts change over time. To do that, we should add a new section in the code to save our results on a CSV file with the time stamp of the current date. Let’s add a section to save our data:</p>
<div class="sourceCode" id="cb230"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">scrapex</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://xml2.r-lib.org/">xml2</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://magrittr.tidyverse.org">magrittr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://purrr.tidyverse.org/">purrr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tibble.tidyverse.org/">tibble</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyr.tidyverse.org">tidyr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://readr.tidyverse.org">readr</a></span><span class="op">)</span>

<span class="va">newspaper_link</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/scrapex/man/elpais_newspaper_ex.html">elpais_newspaper_ex</a></span><span class="op">(</span><span class="op">)</span>

<span class="va">all_sections</span> <span class="op">&lt;-</span>
  <span class="va">newspaper_link</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="http://xml2.r-lib.org/reference/read_xml.html">read_html</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="http://xml2.r-lib.org/reference/xml_find_all.html">xml_find_all</a></span><span class="op">(</span><span class="st">"//section[.//article][@data-dtm-region]"</span><span class="op">)</span>

<span class="va">final_df</span> <span class="op">&lt;-</span>
  <span class="va">all_sections</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map</a></span><span class="op">(</span><span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu"><a href="http://xml2.r-lib.org/reference/xml_find_all.html">xml_find_all</a></span><span class="op">(</span><span class="va">.x</span>, <span class="st">".//article"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://magrittr.tidyverse.org/reference/aliases.html">set_names</a></span><span class="op">(</span><span class="va">all_sections</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="http://xml2.r-lib.org/reference/xml_attr.html">xml_attr</a></span><span class="op">(</span><span class="st">"data-dtm-region"</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://tibble.tidyverse.org/reference/enframe.html">enframe</a></span><span class="op">(</span>name <span class="op">=</span> <span class="st">"sections"</span>, value <span class="op">=</span> <span class="st">"num_articles"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/nest.html">unnest</a></span><span class="op">(</span><span class="va">num_articles</span><span class="op">)</span>

<span class="co"># Save the current date time as a column</span>
<span class="va">final_df</span><span class="op">$</span><span class="va">date_saved</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/format.html">format</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Sys.time.html">Sys.time</a></span><span class="op">(</span><span class="op">)</span>, <span class="st">"%Y-%m-%d %H:%M"</span><span class="op">)</span>

<span class="co"># Where the CSV will be saved. Note that this directory</span>
<span class="co"># doesn't exist yet.</span>
<span class="va">file_path</span> <span class="op">&lt;-</span> <span class="st">"~/newspaper/newspaper_section_counter.csv"</span>

<span class="co"># *Try* reading the file. If the file doesn't exist, this will silently save an error</span>
<span class="va">res</span> <span class="op">&lt;-</span> <span class="kw"><a href="https://rdrr.io/r/base/try.html">try</a></span><span class="op">(</span><span class="fu"><a href="https://readr.tidyverse.org/reference/read_delim.html">read_csv</a></span><span class="op">(</span><span class="va">file_path</span>, show_col_types <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>, silent <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="co"># If the file doesn't exist</span>
<span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/class.html">inherits</a></span><span class="op">(</span><span class="va">res</span>, <span class="st">"try-error"</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span>
  <span class="co"># Save the data frame we scraped above</span>
  <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="st">"File doesn't exist; Creating it"</span><span class="op">)</span>
  <span class="fu"><a href="https://readr.tidyverse.org/reference/write_delim.html">write_csv</a></span><span class="op">(</span><span class="va">final_df</span>, <span class="va">file_path</span><span class="op">)</span>
<span class="op">}</span> <span class="kw">else</span> <span class="op">{</span>
  <span class="co"># If the file was read successfully, append the</span>
  <span class="co"># new rows and save the file again</span>
  <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">res</span>, <span class="va">final_df</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://readr.tidyverse.org/reference/write_delim.html">write_csv</a></span><span class="op">(</span><span class="va">file_path</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>In summary, this script will read the website of “El País”, count the number of sections and save the results as a CSV file at <code>~/newspaper/newspaper_section_counter.csv</code>. That directory still doesn’t exist, so we’ll create it first.</p>
</div>
<div id="the-terminal" class="section level2" number="7.2">
<h2>
<span class="header-section-number">7.2</span> The Terminal<a class="anchor" aria-label="anchor" href="#the-terminal"><i class="fas fa-link"></i></a>
</h2>
<p>To use <code>cron</code>, and in general to executing R scripts, you’ll need to get familiar with the terminal. Let’s open the terminal. On Linux, you can do it by pressing the keys CTRL + ALT + t together. For MacOS click the Launchpad icon in the Dock, type Terminal in the search field and then click Terminal. For both operating systems you should see a window like this (not exactly but similar) one pop up:</p>
<div class="inline-figure"><img src="images/automating_scripts/basic_terminal_ubuntu.png" width="100%" style="display: block; margin: auto;"></div>
<p>This is the terminal. It allows you to do things like create directories and files just as you would do in your computer but through code. Let’s create the directory where we’ll save the CSV file and the R script that performs the scraping. To create the directory, we use the command <code>mkdir</code> which stands for <code>m</code>a<code>k</code>e<code>dir</code>ectory. Let’s create it with <code>mkdir ~/newspaper/</code>:</p>
<div class="inline-figure"><img src="images/automating_scripts/create_newspaper_dir.png" width="100%" style="display: block; margin: auto;"></div>
<p>Great, the directory was created. Now you should copy the R script we wrote down above and save it in <code>~/newspaper/</code>. Save it as <code>newspaper_scraper.R</code>. You should have only an R file within <code>~/newspaper/</code> called <code>newspaper_scraper.R</code>. You can confirm this by typing <code>ls ~/newspaper/</code> which will list all files/directories inside <code>~/newspaper</code>/. You should see an output like this one:</p>
<div class="sourceCode" id="cb231"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb231-1"><a href="automating-scripts.html#cb231-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ls</span> ~/newspaper/</span>
<span id="cb231-2"><a href="automating-scripts.html#cb231-2" aria-hidden="true" tabindex="-1"></a><span class="co"># newspaper_scraper.R</span></span></code></pre></div>
<p>Your script was saved successfully inside our directory. Let’s switch our ‘directory’ to <code>~/newspaper/</code> in the terminal. In the terminal you can change directories with the <code>cd</code> command, which stands for <code>c</code>hange<code>d</code>irectory, followed by the path where you want to switch to. For our case, this would be <code>cd ~/newspaper/</code>:</p>
<div class="inline-figure"><img src="images/automating_scripts/cd_newspaper.png" width="100%" style="display: block; margin: auto;"></div>
<p>As you can see in the third line of the image, there now appears <code>~/newspaper</code> in blue, denoting that I am at the directory right now. To execute an R script from the terminal you can do it with the <code>Rscript</code> command followed by the file name. For our case it should be <code>Rscript newspaper_scraper.R</code>. Let’s run it:</p>
<div class="inline-figure"><img src="images/automating_scripts/running_scraper_once.png" width="100%" style="display: block; margin: auto;"></div>
<p>The first few lines show the printing of package loading but we finally see the print statement we added when the file doesn’t exit: <code>File doesn't exist; Creating it</code>. If you opened the CSV in your computer you should see the a sheet like this one:</p>
<div class="inline-figure"><img src="images/automating_scripts/newspaper_scraping_excel.png" width="80%" style="display: block; margin: auto;"></div>
<p>Great, our scraper works! We have about half the job done. Now we need to come up with a way to execute this <code>Rscript ~/newspaper/newspaper_scraper.R</code> on a schedule.</p>
</div>
<div id="cron-your-scheduling-friend" class="section level2" number="7.3">
<h2>
<span class="header-section-number">7.3</span> cron, your scheduling friend<a class="anchor" aria-label="anchor" href="#cron-your-scheduling-friend"><i class="fas fa-link"></i></a>
</h2>
<p>Luckily for you, such a program already exists. It’s called <code>cron</code> and it allows you to run your script on a schedule. For Ubuntu, you can install <code>cron</code> with:</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb232-1"><a href="automating-scripts.html#cb232-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get update</span>
<span id="cb232-2"><a href="automating-scripts.html#cb232-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get install cron</span></code></pre></div>
<p>For MacOS you can install it with:</p>
<div class="sourceCode" id="cb233"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb233-1"><a href="automating-scripts.html#cb233-1" aria-hidden="true" tabindex="-1"></a><span class="ex">brew</span> install <span class="at">--cask</span> cron</span></code></pre></div>
<p>In both cases, after the install is successful, you should be able to confirm that it works with <code>crontab -l</code>:</p>
<div class="inline-figure"><img src="images/automating_scripts/crontab_list.png" width="100%" style="display: block; margin: auto;"></div>
<p>The output means you have no scheduled scripts in your computer. To schedule a script with <code>cron</code> you need two things: the command to execute and the schedule. The command to execute we already know, it’s <code>Rscript ~/newspaper/newspaper_scraper.R</code>. For specifying schedules, <code>cron</code> has a particular syntax. How does it work? Let’s take a look:</p>
<div class="inline-figure"><img src="images/automating_scripts/crontab_syntax.png" width="80%" style="display: block; margin: auto;"></div>
<p>At the bottom of the image we see 5 <code>*</code>. The text above it tells what each of these <code>*</code> stands for. In this order, they represent minutes, hours, day of month, month and day of week.<code>*</code> is in fact a placeholder to signal that whenever there is a <code>*</code> it means the command will be repeated at each instance of the place holder. Complicated? Look at this example:</p>
<pre><code>* * * * *</code></pre>
<p>By writing <code>* * * * *</code>, we’re scheduling the program to run at every minute, of every hour, of every day of the month, for every month for every day of the week. Say we changed the schedule to run every 30 minutes for each hour of each day of each month for each day of week. That sounds awfully complicated to say. A simpler way is to say that the script will run every 30 minutes <em>because</em> all other date parameters are <code>*</code>, which means on every unit of each of the parameters.
How would such a schedule look like? We know the first slot is for minutes so we can write <code>30</code> in the first slot:</p>
<pre><code>30 * * * *</code></pre>
<p>We’re effectively scheduling something to run at minute 30 of each hour, each day, each month, each day of the week (if day of week, the last slot, clashes in a schedule with the third slot which is day of month then any day matching either the day of month, or the day of week, shall be matched).</p>
<p>Now that we know that each <code>*</code> represents a date parameter we can start to develop more interesting schedules. For example, Wednesdays are the third day of the week (if we start counting on Monday), so we can run a schedule every 30 minutes but <em>only</em> on Wednesdays:</p>
<pre><code>30 * * * 3</code></pre>
<p>Or you might want to run your scraper at 05:30 AM only on Saturday and Sunday:</p>
<pre><code>30 5 * * 6,7</code></pre>
<p>The expression reads like this: run on the 30th minute of the 5th hour every month but on Saturday and Sunday (6th and 7th day of the week). These simple rules can take you a long way when building your scraper.</p>
<p>Let’s say we wanted to run our newspaper scraper every 4 hours, every day, how would it look like? That sounds a bit different to what we’ve done until now. The syntax we’ve discussed specifically writes down the day / hour / minute we want to the scraper to do. We have no way of saying, regardless of the day / hour / minute, run the scraper every X hours. For that <code>cron</code> has additional tricks. If we wanted to run a scraper every every 4 hours we would write it like this:</p>
<pre><code>1 */4 * * *</code></pre>
<p>For the date parameter you want to make recurrent, add <code>/</code> by the frequency you want. If instead you wanted to run the scraper every 4 hours, every 2 days, you would write something like this:</p>
<pre><code>1 */4 * * */2</code></pre>
<p>So there it is. That’s the basics of <code>cron</code>. These simple rules will allow you to go very far in scheduling scripts for your scrapers or APIs. These details are enough to get our example running.</p>
<p>Let’s schedule our newspaper scraper to run every minute, just to make sure it works. This will get messy because it’ll append the same results in the CSV file continuously, filling out with CSV with repeated data. However, it will give you proof that your script is running on a schedule. If we want this to run every minute, our cron expression should be this <code>* * * * *</code>, the simplest expression.</p>
<p>To save the cron expression type <code>crontab -e</code> in your terminal. If this is your first time using <code>crontab</code> you should see something like this:</p>
<div class="inline-figure"><img src="images/automating_scripts/crontab_choose_editor.png" width="100%" style="display: block; margin: auto;"></div>
<p>This will allow you to pick the editor you want to use for editing your cron schedule. Pick whichever of the options points to <code>nano</code>, the easiest one. After choosing the editor (if it prompted you to pick the editor), it should continue to open a new file like this one:</p>
<div class="inline-figure"><img src="images/automating_scripts/crontab_schedule_file.png" width="100%" style="display: block; margin: auto;"></div>
<p>This is the file where you write the schedule and command that you want <code>cron</code> to run. Either scroll down with your mouse or hit the scroll down key at the bottom right of your keyboard to go to the last line of the editor. In the last line, we need to write our <code>cron</code> schedule expression and the command we want to execute:</p>
<div class="sourceCode" id="cb240"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb240-1"><a href="automating-scripts.html#cb240-1" aria-hidden="true" tabindex="-1"></a><span class="ex">*</span> <span class="pp">*</span> <span class="pp">*</span> <span class="pp">*</span> <span class="pp">*</span> Rscript ~/newspaper/newspaper_scraper.R</span></code></pre></div>
<p>When you finish writing that, your terminal should look something like this:</p>
<div class="inline-figure"><img src="images/automating_scripts/crontab_newspaper_scraper.png" width="100%" style="display: block; margin: auto;"></div>
<p>To exit the <code>cron</code> interface, <strong>follow these steps</strong>:</p>
<ul>
<li>Hit <code>CTRL</code> and <code>X</code> (this is for exiting the <code>cron</code> interface)</li>
<li>It will prompt you to save the file. <strong>Press<code>Y</code> to save it</strong>.</li>
<li>Press <code>enter</code> to save the <code>cron</code> schedule file with the same name it has.</li>
</ul>
<p>After you do this, you should be back at the terminal and you cron job should be saved. Nothing special should be happening at the moment. Wait two or three minutes and open your the CSV file again. You should find the same records duplicated but with a different time stamp:</p>
<div class="inline-figure"><img src="images/automating_scripts/newspaper_results_crontab.png" width="80%" style="display: block; margin: auto;"></div>
<p>Our <code>cron</code> schedule worked as expected! You will see different time stamps in the <code>date_saved</code> column, reflecting that the scraper was run every minute. Before we close this chapter, remember to <strong>remove the schedule and <code>Rscript</code> command from <code>cron</code></strong>. Enter <code>crontab -e</code>, go to the last line, delete the text and exit the <code>cron</code> interface with the instructions detailed above.</p>
</div>
<div id="conclusion-2" class="section level2" number="7.4">
<h2>
<span class="header-section-number">7.4</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion-2"><i class="fas fa-link"></i></a>
</h2>
<p>This framework of building a scraper, testing it and then scheduling it to run on frequent intervals is very powerful. With these commands you can automate any program (in fact, not only R but any programming language or program). However, this approach has limitations. Your computer needs to be turned on all the time in order for your <code>cron</code> schedule to run. If you’re doing a school project and it’s possible, you might get around with only using your computer. However, for more demanding scrapings (lots of data, frequent intervals) it’s almost always a better idea to run your scraper on a server.</p>
<p>Launching a server and running a scraper is out of the scope of this chapter but keep that in mind when building scrapers for your work. There are many tutorials to do that over the internet.</p>
<p><code>cron</code> can also become complex if your schedule patterns are difficult. There’s a bunch of resources that can help you on the internet. Here are some that worked for me:</p>
<ul>
<li><a href="https://crontab.guru/">Crontab Guru</a></li>
<li><a href="https://linuxhint.com/cron_jobs_complete_beginners_tutorial/">Cron tutorial</a></li>
</ul>
</div>
<div id="exercises-5" class="section level2" number="7.5">
<h2>
<span class="header-section-number">7.5</span> Exercises<a class="anchor" aria-label="anchor" href="#exercises-5"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li><p>What is the <code>cron</code> expression to run every 15 minutes on Monday, Wednesday and Friday but only on February?</p></li>
<li><p>An R packaged called <code>cronR</code> allows you to setup <code>cron</code> schedules within R. Can you replicate what we did in this chapter using the <code>cronR</code> package? The package documentation can be found <a href="https://github.com/bnosac/cronR">here</a>.</p></li>
<li><p>Can you write down a script that empties the trash folder on your personal computer every Monday at 11AM? Write down both the scraper as well as the <code>cron</code> expression. <strong>Remember to remove this <code>cron</code> after deploying it to avoid unexpected files being deleted</strong>.</p></li>
</ol>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="spanish-school.html"><span class="header-section-number">6</span> Case study: Scraping Spanish school locations from the web</a></div>
<div class="next"><a href="scraping-javascript-based-website.html"><span class="header-section-number">8</span> Scraping JavaScript based website</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#automating-scripts"><span class="header-section-number">7</span> Automating Web Scraping Scripts</a></li>
<li><a class="nav-link" href="#the-scraping-program"><span class="header-section-number">7.1</span> The Scraping Program</a></li>
<li><a class="nav-link" href="#the-terminal"><span class="header-section-number">7.2</span> The Terminal</a></li>
<li><a class="nav-link" href="#cron-your-scheduling-friend"><span class="header-section-number">7.3</span> cron, your scheduling friend</a></li>
<li><a class="nav-link" href="#conclusion-2"><span class="header-section-number">7.4</span> Conclusion</a></li>
<li><a class="nav-link" href="#exercises-5"><span class="header-section-number">7.5</span> Exercises</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/cimentadaj/dataharvesting/blob/main/book/07-automating_scripts.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/cimentadaj/dataharvesting/edit/main/book/07-automating_scripts.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Data Harvesting with R</strong>" was written by Jorge Cimentada. It was last built on 2023-01-29.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer>
</body>
</html>

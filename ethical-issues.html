<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>9 Ethical issues in Web Scraping | Data Harvesting with R</title>
<meta name="author" content="Jorge Cimentada">
<meta name="description" content="Although quick to get up and running, web scraping is a delicate issue. It’s delicate because it involves grabbing information that is not yours and using it for your own purposes. In some...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="9 Ethical issues in Web Scraping | Data Harvesting with R">
<meta property="og:type" content="book">
<meta property="og:url" content="https://cimentadaj.github.io/dataharvesting/ethical-issues.html">
<meta property="og:description" content="Although quick to get up and running, web scraping is a delicate issue. It’s delicate because it involves grabbing information that is not yours and using it for your own purposes. In some...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="9 Ethical issues in Web Scraping | Data Harvesting with R">
<meta name="twitter:description" content="Although quick to get up and running, web scraping is a delicate issue. It’s delicate because it involves grabbing information that is not yours and using it for your own purposes. In some...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/_Lato-0.4.1/font.css" rel="stylesheet">
<link href="libs/_Roboto%20Mono-0.4.1/font.css" rel="stylesheet">
<link href="libs/_Montserrat-0.4.1/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><link href="libs/str_view-0.1.0/str_view.css" rel="stylesheet">
<script src="libs/str_view-binding-1.4.0/str_view.js"></script><link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<script>
     (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
         (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
     })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

     ga('create', 'UA-99618359-1', 'auto');
     ga('send', 'pageview');

    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style/style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h2>
        <a href="index.html" title="">Data Harvesting with R</a>
      </h2>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Welcome</a></li>
<li class="book-part">Webscraping</li>
<li><a class="" href="primer-webscraping.html"><span class="header-section-number">2</span> A primer on Webscraping</a></li>
<li><a class="" href="data-formats-for-webscraping.html"><span class="header-section-number">3</span> Data Formats for Webscraping</a></li>
<li><a class="" href="regex.html"><span class="header-section-number">4</span> What you need to know about regular expressions</a></li>
<li><a class="" href="xpath-chapter.html"><span class="header-section-number">5</span> What you need to know about XPath</a></li>
<li><a class="" href="spanish-school.html"><span class="header-section-number">6</span> Case study: Scraping Spanish school locations from the web</a></li>
<li><a class="" href="automating-scripts.html"><span class="header-section-number">7</span> Automating Web Scraping Scripts</a></li>
<li><a class="" href="scraping-javascript-based-website.html"><span class="header-section-number">8</span> Scraping JavaScript based website</a></li>
<li><a class="active" href="ethical-issues.html"><span class="header-section-number">9</span> Ethical issues in Web Scraping</a></li>
<li class="book-part">APIs</li>
<li><a class="" href="intro-apis.html"><span class="header-section-number">10</span> Introduction to REST APIs</a></li>
<li><a class="" href="primer-apis.html"><span class="header-section-number">11</span> A primer on APIs</a></li>
<li><a class="" href="a-dialogue-between-computers.html"><span class="header-section-number">12</span> A dialogue between computers</a></li>
<li><a class="" href="json-chapter.html"><span class="header-section-number">13</span> What you need to know about JSON</a></li>
<li><a class="" href="case-study-exploring-the-amazon-api.html"><span class="header-section-number">14</span> Case study: Exploring the Amazon API</a></li>
<li><a class="" href="automating-api-programs-real-time-bicycle-data.html"><span class="header-section-number">15</span> Automating API programs: real-time bicycle data</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/cimentadaj/dataharvesting">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="ethical-issues" class="section level1" number="9">
<h1>
<span class="header-section-number">9</span> Ethical issues in Web Scraping<a class="anchor" aria-label="anchor" href="#ethical-issues"><i class="fas fa-link"></i></a>
</h1>
<p>Although quick to get up and running, web scraping is a delicate issue. It’s delicate because it involves grabbing information that is not yours and using it for your own purposes. In some respects that might be even contrary to the terms of services of the website. It also involves <em>cluttering</em> the servers of the website with potentially many requests, making the functioning of the website less optimal. In this chapter we’ll describe how you affect a website when you scrape it, how you can avoid problems with the website owners and how to figure out if indeed the website <em>allows</em> you to scrape that information.</p>
<div id="make-your-scraper-sleep" class="section level2" number="9.1">
<h2>
<span class="header-section-number">9.1</span> Make your scraper sleep<a class="anchor" aria-label="anchor" href="#make-your-scraper-sleep"><i class="fas fa-link"></i></a>
</h2>
<p>Whenever you run a scraping script, your program makes a <em>request</em> to the website. A request means that you <em>ask</em> the servers behind the website to send you information. When I learned that scraping worked like that, I was completely shocked. I thought scraping was more like copying the content of the website into my local computer. Nothing harmless in that, right?</p>
<p>Well, to my surprise, I was making that request many times every day without knowing it. Scraping is the same as entering a website through a browser. That moment between when you hit enter to go to a website and when the information is actually displayed is the time it takes for the server behind the website to return all the content. That operation is important for a website. It involves requesting information, waiting for the server to return it and rendering it in your browser. A human is slow enough for a server to handle the requests it needs but imagine a human the tried to enter a website 5 times per second continuously for 48 hours. That amounts to 864,000 requests. That’s a lot.</p>
<p>Big websites such as Google or Amazon have enough servers and throttle to handle millions of requests per second but most of the content on the internet does not. For that reason, it’s important that whenever you make a request to a website (calling <code>read_html</code> or <code>read_xml</code>), you add a <em>system sleep</em> to your scraper. In R you can do that with <code>Sys.sleep(amount_of_seconds)</code> where <code>amount_of_seconds</code> is the amount of seconds you want to sleep before making a request.</p>
<p>If we were scraping a website only once, making only a single request, adding a system sleep should not matter. What we want to avoid is making <em>many</em> requests in shorts amount of time. So for example, in our example in chapter <a href="spanish-school.html#spanish-school">6</a> we scraped information for several different schools. Each school that we scraped, meant making a request to the website. That’s a perfect example where want to sleep before making a request. So in R code, the skeleton code would look something like this with the system sleep:</p>
<div class="sourceCode" id="cb272"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">scrapex</span><span class="op">)</span>

<span class="co"># List of links to make a request</span>
<span class="va">school_links</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/scrapex/man/spanish_schools_ex.html">spanish_schools_ex</a></span><span class="op">(</span><span class="op">)</span>

<span class="co"># List where we will save the information for each link</span>
<span class="va">all_schools</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="op">)</span>

<span class="va">single_school_scraper</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">single_link</span><span class="op">)</span> <span class="op">{</span>
  <span class="co"># Before making a request, sleep 5 seconds</span>
  <span class="fu"><a href="https://rdrr.io/r/base/Sys.sleep.html">Sys.sleep</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span>

  <span class="co"># Perform some scraping</span>
<span class="op">}</span>

<span class="co"># Loop over each link to make a request</span>
<span class="kw">for</span> <span class="op">(</span><span class="va">single_link</span> <span class="kw">in</span> <span class="va">school_links</span><span class="op">)</span> <span class="op">{</span>
  <span class="co"># Save results in a list</span>
  <span class="va">all_schools</span><span class="op">[[</span><span class="va">single_link</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu">single_school_scraper</span><span class="op">(</span><span class="va">single_link</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>We create a single function that works well for scraping a single school. Before we launch that to scrape all different schools, we add <code>Sys.sleep(5)</code> before scraping each school. This way, before making a new request, we’ll let the servers rest and avoid too many requests in such a short amount of time.</p>
<p>How many seconds should you wait? Sometimes the <code>robotstxt</code> file (see the section below) will tell you how many seconds per scrape you should wait. Other than that, there’s no real estimate. Depending on how many requests you make you’ll want to lower the number of seconds to 2 or 3. Multiplying the total number of websites you’ll scrape by the number of seconds you’ll sleep will give you a rough estimate of how much time your program will last.</p>
<p>As a rule of thumb, it’s always a good idea to limit your scraping to non-working hours such as over the evening. This can help reduce the chances of collapsing the website since fewer people are visiting websites in the evening.</p>
</div>
<div id="terms-of-services" class="section level2" number="9.2">
<h2>
<span class="header-section-number">9.2</span> Terms of services<a class="anchor" aria-label="anchor" href="#terms-of-services"><i class="fas fa-link"></i></a>
</h2>
<p>When the General Data Protection Regulation (GDPR) came in effect in Europe, all websites needed to make sure every user agreed to their terms of services. These terms of services are lengthy and contain a lot of information for what the website can do with your data. However, it also contains information on what <em>you</em> can do with their data. For most internet users, this simply doesn’t matter. If you’re building a scraping program, however, this is <a href="https://12ft.io/proxy?q=http://fortune.com/2016/05/18/okcupid-data-research">important</a>.</p>
<p>Whenever you intend to scrape a website, make sure to read the terms of services. If a website clearly states that web scraping is not allowed, you must respect that. For example, Facebook has a clause specifically on automated data collection:</p>
<blockquote>
<p>You will not collect users’ content or information, or otherwise access Facebook, using automated means (such as harvesting bots, robots, spiders or scrapers) without our prior permission.</p>
</blockquote>
<p>If a website explicitly prohibits you from scraping, you should not do it. <strong>Let me make that clear again: if the terms of services forbids you from scraping their website, you should not do it. It can have legal consequences.</strong></p>
<p>If it’s not clear from the terms of services, contact the website and receive written confirmation from the website.</p>
</div>
<div id="copying-information" class="section level2" number="9.3">
<h2>
<span class="header-section-number">9.3</span> Copying information<a class="anchor" aria-label="anchor" href="#copying-information"><i class="fas fa-link"></i></a>
</h2>
<p>Even if a website allows a user to scrape its contents, they might have some preferences on <em>which</em> sections of the website you can scrape and which are forbidden. For that there’s a standard file called <code>robots.txt</code> on nearly all website on the internet which tells you want parts of the website can be scraped. The <code>robots.txt</code> is just a convenient form for a website to tell you which URLs you can scrape; it will not enforce anything nor block you in any way. <strong>In all cases, you should follow the guidelines of the <code>robots.txt</code></strong>.</p>
<p>The <code>robots.txt</code> file of most websites is located in the main URL. So for example, the <code>robots.txt</code> of Facebook is at <code>www.facebook.com/robots.txt</code>. Similarly, the <code>robots.txt</code> of Google can be found in <a href="google.com/robots.txt">google.com/robots.txt</a> and looks like this:</p>
<p>It documents each URL of the Google website and it explicitly tells you which ones are allowed or disallowed. In this section we’ll use the <code>robotstxt</code> R package which makes it very easy to tell whether a website can be scrapable. For example, we can figure out if we can scrape the landing page of Wikipedia with this:</p>
<div class="sourceCode" id="cb273"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://docs.ropensci.org/robotstxt/">robotstxt</a></span><span class="op">)</span>
<span class="fu"><a href="https://docs.ropensci.org/robotstxt/reference/paths_allowed.html">paths_allowed</a></span><span class="op">(</span><span class="st">"https://wikipedia.org"</span><span class="op">)</span></code></pre></div>
<p>The <code>TRUE</code> statement tell us we can do it. Let’s see if we can scrape the Facebook homepage:</p>
<div class="sourceCode" id="cb274"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://docs.ropensci.org/robotstxt/reference/paths_allowed.html">paths_allowed</a></span><span class="op">(</span><span class="st">"https://facebook.com"</span><span class="op">)</span></code></pre></div>
<p>Using <code>paths_allowed</code> you can provide any link to a website and it’ll automatically extract the <code>robots.txt</code> and figure out if you can scrape it. Before scraping any website, you should check whether the URL you want to scrape is allowed.</p>
<p>Another thing to be aware of is that <code>robotstxt</code> files often contain a field for <code>Crawl-delay</code>, suggesting the time you should wait between requests. You should keep that in mind for when you <code>Sys.sleep</code> in between requests.</p>
</div>
<div id="identifying-yourself" class="section level2" number="9.4">
<h2>
<span class="header-section-number">9.4</span> Identifying yourself<a class="anchor" aria-label="anchor" href="#identifying-yourself"><i class="fas fa-link"></i></a>
</h2>
<p>Even if the website allows you to scrape the URL that you’re after, you need to be extra careful and identify yourself. That means that you need to give the website a clear identification of who is making those requests. This way the website can contact you if they find there’s something wrong with your requests, or even directly block you. Remember at all times: we’re scraping data that is not ours and we should be polite when grabbing that data. If the owner of the data considers that they don’t want to give you the data, they’re in their right to do so.</p>
<p>To identify ourselves we need something called a “User-Agent”. A User-Agent contains information about our computer and our browser. You can find your user agent googling “what is my user agent?”. Google will directly tell you what it is:</p>
<p>With that in hand, we just need to tell our scraper to incorporate that in each request. How do we do that? With the <code>httr</code> package. Below is the code to do it:</p>
<div class="sourceCode" id="cb275"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://httr.r-lib.org/">httr</a></span><span class="op">)</span>

<span class="fu"><a href="https://httr.r-lib.org/reference/set_config.html">set_config</a></span><span class="op">(</span>
  <span class="fu"><a href="https://httr.r-lib.org/reference/user_agent.html">user_agent</a></span><span class="op">(</span><span class="st">"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:105.0) Gecko/20100101 Firefox/105.0; Jorge Cimentada / cimentadaj@gmail.com"</span><span class="op">)</span>
<span class="op">)</span></code></pre></div>
<p>Notice how I added my name / email to the user agent? I often do that so websites can contact me in case they believe I’m breaking any of their terms of services or they want to know the purpose of my scraper. In any case, it’s just a way of being polite. In R the user agent can be set once at the top of the script. No need to include it anywhere else inside the scraper or inside a loop that scrapes many websites; the user agent is set globally and reused in all requests.</p>

</div>
</div>



  <div class="chapter-nav">
<div class="prev"><a href="scraping-javascript-based-website.html"><span class="header-section-number">8</span> Scraping JavaScript based website</a></div>
<div class="next"><a href="intro-apis.html"><span class="header-section-number">10</span> Introduction to REST APIs</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#ethical-issues"><span class="header-section-number">9</span> Ethical issues in Web Scraping</a></li>
<li><a class="nav-link" href="#make-your-scraper-sleep"><span class="header-section-number">9.1</span> Make your scraper sleep</a></li>
<li><a class="nav-link" href="#terms-of-services"><span class="header-section-number">9.2</span> Terms of services</a></li>
<li><a class="nav-link" href="#copying-information"><span class="header-section-number">9.3</span> Copying information</a></li>
<li><a class="nav-link" href="#identifying-yourself"><span class="header-section-number">9.4</span> Identifying yourself</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/cimentadaj/dataharvesting/blob/main/book/09-ethical_issues.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/cimentadaj/dataharvesting/edit/main/book/09-ethical_issues.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Data Harvesting with R</strong>" was written by Jorge Cimentada. It was last built on 2024-02-12.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer>
</body>
</html>
